should I just go ahead and apply this kind of Q learning code that we've been doing for a long time oh no in this scenario it's going to be very error prone because you don't have identification or the thing that you're trying to optimize and the thing that you are actually optimizing are different and I'm going to like different outcomes so with respect to AGI if we try to Anchor it from a a causal perspective I think we can agree that hey causal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine Lear learning on the internet today we're traveling to an undisclosed location to meet our guest he studied economics but moved to statistics he finds inspiration in modeling natural phenomena that's why he preferred to study systems biology rather than financial markets during his PhD he loves hiking learning languages and teaching others senior researcher at Microsoft research ladies and Gentlemen please welcome Dr Robert n let me pass it to your host Alex molac ladies ladies and Gentlemen please welcome Robert NES nice to meet you thanks for having me on the uh podcast hi Robert how are you today good Prett feeling pretty good today yeah you started uh your your educational Journey with with economics and then you gradually moved towards things related to computation and then causal inference what was the a a unifying aspect of all those all those choices ah so what connects all of that so I knew fairly early on that I wanted to do something quantitative um but also connected to being out there in the real world and and then solving applied problems and I knew it would be related to statistics because I I learned fairly early on that I had perhaps a little bit of a superpower when it came to this two statistics of all the formal topics mathematical topics or quantitative topics that I had approached this one just seemed to come to come to me so naturally very naturally in a way that I couldn't really understand why given the amount of effort that I had to put in other things what did you find attractive in statistics was there anything else beyond the fact that it was just easy for you yeah that was it it was just easy like I remember there was one and it's funny because uh probability theory was actually kind of not that easy for me I mean I I eventually got good at it but I wouldn't claim that I had some kind of natural gift at it but every time I would study and then take a stats related exam in undergrad and then graduate school it things just clicked really easily uh but I wouldn't say that you know with the uh it was statistics itself that attracted me was I think the process of modeling of data um doing data science of uh building models I think that's the part that I enjoyed but the theoretical ideas always just kind of um came naturally so I think I just leaned into that how you experienc in um systems biology and looking for the lens of biology influenced the way you think about causality today I got into systems biology during my PhD because I wanted to work with a professor I wanted I wanted her as my adviser and I liked I liked a little bit about what she was doing she was doing mostly computation or statistical proteomics and I liked that it was in a natural science I think at that point I was interested I had come I had been thinking for a while about working in financial engineering and found it to be yeah that it didn't have the financial markets I I felt lacked a certain type of anchor in reality like you would build this model but what is this model right what where like in the Natural Science you know there's some kind of ground truth in reality that that you're modeling right if you're model if you're building a model of physics or a chemical system or a biochemical system there's some there's some something is either your model is obviously going to be an approximation of the truth but the truth exists and so I think at that point that's what was attracting me to comp bio and and I like I wanted to work with complex systems and dynamic systems and um build models of you that would simulate say for example the workings of a cell and that was close enough to my advisor's um area of expertise that she felt that she could support that even though it would be a new direction for our group and through that kind of working in computational biology systems biology I got introduced to uh the the task of trying to use structure learning or causal Discovery algorithms we might say today to try to reconstruct biological Pathways namely in my case signal transduction Pathways from um single cell uh data so with single cell data you would get enough degrees of freedom to be able to apply these algorithms and and and learn something that's uh approximated to the biology and so um that's that was my my first for in the causality what were the main challenges at that stage for you when you when you tried to apply structural algorithms to to this biological data it was building something that was useful in practice to people like if so causal Discovery generally speaking is this task of taking data could be observational could be experimental and trying to learn the causal properties of that system typically this is learning the the uh the structure of a causal dag uh directed a cylic graph and at the time it was interesting to try to take some of these biological data sets we were we were seeing this kind of explosion in uh measurement technology for molecular biology so things that could measure a lot of variables um at high throughput with decent speed with with decent cost and um could get you know relatively large and Rich data sets for from measuring these systems and you know so you can take this data and you could throw it at some structure learning algorithm some causal Discovery algorithm and you get a graph and then what right what do you do with this and so you would you know so this it became clear to me very early on that I needed to understand not just the biology but what the biologists what the laboratory analysts were doing or were trying to solve uh as a result of this analysis right so typically they don't just want a dag right they don't just learn a dag and then publish it and this is the dag and this and and I'm done right and so they're usually trying to do something with it say for example learn new biomarkers or learn do something with respect to drug Discovery um or understand you know whether or not there's some kind of signaling pathway that they didn't know about and so that became for to through through that need through that kind of pain point that people in the laboratory were facing I started nudging kind of causal Discovery for system biology types types of methods along the lines of of of experimental design uh specifically I was taking beijan experimental design methods uh where there's some kind of outcome that you want where the the the the dag is just an artifact on the way to that that final outcome and you're dealing with uncertainty you're dealing with um experimental design problems what should I measure how much should I measure and just kind of trying to have an entire workflow and perhaps a sequential workflow that somebody in a laboratory setting could use um to you know ideally even automates to to um whatever their final goal was in terms of um biological molecular biology research when you were able to find or discover a structure underlying certain processes there what what was your way back then to evaluate this the structure if this is a correct structure or not yeah so in the papers that came out initially in the space you would right apply the algorithm to the data get a get a graph and then you would know in these cases in these Publications the ground truth graph and then you would just say look at some kind of way of measuring distance between your learned graph and the ground truth graph so maybe like some kind of precision recall style metric or some um um statistics related to say structural hemming distance things like that and that's only of course useful if you have the ground truth model and so there's no way of evaluating how you're doing when you don't you just have a you just get this graph on the other end and so I turn to kind of basian style reasoning to say let's um number one we want to be able to deal with uncertainty number two typically this causal structure or the structure of the system you're Ming is not entirely unknown to you there are some things that you you have some prior knowledge about the size the shape of the system or what's not allowed in the system maybe this protein will never you know never interact with that protein for example and and so being able to give people a way of incorporating that prior knowledge and modeling that uncertainty and and then getting to a a a final answer that incorporated all that knowledge and uncertainty and you know in a in a rigorous way um such that you could show um through mathematical reasoning that's you know given more data and some criteria for um evaluating whether or not you've achieved um the an an answer to the biological question that you're interested in you can be having you can have guarantees that You' be at least moving in the direction of the right answer even if you didn't know exactly what the ground truth right answer was itself you mentioned uh basan experimentation um which is related to to a broader topic of of optimal experiment uh Theory I was wondering what are your thoughts about the the entire field that we could call causal decision making or causal decision Theory yeah so it's there's interesting history there I think um if you look up causal decision Theory online and you'll see a contrast with something I believe people call them perhaps um empirical decision Theory uh where uh it is contrasted as this uh where it's presented as this idea where that when you're a decision-making agent you should only be attending to the consequences of your actions as opposed to the more traditional view of maximizing expected utility uh conditional interaction and there there are some ex interesting examples for where the the causal ideas kinds of lead to suboptimal results like with something called nukem's Problem um but I think now these days where we have a lot more of a mature understanding about causal models and how to incorporate them into some end to end analysis uh we see its um you see it coming up in many places where we're doing automated decision Theory so this is a show CAO Bandit soing I think you know Elias Baron BL for example has this paper on uh conso Bandits uh where you have some kind of adversarial relationship between some uh confounder in the environment and you're trying to estimate a cause of effect perhaps with in a setting where it's not identified and maybe you have some partial identification and you're doing some I think what he calls causal Thompson sampling in that scenario and so I would say that is a question of um can we apply caal knowledge to optimizing in a system where a there's some unknown causal factors and B those causal factors uh would lead to a suboptimal decision if we use traditional um Bandit methods another area that I see kind of um uh having gotten a lot of traction in so-called causal decision Theory or or causal sequential decision- making or so is is causal reinforce in learning uh typically this either shows up in the shape of how can we use causal assumptions to get more sample efficiency in terms of our learning and I think that's very important particularly when you're trying to uh learn a model in a very high dimensional setting and Sample efficiency can make something that's intractable tractable and also an area of credit assignment so when you're trying to understand how or why policy led to a certain outcome you can be asking questions it can be you can pose it as a causal question maybe use some attribution methods or uh root cause analysis or or actual causality style methods from uh causal inference Theory and I would say and personally one of the things that's interesting me about this field is can we think about ways that humans make decisions you reasoning about cause reasoning in other ways and algorith matized them in some sense such that now we know we're automating a lot of that decision- making we're also uh rooting it in the statistical the innate statistical and probabalistic inference capabilities of our current state of machine learning algorithms right like we know that we're very good if you give us enough data we can learn statistical patterns in that data um but a lot of uh the ways that humans reason they're kind of actually not working with they're not conditioning on data the day observed they're imagining hypothetical scenarios and then conditioning on outcomes in those hypothetical worlds what we in C inference would call potential outcomes and so um and so this is something that we do that I think yes is very sample efficient like you don't need to have experienced something in order to make it as in based on the lessons from that experience you could read it from you can read it or imagine it you can observe it and somebody else for example and like okay this is here's what I'm going to do when I when I face the situation right and uh I think Cosmo models give us a semantics give us a language for building those kinds of algorithms and I think much like um uh the caal banded example it's there's a potential here to go beyond just sample efficiency and actually open up capabilities that we didn't have before um in specific scenarios where this where these types of capabilities matter I noticed that many people in the community are interested in causal reinforcement learning uh but we actually haven't covered it it in depth in any of our episodes so far um would you like to share with those people in our audience who are not familiar with the problem setting of why reinforcement Landing might not be causal although it uh involve involves action right and what are the advantages of causal uh of causal reinforcement Landing what problem causal reinforcement Landing uh can yeah and I think a lot of it would align with things that I've just said because to me causal decision Theory and causal reinforcement learning are you know very much related right I mean I think reinforcement learning is essentially asking how can we find policies for optimal decisionmaking that's automated and and done in sequence right and and of course you know reinforcement learning itself is I think a little bit of an overloaded term it generally means as opposed to the specific task of learning with reinforcement that kind of encapsulates everything that's um in some sense related I would say for somebody who's trying to understand what contributions causality could have to reinforce and learning um who has some maybe some intuition about reinforcement learning and less about causality in one sense so I think yeah the low hanging fruit here is probably sample efficiency in the sense that in causality we think a lot about the structure of the data generating process and the and those assumptions allow us to make inferences that we could not make if we were just um modeling statistical patterns in in the data traditional let's say deep reinforcement learning tries to I guess maybe specifically model free reinforcement learning tries to essentially model everything as a as a Markoff decision process with just like if you think about this as a call Dag of this kind of direct relationships between State action and reward um you know over time and a lot of that often ignores some of the causal nuances of the system and just folds everything into state right there's just some State variable that just captures everything about the world and we're not interested in seing separating that out uh those variables into some kind of causal relation related nose and a directed graph and and again most of the time since your your goal is to try to find these the action or set of actions or the action generating policy that maximizes reward that's usually good enough but then there are some cases where the thing that the act that maximize the actions that maximize expected reward and the actions that maximize expected reward when you consider how the actions change the environment can be different and it's and you can use causality to try to understand when and and how those differences happen but often times it's easier just to go and collect more data from all the the various you know again try and fold more data in try to fold everything in the state and then get data that covers as much of that state as possible so uh the causality is not really going to play that much of a helpful role unless you're a dealing with trying to maximize outcomes select actions that maximize outcomes under circumstances that are not seen in training data and even in those cases you want it such that the the actions that maximize the outcome are different from the from the actions that maximize the outcome once you once you account for all of the causal nuances of the system and I think a lot of problems that we kind of see in textbooks or that we see in class like those are the nuances are are small it doesn't seem to matter but I think to get reinforcement learning to a practical place it probably is going to matter particularly if um you know in those settings where you just don't have the ability to to to generate um training data out and and in all the scenarios where you want to apply the model especially that the state variable uh might not contain all the information that is required for caal identifiability right so we can put information we can put more information um that is required and this can bias our results like some collider biases maybe or something like this we can also exclude information colanic information in the state variable and then we'll basically make the best decisions in a purely associative terms but not uh in in Interventional terms yeah I mean there's definitely cases um particularly in causal effect inference where conditioning on some data can hurt you right because it biases the answer things like colliders for example or mediators um and yeah not much of that goes into uh the considerations of the traditional um reinforce learning approach and so I think there could be some wins there you know so reward is not enough I it probably is enough for a lot of things to be honest I mean like so I mean like uh I mean that I hear this a lot from caal inference people where you can kind of find some case where you find some case where hey if you don't consider the caal Nuance in this case um you're going to lead to problems but it might work it still might work very well in most practical problems so I don't want to um so I think that um if you're working in in in causal inference research or applied causality you need to be thinking not just about the toy problems that prove that what other people are doing is not going to work on your toy problem but you need to find practical scenarios that have high value with respect to somebody's um you know goals and research goals business goals uh engineering goals where say hey all right you definitely need a a CM model in this case otherwise it will not work and that takes that that takes a lot of domain knowledge and so it's it takes uh tring that that uh threshold from toy model to real world scenario where this thing is necessary is is a lot of work that I think is um uh well a lot of times we're not incentivized to do it right like you can get you can publish a paper on that toy example yeah U you've proven that some generalist algorithm is not going to work in this in this scenario but like unless you take it to people's problems um know it's it's you're not you're not going to get much of an audience for that work truth is complex all we can hope for for approximations I think that was John F noyman where is the where is the boundary of those approximations where where are they good enough in Practical terms rephrase the question let me make sure because it's a broad question is there yeah it's it's a broad question so so we're talking about decision making and you also mentioned that in certain cases we might not be it might be not really necessary to to to use causal models because the gain for a given use case let it be a business case or whatever a scientific case might not be justifying U building a more complex model okay let me give you an example there I think that's a very easy one to jump into so like I think a lot of people who are who who think about causal inference they're mostly thinking about causal effect inference you know so average treatment effects individual treatment effects conditional average treatment effects and if so if you if if the novice is in the in the audience will forgive me you know this is a causal bands podcast so I'll I'll mention some causal terms that maybe some people will will will recognize and so if I say you know a causal effect you're interested in you know the expectation of Y given uh do x equals treatment minus defination of why given do x equals control right and so like just just say so instead of saying treatment and control we can say you know action one or action to right and and so essentially what you need to do is learn the probability distribution of Y given doe X the do action or even maybe you don't even know need to learn all of that you just learn the expectation of why given do action right and so it's very easy to come up with an example where the expectation of why given due action and the expectation of why given action are different there'll be less cases where the argu the value of x that maximizes expectation of Y given uh dox and they argue the value of x that maximizes expectation of Y given X are different so often times while these two things will be different the the argument that maximizes both both queries will be the same and you know I don't have any kind of measured theoric theoretic quantification of how much this is but you can imagine that it's easy to to figure out the two queries are different but that the argument that maximizes the query might often be the same right and so you know in other words like the expectation the expect of Y given DX is 100 while the expect you know DX equal 1 is 100 the expectation y given xal 1 is you know 80 right and it might be that uh um the value X that maximizes both those queries is the same and and as often the case and so like in that case your expectation of Y given X is uh when you're maximizing it it's maximizing it's you're doing an approximation of the actual causal thing that you're trying to do and a lot of the time that'll get you to where you need to go and so in order for you to find a for you to motivate actually modeling the modeling the dox you need to zero in on instances where these where the argument that maximizes one and the argument that maximizes the other are different and are often different and you need to do so in a practical scenario like it's easier for me to draw draw some kind of toy model where this is true but to do it say for example with robotics or um you know um you know learning how to phot a protein for example that's going to be a lot more work I imagine that knowing in which cases uh this argument will be the same for Interventional and observational uh query might not always be straightforward right so well mean that's the I mean I think that's part of the goal of the causal analysis right to be able to explain to people clearly hey this is when you can expect this this uh this approximation approach number one you're what you're doing is kind of an approximation to the right way of doing it and number two even though you're your way of doing it will actually work in many many practical scenarios here are some practical scenarios that are that are important for a b and c reasons where it will not work and so and then now you've given somebody like a clear guideline right like oh if I want to deploy here I have a I have a learning problem I have a reinforcement learning problem should I just go ahead and apply this this uh this kind of Q learning code that we've been doing for a long time so oh no in this scenario it's going to be very error prone because you know you don't have identification or you don't you or or you know the thing that you're trying to optimize and the thing that you are actually optimizing are different and are going to lead to different outcomes um and so being able to provide people with that type of road map I think is very important you mentioned before in the context of decision making or decision Theory um the that you're interested in looking into ways how we could make algorithms uh make decisions in a humanik in a humanlike way we we know that humans uh might be good in decision making may be good in cality in certain circumstances but in certain other circumstances we are we are just not very good at any of those any of those tasks um and I want to take it into a broader context because because with the rise of generative models we started talking more about AGI and so on right some people said like hey this is Agi already we already have it maybe they change their M later maybe maybe not but AGI seems to be a vague concept right so when I started uh interacting with Chad GPT after it was released I had this impression that it gives a very strong indication of of its own intelligence in a sense but it not NE it does not necessarily comes from the fact that it's very good at some numerical tasks or estimation task and so on we know that in factual when it comes to factual information it can it can be reliable sometimes but sometimes those fa failure modes might be unexpected and very you know and very strong in a sense right that the information might might be arbitrary incorrect but this aspect of humanlike quality that we find in this text I think makes makes it very appealing to to many of us what would be a an AGI system would that be system that would be as optimal in its estimations as possible or would be a system that would be maybe just humanlike accurate or maybe these are not um exclusive exclusive options what are your thoughts so with respect to AGI if we try to Anchor it from a a causal perspective I think we can agree that an AGI model artificial um generalized intelligence would be able to to reason about cause and effect and and so you know one way that I think about that personally as I as I kind of look to some of the work that's come from the computational psychology community in a sense where kind of how how this research tends to go is You observe some ability that humans have in terms of reasoning or some kind of tendency that they have in terms of reasoning and you model it ideally a computational model and uh and then you maybe design some kind of task where you recruit a bunch of undergrads and um ask them uh to read some vignette to answer some questions and you look at the distribution of their answers and then you compare and then you apply your algorithm to those to those vignettes to those examples to those those questions and look at its answers and hopefully the uh distributions align hopefully you can find some other ways of figuring out that your model is a good model of how humans are reasoning about a certain process and I think that's that's interesting in the sense that you know from a causal perspective again we need causal models we need causal inference Theory to um specify the assum a set of assumptions and inductive biases that in addition to observe data or exper Al data we allow us to make conclusions uh that we could not make without those assumptions and so in so far as so I think there's a there's a connection here in the sense that you know is is this computational psychology model is a body of assumptions about how to do a thing uh with respect to reasoning Ed that's a lot of what we're doing in causality and so I think that a lot of that comes together particularly when it comes to causal reasoning tasks and and I think that is AEP step towards AGI in the sense that we're thinking about how do I build models that have a certain type of reasoning capability that we observe in humans and you know and so you know some of that is causal some of it's not causal but and with respect this in so far that we're looking at causal abilities there I think that that's that's an interesting thing to work on and it's it's it's actually a little bit different from kind of how traditional causal inference research Works traditional causal inference research is typically about answering objective questions from a epistem epistemological point of view about a cause or effect relationship that's external to somebody's head that exists in the in in the world right so like does uh this drug treat this illness um better than Placebo does um does this vaccine prevent uh uh catching this disease does um smoking cause cancer right and then those you know and does it does it do so on average across some population and and those questions we can be very objective about but if we're trying to figure out how to emulate a college student reasoning about some information they see uh on in a vignette or um or you know in some kind of in some in some kind of controlled environment then it's less about whether or not that human is making a objectively correct judgment is is whether but is rather whether your your algorithm can align with what that human is doing and and oftentimes these two goals of objective Truth Versus alignment are overlapping right if I'm trying to understand if I'm trying to model how um a human you know makes a conclusion from a scientific experiment for example then that's a lot like what we're already doing with causal effect estimation but if I'm trying to evaluate how a human is you know to use the previous example um observing evidence and then imagining various Futures and then making a decision based on some type of utility function that's very maybe specific to the human maybe that utility function is polluted by some cognitive biases or some kind of inefficient urtic right those are things that um uh or inaccurate urtic rather those are things that maybe I want to capture my model as well and then maybe maybe later on then when I deploy this model I'm like okay now I want to make sure I don't have any of these biases or these you know logical fallacies in there and just kind of make sure it's anchored by the by the data and so I think that is an interesting path towards AGI yeah let pause there because I think there there's some other things you asked but uh that I didn't address but we can we can Circle back to that but I think that that's kind of how I think about the problem in particularly in terms of this alignment versus objectivity problem MH it sounds like having a a general model of of human thinking uh a general causal model that would allow us to turn on and off those biases would be something extremely flexible in this case yeah you know so one example I was thinking of was there's a interesting result from kogai about how humans are we definitely engage in count in counterfactual simulation in terms of our you know what we imagine and and when we're making decisions um sometimes it doesn't quite happen so for example there is so in one case um imagine that I say that uh I broke the machine and now it no longer works and you might say and and a simple label flip there would be to say I failed to maintain the machine and now it no longer works right now from a kind of numerical en coding point of view that's more or less the same thing maybe one is one and the other is zero you can just switch to label it's the same but um it turns out that humans use counterfactual simul ation Less in the absence of actions or the absence of events right and so um you know while if I say if I prompt somebody with that type of uh that vignette story about how you know somebody does something and it causes something to break failing to do something and it and it and it and causing it something to break in a second case often times they won't engage in as much counterfactual simulation and in the first case on average across across groups and that kind of makes sense in this from a from a urtical standpoint in the sense that it's a lot easier to enumerate actions that happen than actions that did not happen or events that happened than events that did not happen right so trying to think of all the events that didn't happen and how the absence of those events could have caused something is a lot harder than enumerating the things that did happen and ask anybody who works in you know event monitoring for like you know a server or an IT system right like it's very difficult to uh detect when something doesn't happen as opposed to when something does happen and so um you know so that makes sense from a cognitive standpoint right because we you know just simply being economical about what types of events we mentally simulate but you can imagine then finding ways algorithmically to address that problem you know maybe you want to scope out the uh the negative events in a certain way that make it much easier to algori uh to the reason algorithmically about the counterfactuals of those missing events or those those absence of events and then so like I think that's one example where you can kind of um modelistic that's maybe isn't you know has certain types of is useful from a tical standpoint but maybe in some sense it's not efficient or it's not uh it's not sound um and then using statistical and algorithmic reasoning to uh um remedy that in in some appropriate way for for a given problem some time ago you published a a very interesting paper with uh s Muhammad tahi um Karen Zach and and other co-authors and although this paper was uh based in in the context of biology again you have shown some interesting identification results um there can you share a little bit more with the audience about this paper and what were the most interesting results in your opinion in this paper so this paper the idea was simple uh we were using a latent variable modeling type of approach that we often see in probabilistic machine learning um or that you see in probabilistic machine learning Frameworks or like uh like pymc or Stan or or pyro um and showing that if you come from a kind of graphical modeling standpoint and uh or or a causal graphical modeling standpoint and your and your intuition here is to say I want to model an intervention on this model by kind of removing edges from the the uh the target of the intervention and then sampling from that model we know that we can do that if we observe all the variables but if we don't observe all the variables then uh all bets are off right we just might just be sampling junk and it's just simple result using a basian proof that uh showed that if your model has a a dag and uh and the ident the um intervention distribution that you're sampling from is identified given the the rules of the do calculus or some other um graphical identification framework then uh your sampling procedure is valid because it's just an an estimator for an estimand that the uh do calculus or whatever your identific your graphical identification procedure proves exists and so it the goal here was you know I think a lot of people might say see tools like PMC or pyro and um or have some background in latent variable models say from topic models for example and they look at them and say well you can write this as a dag can I use this for causal inference and you know the answer is either nobody knows or uh you know we're not sure because uh you know you have to do a whole bunch of kind of mathematics of causal identification to make sure that it works and so this was just showing that like yeah you can do this you can if you have identification using say the do calculus it's going to work um and so we showed this um by sampling from the a predictive distribution of this target intervention distribution and then show that as the as you increase the training data the distribution converges to a ground truth intervention distribution U and so that's a very kind of maybe a basian approach to thinking about this identification problem but uh you know I wanted to make sure with this paper that people who are used to thinking in terms of Laten variable models that you can model with a graph that the world of causal reasoning with these models is open to them as long as they can apply they can prove that what they're doing is valid using graphical identification and graphical identification say with the rules of the do calculus you know might be opaque to a lot of people but the good thing there is that it's algorith matized right so you can use libraries like in Python there's a there's a Library called y not Y is as in y and then the number zero the letter Y and the number zero um and you can say plugin a dag and some set of variables that you're observing in your data and it'll tell you true and false whether or not you can actually Identify some query of Interest given your data and uh and so you know that combined with whatever your favorite late variable modeling approaches you know now you can say if you you know using these algorithms you can prove that you're late variable model can also infer some uh uh causal distributions and causal queries so so I understand that the result of this paper the main result of this paper is uh showing that if we have a quantity that is identifiable using two calculus so we can produce an estimate that can be used to correctly identify causal effect in a given system then if we implement this system in a probabilistic programming framework we're also guaranteed to get to to good to good estimate of the causal unbiased estimate of the causal effect yeah and so for somebody who's coming from caal inference that's like not a big deal it's like okay great yeah you have a you show that you have identification and you just kind of construct a new estimator maybe using some Monti Carlo approach right but from uh if you're somebody who's used to working with these types of systems right where it's kind of very model-based approach you're thinking you know what's the right you know what are the variables in my system how do they relate to each other I'm going to implement this as a as a program I'm going to put distributions on all the variables in my model and if there are parameters I'm going to put priors on all these parameters and then I'm going to implement some sampling based procedure or some um approximate inference procedure like HMC or or variational inference I'm going to sample from a posterior distribution of the variables that I'm trying to Target and then maybe I'll take some of those samples and then I'll apply them to some Downstream function that is estimating some query that I that I want right that's that's the kind of basan or probabilistic reasoning workflow right and you know you're you're using probability to to represent your uncertainty about the system and um you know and and so uh and and you're being very explicit about what your domain your assumptions about D gener domaining process is I think people used to call this modelbased machine learning although I don't think that term is used very much anymore but you know so if you're Ma I know you had Thomas we Keon in the past like you know people who use PMC this is the way they're used to thinking about a problem yeah and the idea that you can then say uh use like an intervention on these models so like PMC now has a due um me due function that will take a model and then modify it such that it's now reflecting an intervention on that system and and so you start sampling from it and then you're like does this work is this is this you know just because my as anybody who's worked with say an mcmc based model work uh system will tell you was like yeah this just because you're getting samples coming out of it doesn't mean they're actually it's converged or that they're that they're U that you actually converge to the uh Target distribution you need to you need to evaluate the traces you need to look at the posterior predicted distribution all of these things that you need to make sure that your models is doing the right thing and so once you now have you start doing causal stuff to your model like then all those traditional beian checks they don't they're not meant to to kind of deal with this kind of causal problem but now if you just say all right listen here's an algorithm from that you can Import in python or there's a library in R if you're an R user um think it's called RCI yeah I think that's right and you just put in the thing that you're trying to estimate and and and some details about your model and it'll tell you yes or no whether or not what you're trying to do is valid and um and you know so it wasn't I think Earth shattering to people who are just who who just see the kind of model that you build and pymc or pyro as a as another estimator but for people who are used to working in those Frameworks particularly and and leveraging their ability to work in high-dimensional settings and you know using like the broadcast semantics of pytorch and stuff like that being able to model images and media and sound and video and stuff like that that you can work with some of these deep problemistic programming languages that's kind of a revelation that you can that you can now these these lat and variable models that you're training in your um uh that you've been training for a while can now be used used to answer causal queries talking about uh probabilistic programming a couple of weeks ago a new library has been released called Cairo or hero depends on how you decide to read the Greek characters and and this Library uh Builds on top of pyro you had an opportunity to work uh a little bit with this library and it helps to abstract certain uh causal operations on graphical models what were Impressions first off there's it's an amazingly Interstellar group of people who are behind this Library um and they've been thinking about these types of problems for a while when I teach about um say for example doing parallel world counterfactual reasoning using something like a structural causal model you know these are these are models where you have some exogenous variable it's like a root node in the model that is sampled from a distribution but everything Downstream of it that variables that actually represent things that you're interested in reasoning about are set deterministically given those exogenous variables now for those who are familiar with um the probabilistic machine learning whenever you want to take something that's set deterministically and conditioned by evidence you have some you know you have an intractable likelihood and so you have to spend you have to think about how to solve that maybe you're there's all there's ways to solve it but these are all now you're becoming like an inference engineer instead of a data scientist or a causo inference you know Reasoner uh or analyst and so um Cairo is is going in the direction of taking a lot of those uh those more difficult abstractions of CAO inference were difficult in a sense that they don't really quite mesh well with existing distractions for deep probabilistic machine learning uh or you know or uh probalistic programming or basian inference or B computational base whatever you want to call it and um making that kind of abstracting that away so that you don't have to really think too hard about that inference side of that of the problem and they're also I think they have a very interesting philosophy if you go into the tutorial um they they make some they make some statements that I I generally agree with things like um you know causal uncertainty is you know or they kind of draw parallels between being uncertain about the causal structure of your model and then just being a certain in a basian sense where you're kind of thinking about moding uncertainty with probability and that's and then vice versa where like uh uh uncertainty by your system can be addressed with making causal assumptions with the system and so like they they I think for people who have a who understand say got a probabilistic machine learning or a a a a basian probabilistic programming kind of um way of thinking about problems and have just seen causality is something that it seems vaguely related like there are there are graphs and there are and there's inference and stuff we're kind of wondering you know exactly where are they connected I think Cairo will be a wonderful library for these people to kind of sync their teeth and syn their teeth into uh still bit early days there's still they've just released it and so um they have some very useful tutorials on there now but you can you can also tell that it's really in a good place to you know for for for continued development MH what do you think will be the next big thing in causality I'm pretty rubbish at predictions um I could tell you things that seem to be uh that that have caught my interest and I think that if somebody solves them could be a big deal I um so one I think is probably the biggest deal like if somebody really just nailed this problem and and then and just put out a few papers that were really just solving it um will really put their name on the uh on the uh in the history books it would be CLE representation learning right and so I mean and this is related again to some of these probabilistic machine learning problems of like disentanglement and you know trying to learn Laden representations that correspond to you know Concepts in the uh domain that modeling um and so and and so what causality really adds here is to say like okay well let's use causality to say you know if if if we're learning some latent representation that corresponds to an actual kind of cause in our in our data generating process then we can use our our knowledge about causality to identify some disera for how this cause ought to behave like if we intervene on it if we you know certain things ought to be modular and invariant so and so and then you know then you can figure out okay okay then what kinds of practical assumptions can I apply to and a modeling problem such that we would learn we can we we can be have some some kind of guarantee that we would learn these types of abstractions I think that um that would have a huge impact if if for anybody who was who managed to do that well well in a both a theoretically and practically theoretically sound and and practically useful way um think Give an example would be like generative AI right have if you've ever tried using one like mid Journey or stable diffusion and you know you generate an image but maybe you you kind of want the head to be tilted generate some figure you want the head to be tilted 15 degrees to the left or you want the uh the glasses to be blue instead of red or something like that if you five fingers instead five fingers inad of you know octopus tentacles and and you go in there and you you imagine that you should be able to go to the prompt and just say I want this image you have here except you know red glasses instead of blue glasses right and it should just kind of work right everything should be the same save for the glasses this is a counterfactual question what would this image you know given that I observed this image with red glass or blue glasses what would this image look like if they were if there was uh red glasses nothing else should change except for the to glasses right or any you know or whatever my hypothetical condition is nothing else in this image should change except for things that based on my causal model of what I'm representing in the image would be causally Downstream of said hypothetical change right so if I say I want you know I want him to be wearing a pirate hat instead of a instead of a baseball hat his ears should not move around right or you shouldn't get all kinds of weird artifacts in the background but that's what happens now when you use these images and it kind it's frustrating right because you know maybe you're trying to get this perfect image and you just keep kind of jumping around the distribution of things that are close to what you want but not quite yeah and um now if you for example could you know operate semantically on those learned causal abstractions uh causal representations behind the image then in theory that should be a lot easier to do um I mean easier said than done um but you can imagine how like um you know in just that one case of of of of image modeling I think you know for all these AI startups out there who are trying to take um that uh generative AI for images or video and and making and turning them into a business it just the ability to kind of give people knobs that they can turn to make adjustments about the image nice think about natural language models like you know chat gbt and Claude and Bard is that when it generates something and it's not quite right you can edit it because it's text with images you can you can do like infill with pixels and stuff like that but you want to actually reason about it on a semantic level not in just terms of the form of the generated OB uh the generated artifact and I think for and that's one example for where more concrete causal abstractions because that's when we we when we when we discuss what it is representing representing in the image we're thinking about you know the image is representing some scene for example and that's and as we think about that scene we're bringing our kind of causal representation of the object in that scene to bear on our interpretation of that scene and so if we can work directly in that language that would be a very powerful um Improvement to this class of models um and so like that's just one example for where kind of better causal representations could I think have a huge impact in in in part in AI particularly in well I think in reinforcement learning as well um but uh uh especially in generative AI when we talk about generative models and and causality uh it brings to my mind this idea of trying to make large language caal and you have some experience uh with trying to do this can you tell us a little bit more about your your project where try to constraint the models in a causal way and what were the results I think this is an interesting space I mean there's a lot lot of folks now thinking about uh you know whether or not a large language models learn a world model and essentially what they mean is is you know a CM model at least in my view um uh that that in other words that learning that the large language model is learning some set of rules about the world and that it can Now operate on when generating responses and and in terms of parsing that problem I found causal inference to be quite you causal inference Theory to be quite useful in terms of like so we we mentioned identification right like yeah this for example using the do calculus to prove that your generative model can sample from an Interventional dist Interventional distribution and so that's we can extend that here to this case as well which is to say like you know we can if in the idea in the theory of identification there's this result called the causal hierarchy theorem um and so if causal hierarchy also called pearls hierarchy and and or PE Pearl's causal ladder so people here who read the book of why might remember it um but it says like you know level one is associational like observational statistics level two is interventions and level three is counterfactual counterfactuals essentially reasoning across worlds like this happened in this world had things been different imagine another world where things had been different how might things have played out right and and we know that in order to answer those questions from a based on this theorem you need assumptions that are the same level of the question so if I want to estimate a causal effect I need uh level two assumptions say for example in the form of a causal dag um if I want to answer that kind of multi-world counterfactual uh I need level three assumptions say for example in the form of a structural caal model but it could take other forms as well and so when we ask ourselves whether or not these or deep neural network models like uh uh large language models or other other models that are with Transformer architectures that are trained on a large amount of data whether or not they can solve these questions um well one one thing we can do is just ask them empirically right we can posee a bunch of causal questions to the large language model and see if it it can answer that and and in truth it can um if you ask it to to you know it does smoking cause cancer it'll say yeah if you ask it uh you know produce some kind of causal analys for you analysis for you in Python code using a library like DUI it'll do it and if you ask it to for some kind of counterfactual simulation question and say you know this happened and that happened would have happened if things had been different it'll it'll simulate for you um but of course we know that these models quote unquote hallucinate in other words that sometimes they will say things as if they were true that are not true and so the question is uh how do we you know can we ensure or or bring some kind of uh can we understand when that's going to happen can we understand even capable of not doing that that is there some way of curing Hallucination is that possible uh you know is there some way that we can bring kind of causal theity to bear on on large language models such that this that this problem is reduced and that we can actually C answer causal questions like this reliably and and there's a lot of angles you can take here kind of back Y what the causal hierarchy theorem is telling us is that you know if you're going to pose these counterfactual questions to the uh large language model reliably and and get or get reliable assumptions sorry get reliable answers reliable Generations then it must be it must be using level three assumptions those assumptions has have to have to exist somewhere maybe they're somehow included in the training data which given how we're told these models are trained um we know it's not true or how we've you know then based on the open source models we we see trained we know that's not true could be in the model architecture itself could be some somehow learned in the parameterization of of the trained model and or it could be in the prompt right now if it's in the prompt we know it's in the prompt if it's in the architecture or you know somehow in what in the parameterization um that the model has learned we don't know if it's there un th we can't prove or validate or ensure that we have this kind of reliable behavior and so um you know it could be you could do something like have some kind of causals good Goodman AMS buman how to pronounce that word HS busman okay some kind ofman hman some kind a causal validator in the uh decoder of in the um decoding of uh represent ations into into tokens when you're generating from your model um uh in other words that's somehow it's checking what's being generated and making sure that it's you know valid According to some Criterion I've been looking into ways of trying to incorporate uh caal information into the structure of the model of the of the Transformer architecture itself such that uh we have some for the same reasons with the uh paper that you mentioned where with the with the latent variable model we uh get some theoretical guarantees from things like the do calculus we would have the same types of guarantees based on the structure of the uh of the transfer architecture based uh model itself and so I have so so anybody's interested in that uh there's a notebook circulating online I'm sure you could find it if you Google around with a very simple toy model where I'm showing how you could you know and the scenario here I'm imagining is that if you imagine that you're uh a production company and you have a bunch of scripts that were all kind of using some kind of software that in that uh it's like you know making films for example that all enforce some kind of event uh some kind of act one act two act three type of structure and then you can kind of say well Act One causes act two and act act one and act two cause act three and and then showing how if you train the model using this structural knowledge you can then simulate outcomes for act three that we're not in the training data and and it's all in natural language and so um right now I'm trying to build that into a much more practical example and train it on some kind of heavy duty models and so yeah that's that's the way I'm taking to uh address this problem um but I think it's it's a if we can you know again I want to emphasize that it's not just about you know guaranteeing that it's only saying causally correct things it's to me it's more think thinking a lot in terms of like the uh the knobs that I gave in the generative AI example like you know can we you know where generative AI is useful in so far as it kind of augments our own creative process right and so it's useful useful for other things a lot of bad things but you know the one that the use case I'm interested in is is how as we as creators um Can leverage these models to you know you know charge our you know hypercharge our creative abilities right and so the more you can make sure that the models are aligned with how we're thinking about the abstractions and the thing that we're we're writing about or or or drawing or or producing um I think the better and so I think this is part of an area of research a lot of that coming from um you might see from papers coming out of Microsoft research where you know if we you know right now there's a lot of emphasis on scaling these models up to ever larg sets of data and I think we're kind of you know there's only so large that the data could possibly get right even if we continue trading them and these models are generally undertrained surprisingly but we can but but there's still a big emphasis on getting more data and pumping them pumping them into these models and if we can go in a different there's there's an interesting area of research that go goes into a different direction which says like okay well let's freeze the data size and let's figure out how much power we can get how much we can improve the models by um trying to develop another directions and so if if for example you're trying to train models more efficiently with get say GPT 4 level capabilities with a smaller data set that opens up a lot of possibilities say for example you can now leverage structure already like innate in the data right so one example I give is you know with GitHub co-pilot it's able to simulate good production quality or production quality it's able to good simulate kind of good uh code and it helps you know uh developers kind of avoid a lot of the boilerplate of coding but we're still generally just kind of feeding it uh kind of disembodied individual documents of code right now you can imagine say like you know there's a lot of structure is being missed this is a repo here's the history of the repo and you can see here the the development of the code through all the throughout time maybe if the person is a is is is good at at doing git then you see very informative git commits that say why this change why this change was made right and um and then of course you could say even compile that uh code into uh an executable and then maybe even provide examples of inputs and outputs of that uh of the ex utable right and all that that you know that's a structured system imagine now you could uh um figure out how to tokenize all that and and and feed it into your Transformer model or somehow you know incorporate and force the architecture to reflect that structure right and so while you know our our our foundation models so far have been focusing on kind of breath this gives us the example to to uh leverage depth and hopefully kind of Leverage more of that stru information some of it cause other there are other kinds of structure hierarchical structure for example that would um that we could leverage in this type of analysis so that's kind of where I'm hoping uh that's that's an area of my research and I'm hoping we see development as a community what were two books that change your life Darren Wilkinson is a professor in the UK he has this book on uh computational systems by ology and it really it has both kind of a really good broad introduction to computational Bays so various kinds of random variable simulation algorithms as well as inference algorithm so like you know from from import important sampling to all flavors of mcmc and then the other half of the book is uh how do we build a computational model of a Dy of a dynamic system and the focus is very much on on biology which is great but there's no reason it going to be on I don't know marketing or or economics or you know agent models and you know I think I you judge an impact the impact of a book on your life by figuring you know by thinking about how often you go back to to to think about the principles that you learn from that book or how often you cra you crack it open that book has been you know super impactful in terms of just how often I've just been leveraging those ideas and probably how I shaped my research too um and I don't I no longer work in biology but I still often turn you know you know it wasn't not too long ago I was building a kind of computational model of advertising it was it was very heavily influenced by that book um so I think that's one another book pick any of these books that now um I want to say Hadley Wickham wrote a book on the r language and it was very much influenced by another functional programming Paradigm and book books like structure and interpretation of computer systems which I think you know people who have a computer science background like maybe their experience with functional programming might be something like sicp or or say a book on I don't know uh closure or something like that um but you know me being you know I did my PhD and statistics and so me having been trained in r as a statistician just was kind of my first foray into this world it really kind of taught me you know it was basically my class as a statistician you know what a computer scientist in in graduate school if they took a class on um the design of programming languages uh design and implementation of programming languages um uh this book was maybe my introduction as a statistician to that uh field and I don't think it was specifically designed to do that it was just that the philosophy that he was bringing the bear really came out there and um and so and and that led to a bunch of other Explorations in that field which I think really impacted me as a modeler who would you like to thank my wife for putting up with uh a lot both in terms of uh all the sacrifices it takes to kind of become uh somebody who does what I do um I'd say my adviser for sure from my my PhD adviser oga Beek who um you know turned me into a respectable person essentially um and I'd say Karen Sachs who is another um researcher who's had a huge impact on caal Discovery in particular I met her in the middle of my PhD right when I was kind of finding my footing with my my topic and my dissertation topic and I think had yeah she that relate my relationship with her both kind of professionally and as a friend um you know really um shaped me as a researcher got me through my PhD and kind of gave me kind of source of inspiration and kind of and and guidance at the time when I really needed it um U yeah so I think uh I owe a lot there what would be the most uh precious thing that you got in your life from somebody else I don't know or or just from from from from the Fate that helps you in your career learning you know the power of routine and habits right I think maybe particularly for um yeah Millennials like myself like you kind of you kind of start start off thinking like you really need to find something that motivates you and that you're really excited about but of course motivation and excitement is unreliable as a source of fuel to kind of keep going in the directions you want to go sometimes you're just not feeling it sometimes you hit a you know you hit a kind of emotional Plateau or even a you know a valley and you just don't feel motivated but if you have you know the right habits and routines in place then you can keep at it and you it becomes something that you can rely on especially in those times when you maybe are feeling a little bit unmotivated when I think about what I want to like pass on to my son if I I you know if I have one thing that you know when he's in his 20s I'm hoping that he has learned and picked up it's just the power to be able to design their own habits and routines and then you know essentially U set them in place and then put them on you know autopilot where can people find more about you and your work sure um you can always uh find me on Microsoft researchers website I also uh teach online at um alt deep.

and there's also links there to a GitHub repo uh so that'll be the repo is alt and deep SL caom ml where you'll see a lot of free tutorials and jupit or notebooks on stuff online for a lot of the stuff that I teach about um yeah those are my two main kind of online places to look I I try to keep my research Pro oh I'm on LinkedIn yeah you can see me on LinkedIn people should add me if they want to talk um yeah so those those Street places Al deep. AI linkedin.com and Microsoft Microsoft research is uh you know profile Pages I'm not sure what the URL is before we before we wrap it up um what it be your advice to people who are just studying about causality and they're thinking about the research path so you're thinking that these are people who want to like focus on causal inferences a research field themselves or they kind of maybe they're engineers and they're trying to find apply causality to a problem they they're wanting to work on or so let's think about people who who' like to become researchers in in the field of causality Let It Be inference or structureal learning or representation learning whatever the whatever the field I mean it sounds maybe maybe a bit obvious but it would be to uh try to well for one maybe find a good you know book work through it um and there's a lot of candidates for that but I think uh and so I kind of at least you can kind of understand what the shape of the territory looks like um and you know because you once you know the shape then you can go find out where the frontier is and figure out how to push against it right and I would say um with resp you know once you understand kind of where the directions are that people are developing then you want to find out who are the people in that place you know who are making an impact um and you know so they'll be giving workshops at conferences and and uh maybe doing podcasts and and so then you um uh you look at their papers and figure out who they're citing and get a feel for kind of what the cutting edges there and then um you think I think it's important to think a lot about um connecting with those people during one's training right whether that's a PhD or somewh or something else you want to be in proximity to um that network of people and so I think that's you know true of anything right like if you want to work in finance you should probably live in Chicago or or or New York right if if you're American or London maybe if you're in Europe or something like that but like you don't want to work in Nebraska um because because you want to go to where the people are who are also doing this thing I mean I think in in um in research is probably a bit more to some extent it's less about being in a physical place but um although sometimes it can be um but really just figuring out kind of you know what the uh who are the people there who are trying to have an impact and then um and then figuring out how to develop you know from that very kind of person that personable way as opposed to say for example like just picking trying to figure out what the most influential institution is or figuring out you know how can you go get the you know I think a lot of people these days think about getting as much Prestige as possible on their CV and listen Prestige doesn't hurt obviously but it's you know unless you want to work in some crowded domain where you know everybody knows that if you're around a bunch of prestigious people then you know it tends to be good so like those T you know so there's a lot of competition with that strategy so about keeping that in mind focusing on the people who are doing doing the things that you actually care to do I think uh that's uh that's that's the advice I'd give is the future causal I mean the past present and future or causal right sure yeah I think I think these questions aren't going away I don't think for example that's you know we're going to invent some new deep learning architecture that's going to make all these questions obsolete I do not believe that that will happen thank you Robert it was a pleasure thank you congrats on reaching the end of this episode of the caal bandits podcast stay tuned for the next one if you like this episode click the like button to help others find it and maybe subscribe to this channel as well you know stay caal