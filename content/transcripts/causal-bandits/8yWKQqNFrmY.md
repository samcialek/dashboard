I think for agents it's it's kind of very new but my my thinking is that causality will start becoming very important uh because agents will start interacting in the real world right or even in the web world they're interacting with other people and so now these notions of action and reward and sort of what is behavior you want to optimize what is behavior you want to penalize all of these are at their heart causal abstractions Hey, which agent is the most useful here? Which agent should be rewarded more? Or count of action notions of blame? If there are agents in a system that are not doing well, you probably want to remove them.

We have some very interesting papers out there about agents and causal modeling. There's a paper from uh from Riches and Everett uh guys from from Google Deep Mind who presented a theory of how agents can learn uh robust world models. How do you think you mentioned counterfactuals these ideas are mainly about interventional distributions? So making an intervention in the environment and learning the response. How do you think about causality and those new questions about causality that you think might emerge with with agents? Someone else did experiments of these different graphs and gave us this aimatic data about it and we are just learning from this passive data to build causal reasoning.

they will find statistical shortcuts and maybe that's what they're also doing for this aimatic training because remember I we observed that for let's say graphs of length 10 they do well but their performance does taper off as graphs of length 14 appear Hey causal bandits welcome to the second season of the causal bandits podcast the best podcast on causality and AI on the internet he believes that data tells stories and he wants to tell causal ones with his research. He sees causality as a goal, even if we don't always know how to achieve it.

Open-source contributions to societal impact and back. Creator of DUI and principal researcher at Microsoft Research India. Ladies and gentlemen, please welcome Dr. Amit Sharma. Let me pass it to your host, Alex Mo. Hi, Amit. Welcome to the podcast. Hi, Alex. Thanks for inviting me. Thank you for finding time for this conversation. And I'm super excited about this one partially because my adventure with computational causality really started with with DUI. DUI was one of the first computational devices or libraries that that I used when I was in the beginning of my journey.

And you are for those people in our audience who don't know this the original creator of the Y. But before we move to a conversation about the why that I would love us to have today, I want to start with something else. A few months ago, all the hype was around large language models and today it's agents. Yeah. Yeah. What is your thinking about causality in the context of what is happening at the forefront of of machine learning or at least machine learning as as we can see it in in the more let's say popular media.

Yeah. So I think it's been so fast that sometimes it's hard to step back and and sort of form thoughts. But I can answer your question in in two ways, right? So at least I have a better understanding of LLMs and then we can go to agents. So I I think what LLMs have shown us is that causality was always requiring two things domain knowledge and then discovery and reasoning. And I think our field spend a lot of time on the second one and given some data set how do you get good graphs and then reason over them.

But we kind of left the part about domain knowledge very vague, right? That someone will come and and give the domain knowledge. And what large language models essentially showed us is that there are ways of learning that domain knowledge. And for a lot of cases, if you're able to get that domain knowledge right, you may be even able to do reasoning in in many situations, right, which do not require you to have data. So that's how I think of at least large language models, right? whenever people show results and say, "Oh, amazing.

This is reasoning." Right? So, I still think it's a lot of domain knowledge plus some sort of ex generalization that's enabling it to do what appears to be uh reasoning. But the the lesson is that for the future of causality and the future of causal inference, we have to take both into account, right? and think about how do we also supercharge our methods for getting domain knowledge correct and inserting them into real world algorithms. I think for agents it's it's kind of very new but my my thinking is that causality will start becoming very important uh because agents will start interacting in the real world right or even in the web world they're interacting with other people and so now these notions of action and reward and sort of what is behavior you want to optimize what is behavior you want to penalize all of these are at their heart causal abstractions So maybe in the training of agents it causality may not appear as much.

It may just be plain RL where you just have some rewards in mind and you're optimizing them. But I think the moment they start interacting with our agents or they start sort of doing stuff collaboratively. We'll start having to grapple with questions about hey which agent is the most useful here? Which agent should be rewarded more or uh counterfactual notions of blame. If there are agents in a system that are not doing well, you probably want to remove them uh or improve their uh functioning over time.

So I feel in in the future especially with agents there'll be new kinds of causal questions that will come up. I think perhaps a little bit like social science but I think a lot more understanding about designing these systems from the grounds up so that they behave in the ways that we want them to behave. So that I think would be one critical part in which causal thinking would come. Um and I think maybe some training algorithms also that might be motivated by that. We have some very interesting papers out there about agents and causal modeling.

There's a paper from uh from Richardson and Everett uh guys from from Google deep mind who presented a theory of how agents can learn uh robust world models. How do you think you mentioned counterfactuals these ideas are mainly about interventional distributions? So making an intervention in the environment and learning the response. How do you think about causality and those new questions about causality that you think might emerge with with agents? Yeah. So I think those papers are some of my favorite ones.

Uh but actually they are kind of answering a different question which is that if you don't think about causality how can you still learn some principles or how can your behavior mimic what an ideal causal agent would have done. Um and I think it's it's quite instructive to sort of understand the import of those papers to be able to also understand how causality can now help in this new sort of ML future. My learning from those papers was that there are different ways to learn how to act correctly in a system, right? Uh at its core abstraction, right? Causality is about making the right actions that would optimize the right outcomes.

And what they showed is that if you just have enough diverse environments for an agent to operate in, even if that agent is just trying to do well in all those environments, that's maybe that's only their objective. They have no knowledge about a word model. They have no understanding about anything else in the limit like they would actually start doing actions where that would mimic something that a true causal agent would do which had the world model with it. Right? And and they also have some interesting sort of theoretical abstractions which can say that you can even recover the world model if you just observe their actions.

Right? So if you just observe what they're doing uh it's possible to understand the world as well. So to me I think these are new results that are emerging which are just showing us the power of data especially diverse data. There's a bunch of other work also from Schulov's group which talks about exchangeability and how those just the fact that you now assume you have exchangeable data which is similar to data from multiple distributions. Uh you can now start doing much better in discovering causal relationships.

So I think it's a very promising trend in thinking about a new kind of imparting causality right like what are the diverse domains or what are the diverse ways in which I want to expose my agent so that by itself it's going to learn what's what's the optimal relationship between variables that's the one that helps it do well in in all the environments and I think this is again sort of pointing out to what I was saying earlier right like in my past two years I've just spent thinking about what is it about causality that has changed now or what is it about causality that I didn't fully understand before LLM and I think the the one answer that I can give it is a very clear answer is that there are multiple ways to reach a causal agent I think prels there was some sort of a consensus where the idea was you have to think about the right mechanisms you have to think about the right graph you have to have the right biases inductive biases into the model which are derived from causal principles and that's the way to get causal actions right but now I feel many results are showing like now there's clear evidence that there are other ways as well right so one of the ways is just collect a lot of diverse data and as long as you have done a good job um of course there's no way to verify so it's not a very intellectually satisfying exercise but if you've done a good enough job of collecting enough diverse data uh we have both empirical and theoretical evidence.

Now that you can actually generalize and you would be able to learn at least the causal actions even if you have no idea of of the causal model and then there are other kinds of thought experiments that you can also think about which is another way of looking at causality is just generalizing from a set of rules right so if you think of let's say rules in physics this is how many LLMs today appear to also answer causal questions is because they've read books which have told them theorems in physics And so if they if you ask them to let's say predict the velocity or momentum of an object given some kind of input state one way of solving it is to go from first principles discover the relationship of how objects move maybe discover the Newton's law and then solve that problem.

The other way is to cheat and just learn from what Newton has already told you and just apply that equation and again get the right answer. Right? So I think that's the other way of getting causal actions is that you just build your foundation on domain knowledge or you build your foundation on many theorems and many axioms that have been proven to be true in the world. And so even at test time if all you're doing is applying a bunch of variables in an equation, it appears that that your actions are consistent with what a causal agent would also have done, right? And so I think that's that's perhaps a summary of of my reflections on this topic that I think there's maybe a third way as well.

We don't just don't know like there are many different ways of reaching good causal models. Uh data augmentation and domain knowledge definitely are have been shown but I'm now looking for more ways to do this uh in a scalable way as well. There's also a question if if what what you call cheating here quote unquote is is really cheating. Right. I I remember our conversation with with Andrew Lampin and also from Google Deep Mind where we were discussing the the idea of behavioral cloning and this is also how we as humans learn right we also read books we also observe our parents as children we learn how they act in certain situations and so that's certainly a way to also speed up our learning to learning some valid strategies at the same time we of course know that there is no guarantees in behavioral cloning because there is no guarantee that the agents that you are observing are actually correct right or optimal I said cheating because it's kind of having access to some theorems or past information but a very nice thought experiment is regarding the gravitational constant now of course you and I know that it's different so it's different on earth it's different on the moon but now imagine if we are in the classical causality world I'll give you a lot of data from the earth right and maybe I give you a little bit of data from the equator I give you a little bit of data from the uh poles and there's a slight difference in gravitational constant there as well.

So in the classical causality what we hoping is by giving you enough variation that I can on earth and let's say I'm telling you the mass and weight of every object in all these places. I'm hoping that you're able to discover that there is some g which is the division of weight by mass and that it varies across in different parts of the earth. So when I take you to the moon and I ask you to do the same thing, you would perhaps do some more experiments again learn the G and and get the answer. But this is a much difficult learning process, right? It's most likely many models might just learn G as the average of whatever is on the equator and the poles and then when you go to moon they'll just apply that, right? And they might fail.

So I thought in the classical causality it was very ambitious and we were asking it to do a much harder task of generalizing from this knowledge whereas in this new world models are capable I would just perhaps teach it the gravitational law and say there's something called G. You have to look out for what that G is and G varies by planets. And then now that you know this information, not only you can do a better job at Earth because on Earth you can kind of use your statistical data to estimate it, but the moment you go on the moon, you would also suddenly realize that your stat statistical data is not enough.

You'll just make maybe a few more observations on the moon and and be done with it. I guess that's how humans learn. So it's it's a much more attractive way I feel of trying to get to those the dream of a causal model, right? or a dream of a agent that can act causally. Uh maybe it's not cheating. Maybe it's just a better way of structuring the initial knowledge that the model starts with so that it doesn't have to do really hard tasks to do well like it can actually know some of the context and adapt easily.

What do you think the contemporary agentic frameworks are missing the most today? It's very early. So I think a lot of there's a lot of things missing. I'm just thinking about maybe I'll say verification is is what I think is the biggest thing missing. So most of the contemporary agentic frameworks have been built in I would say slightly well controlled areas like coding where the there are verifiers like the compiler which will tell you whether you're good or not or in terms of calling APIs where again if the API fails there's a very clear signal that the API has failed and so agents can adapt and then they can backtrack and do something else or there are cases where the agents are just told to write a research report on causality in the last 5 years and they do something and then the user at the end just says yes or no, right? And they would give some feedback.

I think what's really missing is how do you take these kinds of frameworks and apply them to these vague problems in the real world where uh maybe even humans cannot give good rewards to to these and anyways I think that's not scalable right so we need some way of creating verifiers for new domains maybe just for the outcome to begin with but then also starting to maybe give some kinds of rewards for different actions that the agent is taking right so I'll give you two examples like one is very simple the other is like the cutting edge of what I think causality could do very simple example would be like summarizing a meeting right so we are having a conversation there'll be a transcript uh we can imagine agents to or even LMS to kind of summarize that and there you can have all these kinds of different rewards like is it precise enough is it recalling all the important stuff what is important to Alex what is important to and and so on right uh so even here you see that there's so much of verification that needs to be done that if done well can only help you improve the output but if not then I think the agents can go haywire and I think the more kind of harder example is in doing science itself now suppose you have agents which can generate literature reviews uh they can read the literature the next step would be they can suggest experiments and then you would want to start thinking about how to give feedback to these agents so that they actually start designing better experiment experiments uh and and so on.

So thinking about verification in a more holistic way and like how do you create verifiers for all these kinds of scenarios I think would be a big leap right because of once you have that you can also train better ver agents you can also test them at test time but I think it's kind of a art and science where based on each domain you want to think about how to what properties need to be verified can you actually create some kinds of causal systems that give you necessary properties to prove move uh in that world.

Yeah. So I think that's what I think is missing right now. Verification is also one of the very fundamental problems in causal modeling. Yes. Yeah. In in do I always had some ideas on how to right verify models maybe not directly verify but how to try to falsify them. In recent uh I think two years you added more modules to DUI and economl that can help with with these tasks related to assessment evaluation verification. Can you tell us a little bit more about this and what ideas are at the at the base of of those of those solutions that you proposed in the library or libraries? Yeah.

So I think uh for causal inference uh it was more of a clear problem because you would get some estimates from observational data and without doing a randomized control trial there was no way of knowing how how good they are. Right? So at least I think in the agentic world the problems are simpler. So you can imagine having some simulator or some causal model that can at least verify what you're doing. in Dubai her question was how do you evaluate this causal simulator who knows whether this causal effect that you've got who knows whether your graph that you've got is is correct or not and to me I think when Embry and I were building Dubai I think the first three steps of the pipeline were very clear because I would read Paul's book and he would clearly say that it's not just about estimation it's about thinking about the graph it's about thinking about identification and all of those things can happen without the data.

So the first three steps were clear right for this last step which was reputation which as we finally coined it we had to actually look to how economists are doing it. So when I was at Microsoft research uh in New York as a posttock, I had the good fortune of being in a lab which had computer scientists but also economists and also a lot of visiting economists who would come and sort of I would get a chance to talk to them. And I realized that in the the way studies are evaluated in economics and social sciences in general is that you present something and you go seven hours and then a lot of good researchers will give you feedback.

Hey, have you thought about this? What if this part of your cause and model was wrong? Hey, have you done that robustness check? And that you should really try to check whether your placeos are really placeos or not. And it was only after a bunch of such sessions have happened that you would finally write your main paper and kind of submit it and do all these robustness checks right and so that's what we have tried to include in DUI since the start. I think initially we tried to include a sensitivity analysis which are simply just thinking about what happens if you add had missed some unobserved confounder.

Slowly after that I started moving towards more determinist not deterministic but like objective quote unquote tests which would give you a yes or no answer like these are like significance tests and here I drew a lot from biomedical literature because they had this idea of negative controls. The idea was very simple like the moment you find something amazing in your lab experiment. You want to make sure that it was because of the reagent that you think is the cause and not because of something else in the lab at the time.

Right? So negative control would simply mean you would remove the things that you don't think matter, redo the experiment with the same reagent and still see if you get the same result or not. Right? So we kind of ported it back to causality. So we created situations where I can add a random common cause to your analysis, right? And so that's like something that should not really change or I can replace your treatment with a placebo. This is also something you can do automatically because you can there's a column called treatment.

You just replace it with a random variable. And I think more recently what we've been trying to do is to marry these kinds of ideas with real data so that the tests just become more and more powerful. So in one of the methods that we added, we created this notion of a dummy outcome which is very similar to a placebo treatment where the outcome gets randomized. But now you can do something special where you can still have the outcome as a function of the confounders but not as a treatment. And actually this is very simple to do because I can cook up any function that's a function of all the confounders and call that my new yi.

And by definition I know that this is not caused by treatment. And so now if you create such a dummy outcome you should again get a causal effect of zero. And so we start seeing that now you add these kinds of tests now like some of the methods especially at some lower sample sizes start to struggle. And uh some of the more recent tests actually have been added by contributors of of 2y. So there's there's one test which tests the overlap of uh the treatment across different confounding variables and tries to give you some sense of like where you can actually trust the estimates and where there's there's little overlap.

So that was done by Michael Oust who's was a PhD student at the time at MIT. Um and I think some of the more recent tests have also come from uh a team in Amazon who are now collaborators on Dubai as well. And they've been going very hard at this problem of verifying the graph itself, right? Because so far all that I talked about was verifying the analysis. Maybe you made some errors in the estimation or the model that you're using. Uh but ultimately it all starts from the graph and and so if you got some wrong assumptions in the graph well like good luck.

Uh so what we had initially in Dwayi was something very simple. We would just test conditional independences right we could say that hey if the graph has the structure then there are some conditional independences that come out naturally. So let's just test them right. uh but I think uh Patrick and others have built now much better statistical tests which try to compare the information that the graph is giving compared to the information that a completely random graph would give and that they sort of try to give you confidence on how informative is the graph that you're getting and how well supported is it by the current data right and so I think yeah so Dubai over the years I to be honest like uh is now I think of it more like a community project.

So I think when I started it was more of this initiative to kind of create the right API get the right functions in uh but I think now much of the interesting or the more recent work that you might be seeing in DIY is really like great collaborators or contributors who who have just built great methods and then they've included them in DIY. H how has it all started with with with Doy? What was the what was the the breaking point for you that you know you you decided and then team up teamed up with Emra to create this library and make it public? Yeah.

Well, it was it wasn't like something that we consciously decided. I think maybe going a little bit back uh this this all started I was working on causal inference in the context of recommener systems. uh for my PhD and also for my posttock and at the time I was very new to the field uh and I remember I would spend days reading Yudia Po's book I would also read potential outcomes from statistics and and other books and it would be really hard at times I after I think a few months I finally understood that what these books are talking about are very simple concepts.

Uh, and by the way, I'm talking about Paul's causality book. Like the book of Y was still not out. So it was it was the book that it's really hard to read for a beginner. Um, so I think I had two realizations, right? So one was that the concepts are very simple. The concept of a back door and controlling for variables is a very simple idea but they are presented in these books in a very formal way and that makes it very hard for people to understand right and it was pain for me to also go through and finally do my some of my experiments that I wanted to do.

Uh the other part I was also seeing was that the potential outcomes framework and the graphical models framework when I entered the field I got a sense that they are very different and at the start you have to choose one and then one of them is better maybe uh but as I read them I realized that they're the same I mean no okay they're not the same but I think they're solving the same problem but with a different lens and so in some sense they are compatible like if if you're thinking of how to frame your problem.

Graphical models are great because they give you a interpretable way of communicating your assumptions. You can do do calculus and back door on them. But the moment you turn to estimation once you found your estimate, there are some very nice methods from the potential outcome literature, right? The propensity score based methods came out of the potential outcomes framework and they are excellent ways of actually working with data in estimating things. So I think it was really the sort of the combined sort of confluence of these things where I felt like if I'm doing a next project on causality, I don't want to go back to these books and kind of grapple with them until I find the right estimate and then sort of reimplement some estimator on my own.

I thought like what if what would it be great if there's just like one tool that helps you do all these steps in at once. So and then I contacted Embry and I think he was facing similar problems. he was also working on causal inference for longitudinal data and so we thought that yeah I mean let's just do So I think initially we were just thinking of doing it for ourselves like we wanted something that is reusable for us and then of course I mean we open sourced it and then I think a lot of people liked it.

So that's that's how it all started and and so what's next for DUI? So two uh great things happened over the last couple of years right one was the collaboration from Amazon which made us realize that hey it's not just about a bunch of methods that we've put in maybe this can become bigger right so we created a new GitHub or called pi uh which is kind of where do sits now so it's not no longer associated uh under the Microsoft account I think it's now a much more global community of people so I think one of the things that has happened through it is that we are trying to build a causal ecosystem of solving any causal problem right so Dubai was focused on effect estimation I think then we also did root cause analysis but nobody starts with a graph right maybe you start with the data and so you need discovery so that's where causal learn which is a package from kung's team he kindly agreed to also sort of make it interoperable and put it under the py organization and then econ of course uh from MSR our new lab they also collaborate with us heavily and so they also came in so I think one of the next directions for us is to make the process of causal inference with data as seamless as possible so that you don't feel as if you're interacting with doy for this project you're interacting with causal learn for some other project you you just have this pi y if I may and then you just interact with that and you say what I want to do and these libraries under the hood talk to each other and help you execute that task right So I think that's the first level of vision where we're trying to bring these together and build some notebooks to show people what's possible and basically you start from your data and then you get a notebook that combines the analysis across different angles.

The second thing that I'm very excited about is a new library that we have started called PY LLM which is like the higher level abstraction of the same thing. Right now of course it's it's basically just showing that LMS can do causal discovery or in the sense of getting domain knowledge for your problems. But I think the way the direction in which we are going with that is that this makes it now even possible for doing causal inference for non-experts. Right? So think of citizens or general public right if I have a question about let's say some medicine right and my doctor has recommended it and I just want to understand what's the outlook in science about it or actually that might be a much more serious question to ask I don't think we're there yet but maybe something more simpler like you you're trying to understand uh what's the effect of some kind of intervention in schools and then how how does it help affect your own learning what kinds of things you should learn and so on right now there's just Google right or maybe Now this chat GPT as well but what if we could make a much more causally aware version of a system like that right and so that's why that's why we want to move PY by LM2 where we are solving causal questions but the input is text and the output is also most likely text and within the loop we are using all these advanced libraries and all these advanced tools that we built in causality and so for questions like what should let's say be the optimal coffee for happiness, right? Maybe there's a way we can look at all the research papers and give a summary that maybe doesn't matter that there's there's enough evidence on the contrary to both sides.

And maybe for other things, we would say that oh, how do I optimize the faces in my factory so that my factory never has any faults right in that case maybe would come back and say that you have to give us data and then you give us data and then we do all that analysis for you. Right? So I think that's the kind of dream of Yiy in in the future. First is to enable experts to do the analysis that they want without having to think about which library to call. But I think ultimate goal is to really help anyone sort of engage with causal questions.

And if you if you remember like it comes back to the first question you asked right that I now have a much more broader view of causality right so if someone is asking a question about let's say coffee and health probably it doesn't require any data right all it requires is someone some agent to go look at all the research papers provide a good summary to someone and provide it in a very accessible way that that's one end and the other end is some scientist or some industrial operator who who finds a lot of faults in their let's say energy system and so they're thinking that hey I let me ask this this py agent why are there so many faults in my system right can you help me and then of course you'll have to give us some domain knowledge you'll have to give us some sensor data but that's the only input you give us and the ideas that everything else helps you get answers to your questions are there any particular bottlenecks that you think this integration with LLMs could address I think the biggest bottleneck is the graph like uh that's the overwhelming feedback we got for Dubai is that DIY starts with a graph.

How do I get a graph um for my system? And that's true. I mean except for like some situations like I talked about this energy system, right? Maybe there you have a graph because you've engineered the system. But in most cases, yeah, I mean it's it's really hard to get a graph and it's really hard to convince even yourself that your graph is correct. So I feel that perhaps is what we are focusing on in the short term is to really make it easy for people to generate plausible or credible graphs for their problem.

And the other thing that is also a bottleneck is in kind of thinking about ways to test your system. So like I was giving the examples of negative controls, right? Of course in Dubai we can simulate them but DIY also allows you an option to to specify which variable is a placebo or which variable is a dummy outcome. Now again I mean in the real sciences it takes an expert to say that oh actually this outcome also happened around the same time but it happened in a different city. So I can call it a dummy outcome because it has all the same confounders but it doesn't have the treatment that I'm looking at.

Right? as we are finding that LM are also good sometimes in giving these kinds of suggestions on what might be good reputation tests to run. So I think in the short term we're just focusing heads on on just these two problems. So one is how do we make the process of creating the graph as simple as possible which means that the LMS will give you a graph that you can critique as a as an expert and then feed it to doy. And the other thing we are looking at is again the second most common feedback perhaps I've got about DUI is that you guys have so many reputation tests I don't understand what they're doing and I don't know which one to trust because sometimes one of them works one of them doesn't work so what do you do right and so even there we are hoping that the broader P LM could actually guide people towards the right tests to run and interpret uh what they need and hopefully build stronger tests for for those problems.

So, so I think that's where we are really going towards uh in the future. So, that's very interesting because what I also hear from my clients and prospects in industry is that LLMs are really useful in making the process of building the initial graph uh much faster. So, they help streamline uh yes streamline the process. That said, LLMs are still limited mainly by the fact that they hallucinate and they hallucinate in ways that are often very very difficult to predict. So they might hallucinate in a case of something very complex in in the case of a very complex task.

But sometimes they can also hallucinate in the context of a very very simple task that would be obvious for any human. So full automation uh seems not really realistic today at at least in the context where accuracy matters or or the cost of error is high. But when we think about LMS and causality, we can think about using LMS to streamline the process of causal modeling. But we can also think about ways how to use causality to make LMS more robust. And I know that some of your research with uh anat for instance focuses focuses on this idea.

Could you share a little bit with our audience about this this line of research and what do you think is important in this line of research? Yeah. Yeah. Definitely. Yeah. So I sort of keep straddling between these two spheres, right? So one is how machine learning can help causal inference and causality. So that's where DUI and I think a lot of the reputation tests come in. uh what you're pointing to is the second strand which is how can causality help uh machine learning. there. I think u I've over the years learned that it's a really sort of a hard question to tackle because often what matters in machine learning is like some accuracy on some distribution whereas what often causality targets is accuracy on some worst case distribution which may not happen right or it just happens very rarely.

So this project with Aniket Vashish one of my research fellows here at MSR India uh and also Vinit Palasan and also a colleague. So here we were looking at uh how so it's clear we just talked about how LLMs or AI can help causality right that was very clear now how how do we reframe this question how can causality help machine learning or or LLMs right and so the previous constructs that we had that oh we could add a regularizer which is informed from uh causal science was was not really making sense here because there was so much data we we don't even have control of of all the data and how these models are trained.

Then I think there were some other questions about world models like we can build world models in these systems. But even that felt out of reach because the world is very complex right now. These systems are no longer talking about a certain small world. It's the entire world they're modeling. That also felt out of reach like there's no way I could build credible things that you could add to these systems. So I think the the last thing that I felt could have a future in this new era is the fact that causal reasoning just like any other formal reasoning system is built on some axioms.

Right? One of the reasons that we like causality is because it's very predictable in the sense that if you knows the graph, you know many things just from the graph. And that's because we have rules like desparation or we have axioms like the transitivity axiom in stable causal systems and so on. And so to me it felt like perhaps what we can embibe in these language models is more robustness by enabling them to learn just like causal systems work. I think the shift in my brain was like not imparting causal knowledge to these systems because that's too complicated but imparting how to learn causal knowledge in in the first place right and maybe if that pro of process is what we can teach them perhaps they'll also generalize better and so that's the idea the idea is actually quite simple just like any other formal system like logic or causality they start with some axioms and then theoretically given those a you can prove every true thing true thing in that formal system.

So we try to replicate that for LLMs but for text, right? So we think about rather than teaching the LLM here's how you apply causal effect estimation for this problem and maybe it'll learn how to apply that for for causal estimation but then maybe it won't do well for desparation, right? Or maybe it won't do well for other tasks. This has been seen for math and and other scenarios, right? So our question was what if we now train LLMs to just learn the basic axioms. So one of the aims we tried was the transitivity axum and the idea is that if you teach it a bunch of axioms would it automatically learn to apply them and even compose them at test time and if any agent can do that like that's I would say the hallmark of intelligence right then you can actually build new theorems on your own.

You don't actually need this all these verifiers and feedback that we were just talking about earlier. So I think yeah we we've sort of taken some baby steps towards it. So currently our results show that if you train a model on graphs of size 3 to six on the causal transitivity axum it's a very small transformer model 200 or so parameters actually 68 million parameters and we just give it information like if a causes b and b causes c then c may cause a sorry then a may cause c. So that that's simple training data and what we find are some surprising results like at test time the same model also generalizes to lens up till 14.

Of course the accuracy drops but up till graphs of size 10 uh the accuracy is quite high. We even find that you can reverse the chains. You can do lots of perturbations to these chains but somehow the model still is able to learn and compose these axioms some uh somehow. And then recently we also tried it on desparation. Uh and so we teach the model desparation. And then to make things interesting we not only test on bigger graphs where of course it works. So I think that's our first sort of state proof that models can generalize to learn these axioms because that's the only way to solve it for bigger graphs if you've not seen them before.

Right? But more interestingly the same model now also starts getting better at new tasks which were causal tasks. So here what we did was we took a llama model. We trained it on our axiomatic data for desparation and transitivity which is completely synthetic completely simple axioms and now we apply to cotoc and cottocos is a popular benchmark from zijing jin and the sharp uh group where they're testing the ability of language models to do like construct graphs from correlational statements. So all you get are statements like A is correlated to B, B square with C and you have to kind of use desparation and other things to give the possible answers to is A and H is a parent of B, is an ancestor of B and and and so on.

And so to our another more surprise, we found that these models actually do well. A llama model which is just trained on axioms of transitivity and desparation which has no knowledge of protocols. The base model does with some performance but there's a significant about 20% point percentage points jump when we apply the model with these axioms learned. So I think this is giving us some hope that maybe there's a different way of training causality for language models but also more generally a different way of imparting reasoning to to these models without having any memorization con constraints.

So that's very interesting. 20 percentage points is is sounds like a really significant difference here. Yes. Yes. Yeah. We're of course experimenting. So I think we'll have an update on the archive paper soon. So I think the numbers may slightly change but I think yeah the the gap is high enough that that it's very encouraging to pursue this direction further. Would you say that this results are congruent with the results presented by Andrew Lampin some time ago in the paper exactly active causal strategies from passive data.

Yeah. So we were inspired by that paper to be honest like the only the way we started this project was after reading Andrew's paper. So yeah, we were uh inspired by that line of thinking and it's the same idea, right? Another way of saying what we're doing what we're doing is that we are learning causal reasoning by just observing demonstrations of axioms which are like passive experiments done by someone else, right? Someone else did experiments of these different graphs and gave us this aimatic data about it and we are just learning from this passive data to build causal reasoning.

So absolutely yeah yeah yeah I think it's it's it's all inspired and in the same line but we are just now thinking of taking it to the level of fine-tuning small language models and and kind of improving their reasoning through this. I'm always very inspired by results like this. So the results uh that Andrew has shown and then you have shown in your recent work. Yeah. But at the same time I feel there's a little bit of a tension there because on the other hand we have results from heander Brook and his team he's from UCLA where they have shown that even if we enforce the model to implement the the correct logical reasoning strategy by setting the weights deterministically so that they implement the algorithm the correct algorithm and then we train this model just for a couple of runs on observational data observational data from the same problem.

It's from it's within the distribution. The model will forget the correct algorithm and it will learn a statistical shortcut that does not generalize for the for the same problem. That's interesting. Yeah, but I can also I mean that's consistent with how I'm thinking about this problem, right? that they will find statistical shortcuts and maybe that's what they're also doing for this aimatic training because remember I we observed that for let's say graphs of length 10 they do well but their performance does taper off as graphs of length 14 appear right which is not what you would expect if I had coded up aimatic training myself I'm sorry if I coded up the axium myself right it would just have a constant accuracy of 100% irrespective of how big the graph is so that immediately points that there is some shortcuts happening.

There is some kind of errors that are getting propagated. But I think the hope is that if and if you scale this data and if you make enough diverse possibilities available in the training data, the space of shortcuts is going to reduce and reduce and ultimately it's going to shrink. Right? And so that's the basic tenant of like the kind of research that this direction that Andrew and I and others are doing which is that if you are able to constrain the space of shortcuts to a very small size then maybe with enough diverse data you'll be able to kind of converge on the causal principles.

Of course, the downside is also that if you don't do that, right? And so in that experiment, for example, if for whatever reason the shortcuts were easy to learn for that observational data, then yeah, I mean there's no guarantee this these systems can can learn whatever shortcuts that that suit them. So yeah, I mean I'm not sure how practical even what I'm working on is, but I think the way I think of it is that people's expectation of computing systems will change in the future. They're going to talk to us.

Not not talk talk us, talk to machines in natural language. And if that's the future, we have to build systems that can not only kind of chat with you in natural language, but when you ask a question that requires some reasoning or requires some kind of causal analysis, we should have systems that can also do that with with some amount of accuracy. So for doing that, there's only two ways, right? one you build a very good parser that goes from natural language to PI wise API runs the runs the analysis and gets back or you just build the capability inside the language model itself so that it does the parsing it does the reasoning and then gets back the output right I'm not sure which method is is more better for the future but we're just trying to make progress on both and just stretch the limits of of what what might be possible with just LLM's doing all the reasoning what do you think is the main challenge in causality today that if we were able to solve it that would give us the biggest leap forward.

Uh so many challenges but maybe I I'll okay maybe I'll tell you one technical challenge and I think one more of a sociological or a or a field level challenge that that we have. I think a technical challenge is that we really have to figure out ways of extracting the most we can from data sets. A lot of the progress in causality in the last 30 years or 50 years has has been made by modeling the world. So just starting from Beijian networks of pearl and then more recently like a lot of these world models that people are building I think or even generative models right.

So they're all focused on building models of the world and then acting and then doing counterfactuals and so on. But I think we need kind of a model free revolution in in causality just like in RL there's this model based RL and then model free RL. I I say that because that is inherently scalable but also as as you just pointed out right the challenge is there are no theorems there's no proof there's there's very little theory right now and so I think a lot of causal theory needs to get built around this kind of model free causality and if we can break this challenge right if we can build methods that not only extract as much information as possible from the data at hand while also giving guarantees of the kind that we expected to with let's say world's graphical models.

I would say that would be the biggest challenge and more concretely uh I think there's some work again from Shov's group where they're proposing the idea of uh exchangeable distributions as a new way of looking at causal discovery which I really like because all they're saying is the same I mean okay not the same thing but at a high level I think it's a similar idea that if you have iid data and if you assume that you just have data from the same distribution there's really not much you can do in terms of causal discovery.

In many cases, you can't even know if a pair of variables which direction causality flows. But if you not change your view and think about exchangeable data, which to some degree of approximation just means you have many environments and the data is coming from many different environments and getting mixed together. Then suddenly you can do a lot more and you can the paper shows that you can learn probably learn causal direction as the number of environments and their diversity goes up. So I think like that area is completely unexplored except for a few empirical studies that that we were just discussing right and if we can sort of build this theory of so wait so now we don't have I data we have diverse multi-istribution data and suppose the distributions go to infinity right you have lots of variations in your data now what are the kinds of things that you can start doing and what are the kinds of relationships you can start learning my guess is A lot of the identification results will become much more richer once we start assuming this this kind of data.

And of course to be clear I mean there have been work on this right people think of this as interventional distributions and then data from a few experiments but I think yeah more work if done in this area could really unlock I think the core question it could really unlock an answer to the core question that many causal scientists get asked is like why is causality not being used broad right I mean if I get this asked a lot at least right if if causal inference is so great right why is not everyone using these tools so yeah that's the first I to first answer your question right that maybe we need to study a lot more in terms of diverse multi-distribution data and how to extract the max out of it theoretically in terms of identification results as well as empirically I think there's also a sociological answer to this which is for the longest time our field has gravitated towards methods which inherently sort of principles based right And I there's a essay called the bitter lesson by Richard Sutin which is one of my favorites.

I think there was a time I was reading it every few weeks just to understand what he's saying. But I think there's kind of a he was talking about the bitter lesson for ML but I I think there's probably again I'm not sure but probably a bitter lesson for causality as well is that maybe causality is the end result that you want in a system. Maybe it's not always the means to get it as well. Right? So in some kinds of systems like I was talking about root cause industrial systems these two align perfectly.

You actually have a graph of the system. You can do very principled modeling of the sensors and so you you get the means which are causal and then you also get the output which is counterfactuals which is also causal. But in many other cases maybe the bitter lesson is that the output is still the same but the means to reach might be varied. And so there is of course one group or one branch of science that we have built which is amazing which is this principles based uh way to reach causality. My guess is that we have to unlock other ways of reaching causality which may be not as obvious to start with like some of these methods.

For example, data augmentation has nothing to do with causality, right? It's the simplest thing you would think of if if you have a problem at hand, right? That's very interesting you're mentioning this because I think like when you think about the exchangeability right the the paper coming from Jing Jing Jing and I think Shilop right then we have this idea that there is something that will not change between the distributions there is something static between those different distributions and this is our this becomes our it comes from my favorite poem from TS Elliott the still point in the turning world kind of right our point of reference Yes.

And so when we think about data augmentation, isn't it the same when we think about like Yan Lun's work in contrastive learning for instance and his team? That's what they're trying to do. But they are trying to do it synthetically so it's easier for us instead of like collecting a lot of data from the from many different environments. they just say like okay let's just like kind of simulate it in a very simple way just by changing the image somehow and and let the model figure out what's this still point in the in in this in this in this thing so yeah so I think this is very interesting and I think this the same spirit is actually present in this paper that we just discussed earlier from richness and ever right right because they say oh these environments they need to be different to an extent right different differences in the coari distribution and so on and so But we assume that there will be causal mechanism has to be the same in all the of the environments and this is this root this this cordex that remains still remains yeah between different environments and this allows us to to understand something when I think about it I always recall I don't know you probably know the elements of causal inference I think from Schulov and Dominicans and and Petas and they have this uh busher busher the the the illusion when you look at the chair at the at this chair from one perspective, you will see the entire chair.

But if you move your head a little bit, you will realize that these are two different objects that are spaced out in Exactly. like Yeah. significantly and so on. So I think this is uh this is so interesting because it goes down to something very very fundamental for our cognition. this ability to change the perspective and see what is static, what is what is still in the in the world and what is changing with our movement together. Exactly. Yeah, I think that's very perceptive. Yeah, that's a great analogy, right? So, either we are moving and we observing the world and learning causal principles or we can synthetically make the world move like like in these images and contrastive images and learn from that.

Right? So I think that's very interesting and I think it's it's worth kind of investing more especially because there are some of these ideas scale is guaranteed right it's just that we have to find the right abstraction to be able to preserve the theoretical guarantees as well for example contrastive learning is amazing like there's a sense in which I feel it's almost like a causal concept because what you're saying is I don't have a causal model so I actually don't know what the right prediction is right but I have enough domain knowledge to know that this should be better than this, right? Or this change should not change the outcome of of the image, right? So, I really love that principle.

It's almost like I have some causal constraints. That's all I have. I don't have an SCM. I just know some constraints that are valid according to the SCM. And now somehow I use it to create losses, loss functions or uh sort of augment my data. I'm really looking forward to how we can sort of take some of these ideas which might appear very simple at first but then embibe them with the fundamentals of causality and build these combined algorithms. Yeah. When you said about contrastive learning now it also reminded me about the idea of testable implications coming from Peril, right? So this is like reversing the reversing the testable implications and trying to learn from this.

Exactly. Yeah. That's that's beautiful. That's that's that's a great perspective. I I really appreciate this. Before we close, I would like to go back to the Y for a second more and ask you what was the source of the idea of structuring the flow of the library, the the flow of the analysis in those four different steps that you have there. Maybe for people who are not familiar with the library, you can also tell them, you can also share with them what are the steps and then tell us a little bit more how did you come up with this this idea? I think it's a brilliant idea.

Oh, thanks. Yeah. So I think this do I started around 2018 right? So context we already discussed how I was not happy with sort of having to reinvent the wheel and uh especially as a computer scientist I felt I learned better by doing and so I was I was more used to the mode of sort of doing some things checking out some GitHub repository learning how it works right and I thought like many other people may also benefit from that the way the steps came u I think was also out of just frustration if I can be honest with the existing libraries.

So they were many of them were in R and they would be like oh here's a library for matching. Okay if I if I've already understood my confounders if I've already written my equations and if I know that I wanted and I've already decided that matching is the best method for my data set. If I've done all of that then I go to that library and I do matching. Right? Similarly, there was one package for different kinds of multi-level regression and and so on. And and then once I've done that, then after that, it's on me to find the right tests to do, right? It's on me to wait for the right expert to tell me that you should do this test and then I'll do that test and and that's about it, right? So even it felt like at the time again, pardon my language, but I think I was very new to causality.

I was coming from recommendation systems. It almost felt like a dark art to me like how economists and other experts would find out the right reputation test to run and I would always be beused like how did this paper decide to do this test and this paper decided that this test was correct. It just felt like dark art to me like there's no book like what is going going on. So that was my state of mind. So I thought okay let let's just make this more like an API right and my thinking was there should be an API for this and you people should not have to sort of think so much about which step to run first or not and so with that perspective I think the first three steps were very clear I mean from Paul's book as well right that it's very logical you start with a graph then there's an identification step actually one of the things that I really liked in Pearl's book uh the first time I understood it was that he writes somewhere that identification and estimation should not be confused with each other like one of them does not require data and so there's really a big barrier between you can do the first two steps of 2I which is modeling the graph and identification without even having access to the actual data so I think there were two principles that we used to create the final API uh and I'll also use that to describe what UI So the first principle was that the process of domain knowledge and identification should be somehow separate from estimation.

People should be able to learn that just from the API and so be able to iterate also better. So that was the genesis of the first three steps which was like first you model the graph then you identify third step which is to estimate. We could have just merged identification and estimation because both of them are automatic. Like you can also do model graph and just say estimate. There's no user input involved. But I think we consciously made that choice to to separate out uh these two things. And then the last one I think I I was mostly referring to scientific theories.

I was almost thinking like causal estimates are also like scientific theories. They can only be disproven. They cannot be proven to be correct. So we thought like let's just create this last API which is API call which should be refute estimate and so that's how I think the genesis of of of the API was and the idea was that for anyone new who's coming to causality they just have some data set some problem right we communicate to them there are four steps come to this library first get a model then identify the effect then estimate it and then refute the effect and if you're not an expert just think the maximum amount of time on the first step which is modeling the graph.

The rest of the things you can just use the defaults like the library would just help you do something reasonable to begin with. But yeah, I think that that was like I think it was more like a pedagogical thing that we had in mind that causal infest should have these four steps and so how do we just make it into an API so it's very easy for other people also to follow that same process. Yeah. And I think you did a really brilliant job pedagogically speaking with this library. I started using the wife very early.

I think I was either 2018 or 19 when I when I first when I first started using it and I also you know I did my research. I checked other libraries, different libraries on the internet and DUI, I just fell in love with DIY because of this very simple, very clear differentiation between identification and estimation and then the idea that you can do anything step by step. You can plug in different types of estimators very easily, right? So you have the identified effect and then you can just go in the loop and try different estimators.

Yes. And so on. And then you can go in another loop and I try different reputation tests. I thought it was really brilliant and really easy, really user friendly, easy to apply from people who I talk to. I often hear the same story. They just said like, "Oh, this is really great." Some people also are readers of my book, you know, and and and they come and they say like, "Oh, wow. This is this is so clear. I was struggling to understand these ideas and now I understand this very clearly. I understand now why we do this step separately and estimation separately and so on and so on.

Oh, that's very great to hear. Yeah, thank you. Yeah, and I'm very grateful for, you know, for you and and Emra doing this uh because yeah, it it also, you know, just had so much impact on on my career as well. Yeah, I'm glad. Yeah, and thanks for writing the book. I think that was very useful for people to broadly understand causality and how to use Dubai as well. Great. Thank you. I'm just working on the second edition. So I'm I'm planning to incorporate some of the ideas the new ideas that we discussed today as well in the boot.

I hope that will be and I'm I'm really confident it will be helpful for many people out there especially when it comes to new reputation tests and and model evaluation modules. Before we finish I wanted to ask you a more personal question. What are two books that changed your life? Two books. First one definitely causality by Yudia. That's for sure. I've spent countless days in my postto reading that book understanding it and I still have it I think at home like I refer to it. So yeah definitely that book I'll tell you what I really liked about that book because when I started I could not understand the book.

I'm being very honest. But I liked at the end of the book uh Pearl has a prologue which is like a a segment of a lecture a public lecture he gave on the importance of causality and uh like the history of causality like going all the way to how Pearson the statistician was thinking about causality and how other people were doing it right and I think that really motivated me to understand that wow this is bigger than the topic that I was interested in but this is kind of how you reason about the world and and it's possibly useful.

Actually, it has philosophical components as well as as it has like real scientific components. So, I think for those of you who haven't read that prologue, I highly recommend it. It's very easy read and you can also find it online. What would be the second book? I'm trying to think of uh something that's not about research. Maybe I'll just say one more from research. It's a book uh if I remember correctly it's it's something like causality without shoe leather uh I think or something like shoe leather statistics.

It's it's a book by a statistician and I'm sort of forgetting their name right now. I think the whole book was about a bunch of essays on causal research where they were trying to explain how some of the findings that we think are very obvious like for example having high amount of salt affects your blood pressure right so people who have blood pressure problems should not have a lot of salt and that book just showed like it was based on some very small sample size and some flimsy evidence and so it's really not clear whether that's true or not.

So it was like a very interesting book with a bunch of essays and and I think the whole point of the statistician was to say uh like they have this famous quote that there's no causality without shoe leather and for shoe leather they meant in the old world like you have to actually go in the world like you have to use your shoe and go around and actually look at the world talk to people and do some experiments and only that's how you it's not that you get a data set at home and then you use dowy and then that's how you get causality.

So I I really love that book because I think it gave me a a very practical notion of of how hard causal research is and to be also thinking about I think the kinds of ideas we discussed, right? Verification, reputation tests, they're all sort of trying to get shoe leather into DIY, right? because people using DUI are not really going out of the world and doing experiments but maybe these tests are kind of helping them test uh sort of avoid some of the gotchas that they might have. What's your message to the cosal python community or causal community more broadly? Ah I would say it's it's a really exciting world.

I think the space of possibilities for causality were kind of steady before LLM. I think after LLMs it's it's exploded right I would say now more than ever is perhaps the best time to enter causality with fresh ideas and look at both sides right how causality would help make language models more robust and again I mean if you don't have the resources to actually train models you can also think about what we discussed early on which is causality in the world of agents right because there you have to think about ideas like what's the right reward to give how should I create these maybe some constraints uh pro textable constraints based on the SCM that's a better reward to give than just give some standard RL reward and also think of blame and all those things right so in an LLM driven agent world causality would be one of the most important things to get right so people who are new perhaps should definitely look at that and of course if if you're more interested in training I think of course the short-term question is how do you make ALMs better through causality And I think on the other side I would say there's al so much to be done for science in uh using LLMs.

And so if you're more towards scientific causal inference, I think the best way to have impact today is to build tools that help scientists make better use of their data, make better use of their own literature and then also propose experiments. Uh like it's all of it is possible. It seems like we we're living in a fairy world, fairy land right now depending on the news and research articles. But some of it might be true, right? So maybe in the future it's possible that LLMs can actually create graphs out of scientific literature.

They can help plan new experiments. What we need then is for scientists to have the right tools so that they're able to do the cycle of science much faster. Right? Uh so I think if people are interested in the causality side I would say that it's not just about doing one study anymore. I think that part of course you can do but I think the ambit has become much broader. It's like we could actually help scientists do their cycle of science much faster. Great Amit thank you so much. That was a wonderful conversation and I'm sure our our audience will will love it love it as well.

Is there any last message that you would like to share before we before we close? I don't know. I think uh maybe I'll just put a plug like for people uh just mentioning again that pi is an open organization. It's like we have under MIT license all our software is open. So this there's no sort of industry affiliation right either with Microsoft or Amazon. So I think I would say if you're a researcher, if you're interested in causality, if you have interesting methods, please do consider contributing uh to all the repositories inside PY and and of course like if you have feedback, if you want to improve it in certain ways, if you want to take in in some directions all of that is welcome and and I think u you obviously for example I am a maintainer of one of the repositories but by no means I control it right.

So if you come up with new ideas and you want to drive it forward, then I think we would welcome it and and you can actually become a core contributor as well. So maybe it's just a plug, right? So perhaps check out PY and if anything interests you. We are a very open community. We have a discord that has about I think 800 users now. So if you have questions about causality, you can come there. More often than not, there are experts better than me who answer the questions. So uh you can check that out as well.

Great. And if I can add just one thing to this, I think not only researchers but also practitioners who face challenges with causality in their work, I think would be very very welcome this community because at the end of the day, their perspective is is is super valuable as well. Definitely. Yes. Yes. Definitely. In fact, one of the things that if any of our listeners can contribute, right, is about real world use cases of causality, right? So if you're working in healthcare, if you're working in uh let's say the social sciences and you have an interesting use case, right? Even if you've not used PY, you can come up with you can come to us and then maybe we can write some use cases with PIY because I think that really motivates people to to see the impact of all the different ways in which uh causality could be used.

Definitely Amit. Thank you so much. Thanks Andex. Yep. Bye-bye. Thank you.