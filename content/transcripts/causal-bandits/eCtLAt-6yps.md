you're essentially going to overfit and then as soon as anything changes slightly if you intervene you're representing the completely wrong distribution everyone gives 110% every day and I don't think as a company we'd be as strong without that hey causal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine learning on the internet today we're traveling to London to meet our guest at school he loved math but he hated chemistry he wanted to become a writer and he built a web browser version of tamagachi using Java he's a runner and a fan of Paul Thomas Anderson's movies director of research at Calin ladies and gentlemen Dr Andrew Lawrence let me pass it to your host Alex molac ladies and Gentlemen please welcome Mr Andrew Lawrence thank you thanks for having me Alex welcome it's a very unusually warm day in London today isn't it absolutely it's quite cold in the office though hence the hoodie airc con's blasting but yeah and how did your adventure with cality start it started uh after my PhD from the University of bath so uh my PhD was focused on basing on parametrics um so not directly tied to causality or anything but I think gave me a good foundational knowledge to kind of pick up the topics quickly um I kind of went and did my PhD quite late so I I worked in industry for a little over five years after uh my bachelor's and decided I was kind of sick at industry I want to go back to school then post Masters and PhD I realized I was kind of sick of Academia and wanted to go back to Industry so when I was uh finishing writing up my thesis at the end of 2019 I started searching for jobs uh in the London area and Southwest England really and uh Calin came up and I applied and really liked everyone I spoke to uh so went through few rounds of technical interviews that was kind of my first experience with any causality topics and then have been learning on the job for four years now regarding the topic of your of your PhD research do you find any elements of basan non parametrics and the way you appli those models to real world data helpful in learning causality absolutely yeah um I always say maybe like to someone who hasn't learned it at all the caal is kind of just figuring out like the right way to factor The Joint distribution that you're looking at right uh and with the I was doing like generative modeling with um basing on parametric so specifically gum processes and toay processes and was trying to learn a latent variable model for some high dimensional data so that uh you know you're factorizing it to the the latent space and then I was making certain distributional assumptions about the latent space there so I find that that is quite applicable because you know you can um pass that on to like uh Basi and networks but it's equally valid to Define The Joint of X and Y as x given y prior of Y and also y given X prior of x but what we're trying to do with cality is really trying to capture what the the true data jaring process is so if Y is a function of X you want to factorize that in the the way that the data is being generated so what you're saying is that we in a sense we look from the distributional point of view at the direction of influence or the information flow between two or more variables ex yeah the information flow exactly mhm so what I what what it sounds to me is that the idea of thinking about structures uh was something that you were able to uh transfer from your original research to learning causality yep yeah so I wasn't looking specifically at like structural equation models or anything to that effect I was specifically looking at you know uh conditional dependence or conditional probabilities really so I you know had no exposure to kind of the higher levels of pears high like latter of causation but would say like okay you know you sample X from the prior and that value of x is informing say like the location parameter of you know your distributional y if you're you're looking at say like a a g scene or something um so it was like kind of the structure of the The observed distribution you're seeing there but but nothing with like uh structural equation models or anything like that MH but it sounds like the idea of of thinking about something that is beyond the data itself is yeah I mean it came quite naturally I I thought a lot of the the concepts were were quite easy to to see the motivation and understand the math behind it yeah this sounds like something that is uh different than what today's data culture uh promotes or what is in inherent to today's data culture where we sometimes just use complex architectures a little bit blindfolded in the sense that we don't think about the data where it came from what's the meaning behind us but we just try to apply different complex tools and move and move forward what do you think about this um this path and this direction yeah I mean it clearly is successful in some domains right so I think what you're kind of inferring is you know some deep uh learning algorithm where the architecture is chosen in a way that you're going to minimize some insample error term right uh and normally the the error term you're you're choosing is based off of you know the data type of the target whatever you're trying to represent so if it's a classification label it's some cross entropy term if it's a continuous one it might be like a mean squared error or something to that effect right and that's kind of at like a first pass you know the main thought going into it but you just kind of try these different architectures until you minimize your in sample and you might end up overfitting quite a bit um but where that obviously fails is if you're choosing the wrong covarius to predict your target that aren't actually driving how the data is generated you might fit really well in the training domain but as soon as you step outside to any out of sample data you're going to kind of fall apart so it's really better to try to find what's driving your target of interest to actually be able to generalize to to unseen data and I think you know any domain where you know you're always going to be predicting uh out a sample and making these decisions that are high value or safety critical so in like a healthcare domain uh you really can't you know just be minimizing your your in Sample error we had a we met yesterday for for a dinner and yep uh I remember you mentioned that when you look at the distinction between causal and associative models one of the vectors or one of the dimensions where you compare them is is it uh a creative job as mhm generating text or generating images or is it something that is safety critical that's one of the dimensions when you when you see the differences between those two models yeah absolutely so I mean obviously with the the like rise in the general of AI recently with like chat GPT but also um Mid journey and like Dolly you know on the image side that stuff is incredible you use it and it it's insane but if you use it where you know there's a real risk of Errors like there was that lawyer who generated um some documentation I don't know the exact use case but it essentially hallucinated specific cases that they were sighting that just didn't exist right and they didn't bother to actually check the case law to see and I believe they lost their job right I don't want to say anything but this was a big failure but if you're using it to like get Hub co-pilot or something to kind of start a class I think it really helps with just getting past that blank page problem and same thing on the image side right you still see it m journey and stuff has issues representing people's hands and things but if you're just trying to to get a starting point and then an artist could iterate you know maybe it it reduces the development cycle and same thing on the writing side um and even like you know Facebook is quite impressive with their facial recognition stuff you can upload photos and they'll recommend which of your friends they think are in it but if they get it wrong they're not losing users per se unless they get it wrong consistently and there's no issue yeah if it is wrong the user just corrects it but if you're telling some company like okay this uh specific cohort say like 18 to 25 year olds are really influenced by this type of advertising that's have a multi-million dollar marketing campaign on it and it has Zero Effect you know they're going to instantly lose trust in your model and not want to not want to use it in practice another di menion where where causal and associative models fundamentally differ uh is the dimension or the distinction between prediction and and decision making those problems seem to be just different structurally yep I wanted to ask you about your thoughts about about this Dimension how do you think about this difference and are there any specific ways that thinking about this difference is impacting your work at causal at causal lens when you work with clients sure absolutely so yeah the question just to maybe uh summarize is just uh blind prediction which is maybe like sitting at lad uh step rung one of like Pearl's causal hierarchy and then higher order um decision making you saying right where you might take some action so uh at causal lens we you know kind of follow the the Pearl School of causality so we are working a lot with structural causal models right so maybe your first question is let's say blindly predicting so we're just passively observing some system uh we have no control over it and we assume the dynamic so the system are stationary over time right so an associative model can do quite well predicting that right but as soon as I go and intervene uh do some action in that system so say I rebalance our marketing budget or we update a manufacturing pipeline or we change something in our Global Supply Chain we've now intervened and you moved away from that observational distribution to an Interventional distribution so the underlying data Jing process has changed right so you would never expect your associative model to actually be able to predict there because the yeah your underlying distribution is now shifted so how we do it at Calin is we've developed our own um structural Cal model which we call caal Net and um we use that for yeah all the customer problems so a typical workflow is kind of iterating on the underlying graph with the customer right so sometimes they come with a lot of domain knowledge they've studied the this problem for years right and they might know maybe a subsection of the graph or some domain knowledge on say like the the hierarchy of variables so say for example you're looking at different um Target audiences for for marketing right I keep using this as example but say you have age bracket right obviously there's nothing that can drive someone's age so that will kind of be at the top of the topological sort of your graph and then you might have uh the channel in which you can intervene so like Instagram email something else obviously you can choose that that'll be further down and then the response might be like click click through rate or something right so you can kind of give this ordering which would help reduce the search space of possible graphs quite substantially so that's kind of always the the first step right we'll we'll iterate with them get some of their knowledge use like a suite of causal Discovery algorithms so you know constraint based score based continuous optimization um model based like a Lin gam or something like we're not very uh you know tied to a sing single method kind of see what works well so some of the score based exact search works quite well for like all continuous data but if you have like mix so meaning like categorical and and continuous you might want to choose a constraint based method where you can choose like a conditional Mutual information say Oracle conditional Independence tester as a way to measure that um so yeah and then it is quite iterative say so like any method you kind of get out you can never recover more than a Markoff equival class of graphs right so you're not getting a single dag that you might build a structural CM model from you're going to sample from that space and kind of speak with the customer see what they think is feasible what also actually you know matches the data right you could say assuming it's this dag and it say it was a perfect like linear SCM what's the actual like error of this graph fitting fitting your observed data um so then we fit a structural CMA model which we have a few different like backends to do it so kind of the the modeling language itself is like agnostic to how we would train it so we have like a p torch engine so just stochastic gradient descent we have a CVX Pi so convex optimization and that kind of limits some of the ass the functional dependencies you can have on the edges and then um a pyro engine so we can do distributional forecasting or not forecasting but yeah like inference on the nodes so this this this engines I use uh as I understand to learn the functional forms of the connections between two different variables exactly so say you have a a small graph with x and z driving Y and you say that there's some linear association between all of them you'd be learning essentially the weights from X to Y and from Z to Y uh we also have like different aggregations at the node so right that that might be assuming a sum but you could have a sum bi sum with bias you could have um higher order interaction terms so you can kind of make it like a polinomial type regression but um the the specific you know Edge functions and aggregations uh we try to make as agnostic to the choice of engine as possible Right with all three of them you could have a linear dependency it's just the type of free parameters you might be learning are different obviously with a pyro engine you want to be uh estimating a a distribution so you're kind of learning um you know uh VAR parameters is kind of what we're doing so we're doing like stochastic variational inference and then from that we can uh estimate the the posterior at the node one of the things that you do at CLE lens is that you take well-known algorithms or even open- Source uh implementations and you modify them in a certain way that serves your clients better are there any improvements that you build in those in this implementation of SCM for instance yeah so for the SCM what we're talking about before right going from the associative to say the observational distribution to the Interventional one uh a very common like technique for unbiased effect estimators is like double ml or debiased machine learning right but that's very specific normally when you you have a known treatment variable and a known outcome and you want to try to learn learn a model for what's the effect of that t on y say um so this is normally you know the the common example is you have uh some backd door path so you have some confounding between treatment and outcome and what you do is you kind of split the data when you're training right so you learn to predict the treatment given your confounding um you get the residuals from that you predict your outcome given your compound and get the residuals of that and then you do regression on the out of sample so you split it in half and then you're doing a residual and residual regression right yeah which helps us get rid of potential bias that we might produce as an artifact of the method that we were using yep exactly so obviously we'd want to do that at the SCM level as well right if you say we're just you have each node uh in your SCM and it's some functional dependency of its parents right so if you just like naively iterate through each node training that function independently of each one right so like I I train the function for x given its parents and I go and train the function why given its parents if you have observations from everything right you're not necessarily looking at how that information is flowing through so in the case of uh if I just did that you know in the simple case where I'm trying to predict uh my treatment from the covariant and then I try to predict My outcome given my not covariate confounder sorry confounder and and treatment the effect that you'd end up getting between the treatment and the outcome is going to be biased because you still have the information from the confounding essentially in your observation of the treatment so we kind of developed a means to train an SCM using like the motivation behind double ml so as opposed to just iterating you know naively across all of the nodes training them independently we build like a a training graph which kind of defines the order in which you have to train the nodes and with which data split to do it the only problem with this um is it limits some of the dependencies to force to be linear so the original DML paper The Edge between you know your treatment and your outcome has to be a linear because that's how like you end up removing the bias because they're orthogonal essentially um there are some methods to extend it with like with kernel methods but we haven't looked into that yet but that we also um I think it works for two of two of the engines so it's not like fully agnostic to the actual training engine under the hood but it should kind of be independent of of our causal net really it's just giving you the order in which you should train the nodes given given a dag and what what portion of the the data to use MH earlier we we talked about a little bit your background and you said about your PhD uh in ban on parametrics uh for many people ban inference is is the go-to uh set of methods when it comes to uncertainty quantification but recently we have another very hot and very popular framework which is conformal conformal prediction prediction and there's some literature recently showing that uh conformal prediction can be successfully merged with causal methods improving uh uncertainty quantification um and and giving valid intervals m is this a direction that you have also explored somehow yeah a little bit um so we've used uh Mappy which is like a open- source library for conformal prediction we don't have it specifically on our SCM but we've integrated it into a causal impact package so we have um you know causal impact is like historically this Google paper for like basian State space model so you want to your your observation all you have is pre-intervention and then post so you only know what happened after you intervened and you want to know what that effect of the intervention is and what you do is you look at similar series uh the the common cases maybe like countries right so some country uh enacted some tobacco rule or like changed minimum wage and you might want to look at similar countries or cities based off their demographics that didn't do it and use them to build a synthetic control so we have like multiple methods in our causal impact package again which is focusing on time series where you have a mix of Interventional and obser obervational one based on the SCM so you train an SCM pre- intervention then essentially Force you know do the do operator so you're going to break any dependencies going into it set it and then predict post um and that we've used a few different methods we have like a quantile regression just like a simple bootstrapping and then also conformal prediction to get uncertainty estimations um but that's kind of all all we've looked at so far mhm and how does the this math work comparing to uh to to other uncertainty quantification methods do you see um a visible Improvement a significant Improvement um H I think it's hard to say because we're using it in a Time series domain where I think the original implementation is focused more on IID so there is some recent work on conformal prediction for time series uh so like where it's non stationary potentially um and I don't think we've explored that enough so like speed wise and stuff it's quite good but accuracy I'm not sure uh so you mentioned the the uh synthetic control M framework and before we discussed peran graphical graphical uh models these two different lines of thinking can be can be complimentary uh but also I can imagine that for somebody who is uh just starting this might be a little bit confusing so I imagine if we put all together like quas experimental methods and SCM based methods all together in one package if the API is not very very clear and it don't doesn't make distinctions maybe between those different approaches it it could be confusing and so one of the ways uh that come to my mind that comes to my mind how to solve it would be to uh modularize the the package M and from from what I know this is also something that that you're doing with your team yep yeah so the product itself is quite modular so maybe just to to give kind of a background it how it's presented to the user is essentially like a cloud platform that can be deployed on Azure gcp uh AWS we also have done like on on site deployments right so um some customers yeah have their own cloud infrastructure and essentially it allows data scientists to to launch different jobs so maybe the the most standard entry point is kind of like a a Jupiter uh lab session that you would launch that gives access to our various packages Andrew we talked about fitting different functions uh for for scms mhm and even when we are in in the causal domain in domain of causal models we might face some some challenges here so for instance imagine that we have some data in the certain range but then in the in the production case we expect that we might go beyond this range what are your thoughts about situations like this yeah absolutely I mean I think that that's probably a pretty standard case actually right it's highly unlikely that your training domain is going to be exactly what you see you know when you're when you're in production so one of the things we do is um we me I mentioned before we kind of have these different Edge functions that we try to be as agnostic to the underlying training engine as possible and um we have like a suite of family of them that I would say is kind of like shape constraint edges so uh obviously like a linear Edge you know exactly how it's going to extrapolate As you move from negative Infinity to positive Infinity right but we have some more uh complex ones like so say like a just a monotonic edge so it's forcing you to increase monotonically from the training domain um so you know it's not going to say like you know loot back it it's always going to as the inputs increasing you're going to increase agosi right um we have like a monotonic with saturation right so like you you're coming up and then as you increase like you're just marginally increasing you're still increasing but you're not going to you know blow up to infity um we have some likewise linear edges right so like you fit where the transition points are with the training domain and then you know as you move away it's just going to be a linear Association negatively and positively away from the the training domain so we find that that actually builds a bit of trust in the models as well right so you can fit some like really deep neural network and it behaves super well the say it's for classification of the syon boundary is great within the train domain but you have no idea what it's going to look like as you move further away right it can loop back on itself uh wherever there's no data you're you're kind of up to the whims of how is initialized random right it's essentially random right yeah exactly um so this yeah is quite helpful we kind of uh it's nice to kind of inspect what's happening at the edges as well to see so you know customers might have some intuition of what they think the relationship should look like between say two of the covariates and being able to inspect down at that level the model is quite helpful and they know you know if there's some Black Swan event say like interest rates or unemployment rate or something is one of your covariates and that like jumps multiple standard deviations you know how the model is going to going to behave it sounds like a like a way of incorporating expert knowledge in in this a little bit less restricted way so so you you are not saying that this will take particular value or this there's a particular Edge between the nodes but you say hey this relationship should not should not be this for sure right so we say that at least yeah exactly it's kind of a way to make like a par parsimonious model so like simplify it a bit and you know each Edge uh like you could say maybe like add a specific node make it some like multi-layer perceptron of all of its parents right but then it it's really hard to know what is learning so uh I think it's a bit better to maybe take away some of the expressiveness of the model to kind of have some functional forms along the edges that that the user can can trust and we actually do you know speak to the customer about that right so I mentioned before kind of this iterative causal Discovery approach until say we we settle on like a some direct today cylic graph or dag I might have used that acronym before without defining it um we also do it at the modeling level right so just finding like a a cal GPH that people will accept is you know just the first step right if you want to build an SCM there's the whole uh functional dependency of a node given his parents essentially you you mentioned causal Discovery uh how many iteration steps do you do typically in practice uh when you work with causal Discovery algorithms and and human exports uh I think it's really dependent on kind of it it's dependent on like on each business to be honest with you so a lot of customers we get uh some are like quite deep down the causality rabbit hole right they they realized that this is something they need to do and they might have also like done some of their own homework ahead of time right so sometimes we come with full graphs of what what they think it is right and then we just are just validating it given the data and what they think think how much does a the underlying like multiple algorithms agree with that others we kind of iterate on we come with say maybe no domain knowledge and we try to like tease that out and you know that takes a few more iterations so I mention before kind of the hierarchy we find that that's quite a easy way to restrict the search space of possible graphs without actually encoding too much bias or error right if you go down into the oh I know for fact there's no edge here or this edge here you know like if you're wrong that's going to propagate through through the whole graph right uh but this kind of like hierarchical knowledge about these nodes can affect these nodes which can affect these uh that's kind of some of the stuff we do more um and I mentioned you before we don't kind of just use like a single method so we do have like quite a a family of caal Discovery method so you know you get a bit of the iteration from that so we might try to try to to present different different results and and see mhm that's very interesting uh many people who are just starting with causality have this fundamental fear that they might come up with a certain dag but they they they might be very very unsure if this dag is correct and then they ask themselves a question what if what if this model is what is if this model is is wrong uh looking from this perspective of this process that you have just described what would be your advice to those people um I mean I I guess it's true with any model right you normally would do some like validation phase uh and some some testing right so like you can almost back test it right for for maybe like using a term from maybe the more like forecasting uh domain um so I wouldn't just you know take all of the data we have fit it and then have nothing right so like you should really do training validation maybe like test split right so you can look at say like predictive information so if you just care about rung one um you can use that uh that dag and STM you might build from it and see how well you can predict various nodes within the graph given given their parents uh if you have any Interventional data as well you can see if the estimated effect you're getting from it matches matches what you you might have measured before um this was kind of a a point I meant to make when we were discussing the dou ml inspired training routine we have for the SCM the main point was being able to you know normally you care about the treatment on this specific outcome we wanted to learn these functional dependencies such that you could pick like any pair within the SCM and say what's the effect of like Z on y xon y anything and then you know you'd have an unbiased estimate it sounds like learning a full SC where you can basically uh change your query and say like I I want to understand effect of this one on this one and then another one on another one and you have this flexibility to ask any question about any effect in the model absolutely so you lose a bit of predictive accuracy right so like if you definitely want to just predict as well as possible like this one target variable you're going to lose some because one you're looking at less data doing the data splitting technique um and two yeah you're going to not have an optimal say prediction of each node but you know that if you were to intervene so break anything going into it you're going to have a an unbiased estimate of that that treatment effect it sounds like finding a balance between causal bias and and estimation statistical bias yeah that's a good way to put it um just to Circle back a bit on your question yeah about maybe the fear of not finding like the perfect dag right um it really depends on what you want to estimate as well right so you could have 50 variables but if you care specifically about how I don't know this one like advertising color or language affects like click-through rate on like a banner ad or something potentially a lot of the structure could be completely like insignificant to that to that treatment effect right um so some of it is like you don't necessarily always need to find a full dag for everything right it really depends on what you're what you're trying to do that's a great Point uh I had a conversation uh recently with naftali vinberg he's a philosopher of science U focused on causality the intersection of causality and dynamical systems and he has this perspective that causality is SC is a scale specific uh phenomenon so at one time scale you might have one model at another time scale you can have another model and this also translates to to spatial uh scales and so one of the examples that he gave in the podcast was that we might have a variable that is actually cyclic mhm right but it might be the case that the cycle is in such a compress scale time scale that we actually don't care mhm for the interventions that that we are that we are doing I I guess actually we can even model it per se if it is cyclic so uh in the elements of causal inference book I think like one of the chapters in the end is kind of About Time series causal graphs so as opposed to having a single node for say some variable y you can have the observations at different points in time right so like like X in the past could drive y more recently in the past and back and forth right so if you were to compress it in time it would look like there's a cycle but again it's all the resolution you're measuring at it like you're saying um I remember reading I think uh the pcmc paper um by yakob rung um and he did an extension of it called like uh pcmc plus which allows for instantaneous effects so the original paper was you can only have lagged variables driving you know the the current time but you never might not be able to measure at that resolution right like obviously if you could get down to the nanc or faster everything is going to be lagged but if you're measuring something at Daily weekly monthly resolution the effect of that treatment is going to be observed in your observation essentially instantaneously um and I I think you can always model that really right I I don't see an issue even if there is some really slow slow cycle you can actually put it that into like an SCM you just build it the the underlying framework for it would be one of these time series causal graphs where you might know at at the weekly scale you are seeing some cyclic effect but when you just care about predicting the the next day you're not going to worry about that or anything yeah you did some research and published some work in in Time series causal Discovery can you share a little bit about this with our audience yeah so what motivated that was trying to break some of the underlying assumptions of the methods so we didn't propose like any new methodology or anything we just proposed a means to generate synthetic data to kind of validate how sensitive some of the methods were to breaking their underlying assumptions so we looked at like a vector autor regressive version of lingam no tears we looked at um pcmc using different like conditional Independence testers and then we're just comparing I think the structural Hamming distance so that's a means to measure how far uh your estimated graph is from from the true one for different scenarios so um you know we would break a linearity assumption we would break like an assumption on gussian of the noise things like that um and then yeah we open source that software just for people to be able to quickly generate time series that that might you know both agree and invalidate some of the assumptions and see how sensitive it is for the algorithms a lot of stuff we were doing prior and still now really is uh applying or slightly modifying these methods to work in like real world scenarios or you know you might not have theoretical guarantees per se right so um the PCL algorithm famously assumes like no hitting confounding but I I'm sure every data set you're going to have there's always going to be some variable you're not able to measure that's having an effect on some of your covariates um it also is assuming a perfect Oracle or conditional Independence tester but that's never the case you always have small sample effects you're never going to be able to measure with 100% uh certainty whether or not two variables are conditionally independent given given some other set um so yeah we kind of we're just doing internal benchmarking essentially which was what motivated that Workshop paper and um we're just trying to test you how how sensitive some of the methods were to to breaking their assumptions I believe that this is a very uh important area of walking cality today especially after the papers uh from Alex razak and also from your former colleagues um your former colleague and your and our CTO yeah and your CTO uh about about un unsuitability of no teers for Cle Discovery and and and showing that um that was more in Alex Alex paper showing that there are might be certain properties in synthetic data that might make the work for algorithms much easier MH and the challenge is that those properties not are not necessarily present in the in the data outside of the of the data generating process the synthetic data generating process yeah their paper was called just for the the listeners and viewers was uh Beware of the simulated dag I think and unfortunately I guess like our uh yeah nerves Workshop paper was just a bunch of simulated dags as well but yeah we were trying more to to break it to see how far we'd have to move away from the ideal case that the algorithms were written for um yeah I I quite like their paper and it brings a good point right and it's actually a somewhat of a problem in general with the causal Discovery literature is there's just not that much ground Truth For Real World data right um I think I remember the pcmc paper looks at climate uh climate data so it's time series about like large scale climate events um but everything is still kind of built by domain experts right so there's not that many real world data sets that that we can provide so a lot of the stuff is just with like Eros Reen uh like random graphs and then generating random like functional dependencies random noise and and that's what the stuff's benchmarked against I think this clear conference this past year they had like a I don't know if it was a data set track or something specifically for trying to get some more data sets benar stuff yeah it was called the call for a call for the data sets yeah and I I think that's uh that's great that we have initiatives like this definitely benchmarking in causal Discovery specifically but in causality in general it's a very important topic and if we think back uh to 20145 to the so-called image net moment um one of the conditions that made this moment possible and all all those breakthroughs in computer vision possible was the fact that we had some uh gold standard data sets right y I think there's still errors in that though right yes there and and we learned with time that they were not as perfect maybe as we as we wished but still there was some something to compare toh the only thing I would add I guess on um the paper on the unsuitability of no teers for causal Discovery I guess the original no te paper actually doesn't specify that it's for causal Discovery it's just for basing networks and then people took it and applied it for causal Discovery so the original authors never made some claim that it was like a perfect causal discovery algorithm right yeah that's that's that's true I think also the offers of the Beware of the simulated dag paper they also don't say that the original offers made some claims but uh this doesn't change the fact that it was a very useful paper eye opening paper uh now there's another paper that is say a follow up we link to both papers in the in the show notes so you can you can read them if you're interested in this um interestingly it turned out that in causal inference we we we encounter similar challenges there's a recent paper by Alisha K from M shap showing that synthetic data used for benchmarking caal inference algorithms might have similar effects similar in spirit let's say MH so that's that's another challenge you also worked on on a very interesting paper uh that uses a so-called AAR algorithm mhm in the causal context could you share a little bit about this work yeah so it was um follow on from a couple recent papers where you're using the AAR like path finding algorithm so it's quite a like historical uh computer science algorithm to find um yeah causal graphs for purely like continuous data is kind of how it works so um there was a paper called like the the algorithm in it was uh triplet AAR and then they quickly followed on with a one called I think AAR super structure and like local AAR to make it a bit faster and at the time when we were working on it I think it was maybe one of the ones that they claimed was you know scalable to hundreds of nodes and like converged within you know before the heat death of the Universe um so how it works is uh it's an exact search score based method and the score is the basian information Criterion for each node so you have a node and a set of potential parents so say like it's a really small you know graph with three nodes uh X1 X2 X3 so say you want to figure out where X3 is in the graph you have a parent graph which is like the null set so X3 would be like a a source node you have X1 you have X2 or you have X1 and X2 and all three of those are essentially like a a branching graph right you can start the null Set uh X1 X2 and then the combination obviously the the size of potential of the parent graph for each node scales immensely as you have a large number of nodes and you're trying to find the shortest path through that meaning the path based off of the score so how well X3 can be predicted given nothing given X1 so given X2 or given X1 and X2 uh and you optimize that parent graph across all the nodes simultaneously essentially um and it assumes linearity and gaussian additive noise um and yeah you have to use the Bic and that's where the like theoretical guarantee is that you'll converge to the correct graph if those conditions are met and there's no unobserved confounding as well so you have to have czzle sufficiency um so the the super structure paper and the local one the local one um allows you to not do it simultaneously and you can parallelize it a bit and the superstructure uses graphical lasso as an initial set so like what seeds the potential parent graphs uh in let's say the naive case or the Baseline case is that each node can be a function of any of the other ones but you use graphical lasso as an initial filtering step to kind of get a sparse estimate of these parent sets so so what graphical lasso gives you is an estimate of the Precision Matrix assuming all of your data is sampled from a centered multivari multivariate Gan can you tell our audience a little bit more about what graphical lasso is yeah sure uh so the AAR super structure method uses graphical lasso as a means to see the set of potential parents so what graphical lasso is all is trying to find is a sparse representation of the Precision Matrix assuming that your uh continuous variables so like large x sub Matrix of variables is a sample of a zero mean multivariate Galan so this means that that this is a method in in its Essence it's an regularization methods over the space of edges yeah you can kind of think of it as like a a way to get a sparse skeleton so like an undirected graph as like a starting point of a causal Discovery method um so but it's assuming linearity and gaussianity of course and that uh again causal sufficiency that all the variables interacting on the system are are observed um so yeah the authors of the the most recent AAR paper which I think was from 2021 nurs but uh I can give you the link for that as well they propos using graphical lasso as initial step so beforehand um in the original paper the naive method is that every parent can be a function of or every node can be a function of all the other nodes so the parent graphs all of them are are quite big uh so using graphical ASO as an initial step you kind of are pruning the parent graph AR priori before running the AAR search uh algorithm essentially so what our paper did was extend that a little bit more but with like domain Knowledge from the problem and we looked at a couple different vers versions of prior knowledge you can have in cause of Discovery there was another nice it wasn't like a literature survey paper but it was a paper that kind of defined a table of different type of prior knowledge so obvious stuff like oh we know there's an edge there but we don't know the direction we know it's a forward directed Edge we know it's a forbidden Edge we know um that it is more likely to be like a a source node so it might have less parents or it's more likely to have more parents um and it follows some causal hierarchy stuff like that uh so we wanted to see how to encode that into the AAR method and how we ended up doing it was essentially being able to further filter this parent graph so you know you're not required to use graphical ASO as an initial step um but we just looked at like what's the speed up you get using the AAR method if you add in one known Edge or if you add in a forbidden Edge so if you add in a forbidden Edge you can imagine you're you're essentially removing one one of the candidate nodes from the parent graph for that specific node um one of the cooler uh things we found and just to shout out my colleague Stephen he is the first author on the paper and it was all you know majority of his work so he did a great job on that we found that if you just defined three tiers so in your full data set you say one node is guarant guaranteed to be a source node so again I gave an example like earlier about age so if in your data set nothing's driving that that's set you can't you can't affect that right and one is um like your sync nod so say click through rates in like the advertising case right that might be your Target and you're not measuring anything else Downstream then you put every other node in the middle tier and that reduces the number of score computations I mentioned before that it uses Bic by half which is quite substantial right so it's not uh like a big O change it's still on the same like order of magnitude but it's quite a big speed up and it's it's fairly easy to encode this information into the parent graph uh for aoria like practically speaking it's it's a major change especially if we think about uh Cal Discovery algorithms that might be a little bit slower with bigger graphs um before we also discuss this this idea of all this fundament mental fear of having this wrong dag and I have a feeling that those two threats are are connecting here a little bit in terms of the meaning of additional information so even if we don't have full information we just can exclude or include one Edge or 10 edges this might significantly reduce the size of the search space absolutely yeah right so in general I think like discovering Beijing network is an MP hard problem right that's been proven and the space of graphs grows super exponentially with the with the number of nodes so anything any domain knowledge you can bring into the problem to reduce the search space helps significantly uh another thing is you know maybe like a course to find approach to use some language from computer vision uh can you kind of look at clusters of variables uh find like how those might interact and then drill in at a higher resolution or I mentioned before you know for some problems you really don't care about having the whole graph right so you just care about some some local structure um one paper I read somewhat recently I think it was called like dag fochi was the method and it was Finding what the like true drivers are of your target variable of interest so what's the parents and it's starts with uh this initial method called like fi FCI and that's trying to find the Markoff boundary of your target variable the Markoff boundary is defined as the minimum size Markoff blanket so I don't know um if you're familiar with Markoff blanket but what it is is it's the parents of the node the children of the node and the shared parent of the children that's defined is like the Markoff blanket and if you condition on all of those nodes your variable is conditionally independent of everything else so it kind of has all the information you would need to predict um and having the parents is really what you would want if there's some ordering right if you have your children but it's a delayed effect you can't use that to predict the other thing right so if you're really you know just care about finding like the robust set of features to predict your Target and you know that like this graph structure isn't changing in time but maybe you know some of your variables are a bit non-stationary so the distribution driving them is is changing you you can use some methods like that so I mean I would say like you know it's never a silver bullet we're not just looking for a single dag and a single like model for every problem you really want to use the tools that are applicable for what you're trying to do MH some people uh have an impression that the the formalism of that directed I graphs might be very very uh limiting especially given that in more complex cases like marketing or any social uh phenomenon that we we might be interested in modeling we cannot really exclude potential hidden confounding and for for many people who are only familiar who are only familiar with directed asli graphs but not other types of graphs uh this seems like a major uh blocker but we know that identifiability in caal terms is not limited necessarily to to dags or directed basically graphs what is your view on that on that and um how do you see this in Practical terms yeah sure um so yeah I think I mentioned earlier that any like causal Discovery method can resolve up to the like a markof equivalence class of graphs so you normally can encode that into a different mixed graph type so like a com like a CP deck completed partial uh directed a cyclic graph there's also maximal ancestral graphs partial ancestral graphs uh the partial ancestral graph is the like normal output of a FCI which is like Fast causal inference but it's actually a constraint based call Discovery method um and yeah you can perform inference on these mixed graph types directly right you don't need to resolve down to a single a single dag um you can yeah figure out is uh the effect of t on y identifiable from this and what's like an a reasonable adjustments that you might need um so yeah that that's also another another Avenue right you you don't need to to resolve down to a single dag depending on what you're trying to to estimate yeah sometimes we we might be just lacky and and just go on with with a structure that looks not very promising in the beginning but it turns out that from the formal point of view do calculus point of view we can actually resolve it yep yeah the other thing I would I would just mention um is a lot of the stuff we do maybe we're we care about being able to identify effects of of multiple things so it's not just you know what's the effect if I change T from True to false right you you kind of want to take it a Next Level so you have these are the five levers I can pull um what's the optimal intervention or set of interventions to perform to say reverse an unfavorable decision so that's kind of the the algorithmic recourse problem right you have a set of intervenable variables and you want to know what's the the set of interventions to essentially get over the decision boundary so you're rejected for a mortgage what you need to do to get over but you can also uh look at at other um things right like uh not just trying to reverse an unfavorable decision but say maximize some revenue or something and I can pull these three lovers what what ones to pull and at which level to pull you worked with many different uh use cases when it comes to applying cality to real world scenarios um complex things like Supply Chain management uh something completely different like manufacturing MH what were the main challenges that appeared in those in those different cases and how were they different between between the scenarios yeah so for supply chain um obviously you know you're never going to have sufficiency in that regard right so maybe that was the biggest challenge is really that this is a complex system that you're never going to be able to capture all the variables interacting on it um it's also highly non-stationary so we looked at like a the supply chain of like a component Electronics component manufacturer that was quite High dimensional and highly nonstationary and we're trying to figure out um what was driving one of their one of their kpis so say like you know through throughput of like component sold or or um lead times stuff like that and um so one the effects you're seeing aren't always on the same scale uh the data is highly non-stationary so we're looking at ways to statione the data to then apply it to maybe more standard caal Discovery techniques so like like you're bringing in some tools from time series to like fractional differences essentially so like if the data is non-stationary you can kind of think of looking at the change of it right so like a one-step difference is kind of like the first derivative but sometimes you want to um not have it be exactly just the prior one but some fractional weighting of the prior um so we were doing like a lot of pre-processing there to then try to figure out what was actually driving their kpi on the manufacturing side that worked quite well actually for a lot of stuff out of the box and I think in general we found things that have an underlying physical system so like some you know Factory where it's very constrained what component is going into which other which I think is also true on supply chain but it's at a huge scale right like not every part can interact with the other there's still some spatial temporal dependency but the manufacturing is is much more constrained like imagine having an assembly line you really know what's the order of stuff so you can kind of model the information flow quite well um for the supply chain yeah a lot of the stuff is more that we've been looking at is a bit more predictive but a bit more like what interventions we can do that you know have some marginal gain where on the manufacturing side a lot of the use cases have been more root cause analysis so manufacturing like maybe some bigger box items or manufacturing specific components that go into another product um you know what's leading to this anomalous outcome so like really bad yield rates or something for some component um what what step in the the pipeline is causing that um some of the stuff we looked at yeah it seems uh from what you're saying that there are many different methods coming from from many different areas or Sub sub areas or of engineering uh computer science and so on that that you are using in your in your everyday work how do you feel to what extent the fact that you did your PhD uh is helpful for you for you today for my PhD specifically or no like doing PhD this experience that the experience of doing PhD how does it Translate to to your work do you do you feel that this is something that that is substantial to the way you work today and you see the problems or yeah I think it's helpful in uh like learning it not necessarily the concepts but how to quickly distill the information I think that that's more of the the skill I got from my PhD and kind of what I see with the team as well so maybe taking a step back I can kind of give you like what our philosophy was uh building up the team cuz obvious viously the stuff I've spoken about here is not just my work right it it's a big group of people and I'm in a team with like machine learning Engineers who are a bit more you know strong in the the raw software development stuff but still understand all the concepts and then we also have some research scientists who you know you could say hey this looks like a new promising um topic here's this paper does it does it seem like something that would be beneficial to to add to to our toolkit um so that's where the PHD I think really helped me is being able to see like a new paper you know you look at the abstract you look at the method you look at the results and like you're not reading it top to bottom and you can kind of make a judgment call on both level of effort to implement it and then potential uplift you're going to see from it and kind of that's what I seen people on the team so um um yeah that's kind of the skill we're looking at when we were building the team up we actually weren't even looking for people with purely causality background it was more do they understand the fundamentals of machine learning can they explain how the underling methods work like how the math Works underneath it and can they like quickly prototype some of the stuff so can you see a paper and turn it into like a prototype implementation pretty quickly and we can we can test it I think that's kind of the the skill gotten from the PHD you worked as a in a bunch of technical roles and now you're a director of research M how was the transition for you from from the technical uh role to a role that involves more um more of like a social interaction and working with people managing people it's still very technical still um so yeah there's a line manag component right so like career progression one to ones but there's a lot of um you know help with like product road map um there's how I kind of see my job now is kind of like an Unblocker of people so beforehand I might have like a specific project where we want to implement like this method add it to one of our packages um and get it in the monthly release and now is a bit more like okay I might own a few items from the road map um but I'm not the individual contributor on each one but like I'm still connected to know okay this person hit a wall looking at this methodology like what what do we need to do here uh this person might have come up with something so I I feel like if I'm able to ensure three other people on the team are are moving quick and unimpeded right that's better than uh if like I'm singly focused on one thing but it it's still quite technical I mean when you came this morning I was you know finishing addressing PR comments pull pull request for those that uh aren't too familiar we were doing a a release candidate today we have like a monthly release Cadence and I still had to get one of uh the poll requests in and then release a new tag so like a new version of this package that would go into the to the release candidate so it's not fully hands off um yeah when I joined the the company was quite flat everyone was essentially a data scientist and then as we grew we kind of got into um specific roles so like there was initially a team for like modeling it was like called modeling R&D and then it kind of morphed more into yeah General uh R&D team uh and then you had a specific you know wing for data data scientists and then we now have like pre-sales data scientists or you know customer success data scientists so depending on where in the customer value life cycle you are you told me that as a child you really liked math and physics M and Science in general no chemistry though I said no chemistry we know this what was it there that that you find so so interesting what motivated you to go this direction um H I don't know that's a deep question I think it it's just seeing stuff work I I think I mentioned at the dinner like I I initially did my undergrad in engineering and I worked a bit as like a electrical engineer and then transition to a software developer in my previous job before going back to Academia um maybe like one of the regrets is I wish I did my undergrad in computer science instead of engineering but when I was doing that I guess I didn't have much experience in uh computer science really saw that it was a viable path everyone just said like oh you should be an engineer cuz you know you're good at calculus which I never understood or something uh I don't know I think what I liked about it I still remember as a kid doing science fairs like you see in like the sitcoms in the US right kids do uh like the volcano is like the the Trope right uh but we made I made like chalk was something which you can kind of grow yourself did some other experiments I just I don't know math always kind of made sense to me like even algebra and stuff early on just being able to to solve for x I don't know it was very intuitive and if you just followed if you follow the rules that are defined within that system you you'd come to the answer uh yeah I think I just more left brain than right brain to to use that stereotype so how did it feel to you when when you you're able to to find a solution I guess rewarding right um like the task is uh to I don't know solve for x when you're in like middle school and then it's uh you know do some crazy path integral when you're in like college or something and uh I don't know I guess it's just rewarding right whenever you can see something to completion and that's how it feels in the job too right so you know just getting your PLL request merged isn't the end of the story right so we uh we'll do like internal user testing so that's what the release candidate is right it's for like internal testing let's see if this stuff works then if it is it goes into the stable release and it's kind of shipped to customers right so the end of the end of the line is actually seeing them use stuff and like you know we work hand inand with a lot of customers it's nice knowing that you know their internal data scientists are seeing value and what what you're delivering and it's working for their their problem who would you like to thank well everyone on my team so uh Franchesca Mato and Stephen uh also uh Marchin Elia and Max Elliott uh everyone you know gives 110% every day and I don't think uh as a company we'd be as strong without them and you I really appreciate all everything they do also uh to Max and Darko for their guidance I've been working quite closely with Max our our CTO uh very recently so it's just great being able to have his ear and have him you know providing input on a lot of different aspects of things um but everyone at the company really like all the different uh teams are are great um obviously my parents if we're going to go go way back they're the ones that nurtured uh my education and academic like Success Through the 37 plus years I've been around so what was one particular thing that your parents did to make you feel supported or nurtured as you as you called it yeah I still remember um ever since being a little kid my mom reading to me every night and I think that that helped a lot with like uh consistency so having like a kind of a fixed routine uh like a thirst for knowledge like always trying to learn more ever since like I was really little like from three to you know in my Preen I remember every night we would we would read something just for even like 20 minutes 30 minutes right just to kind of have that routine and always trying to learn stuff now and still now I always have like some you know fiction book on the go that's kind of my preferred my preferred uh you know uh topic or or type yeah mhm mhm it it sounds like um like something that also builds a lot of closeness mhm absolutely yeah I mean well my family were always quite close actually so my dad would be home for dinner every day um my mom was a teacher so she had quite a nice schedule so she was kind of finished by you know early afternoon um but then when I got older she transitioned to like administrative role at school so she kind of worked a bit later but you know we always had family meals together um quite a quite a supportive family really the whole time is this something that uh translated to your own relationship as well yeah of course so you asked who I would like to thank and obviously the the person I like to thank the most is my wife Leanne she's very supportive um I moved out here to actually be with her so we've been together for for quite a while now I think uh we've known each other for more than more than 10 years uh we got married right before the pandemic which was lucky cuz we had no wedding insurance or anything I didn't even think you know there was much risk uh all of our family and friends flew out from the us so we got married in what March of 2019 and uh if it was a year later everyone would have been stuck out here it would have been crazy but yeah I mean she's great she's supportive of uh me going back to school right so like you know we weren't earning very much while I was doing my Master's or or PhD helped support me through my furthering of my education um we moved from Bath to to London together so she kind of switched her job she's a consultant so she can kind of work from wherever but obviously needed to be at a customer that was London based and not West England based so yeah she's just always uh been here and supportive of anything I wanted to do for my career personal life anything what would be your advice to people who are starting with something maybe they want to go into Co it or maybe just starting with machine learning in general and they feel maybe a little bit overwhelmed that you know there's so much to learn to make those things work um I guess maybe you don't really need to be an expert on everything so there's uh not even for people that are necessarily getting a PhD but there's actually like a quite a nice metaphor which I've seen like in University where they say like oh this circle is the sum of all knowledge in the universe right and PhD is like a small dip in like some super hyperfocused area that just pushes the boundary of human knowledge further and I don't like I don't understand everything in ml or in causality the the field is too big I think you kind of look at what what you're interested in um one of the most General like books is Christopher Bishop's like machine learning pattern recognition book I don't remember the exact title I think that one's quite good um you can really just start at the beginning it's very fundamental stuff like they'll leave and go through like you know properties of like the gaussian distribution stuff right it doesn't start in the deep end um and then kind of see what you're interested in I think also going like maybe use case specific or like what area you want to focus in going that way so the group I did my PhD in at the University of bath they were very uh computer vision kind of focus so a lot of people were looking at like Graphics applications stuff like that and maybe tying it back to where the talk was at the beginning was you know some of these Association like correlation based methods work quite well in non-safety critical systems and some of the stuff being done by Nidia Nvidia sorry and uh epic on kind of this generation of Graphics or visual elements is very impressive and if if you're you know really interested in video games or like movies and stuff you know you want to focus on on that type of stuff if you're really interested in health care right there's specific techniques you you want to focus on um with that so thinking like drug Discovery and things like that it's really what motivates you um and there's no need to become an expert on everything go go where you're motivated what resources would you recommend to people who are just with causality sure um Brady Neil has a really good uh like tutorial um maybe like online course so it's a set of kind of lectures uh and slides and stuff that that are very approachable I mentioned it earlier um but it maybe is a little more advanced as the elements of Cal inference book that one is yeah quite deep and and good um the book of Why by Judea Pearl is a bit more narrative but but uh it kind of puts things in context quite well um let's see what else well I think you have a book right maybe people can uh can start with that um but yeah there's tons of resources online actually there's a lot of uh there's a lot of good like summer school lectures I can't remember specifically but I know for new joiners at causality we kind of have a list of recommended reading which includes uh some previous lecture some some papers some textbooks um yeah there's a lot of stuff out there mhm what question would you like to ask me um in all of your work where have you seen like the the greatest success in causal methods in terms of like outcome for uh business outcome for business so the problem is that I don't I cannot about certain certain things um broad Strokes like maybe uh the use case um so I think one of the one of the use cases where where you can see a lot of value coming from from applying causal methods is marketing I would say that's definitely one of the one of the big ones and um is that just because of the freedom to inter and and run experiments that's one thing uh it's not always the case though MH uh what I think is even more fundamental a more fundamental driver of value is that people uh in marketing Often by education they don't look at into the structure too much MH so even if you build a structure that is roughly okay and even if it's not perfect it might imp prove the outcomes really significantly in general do you find presenting someone with a causal graph helps them understand it better so like it's a bit more intuitive or easier to understand how the information is flowing through their system or not necessarily um definitely uh so I understand your question is if if presenting a graph of of a system representing the the relationships in the system if this is helpful for for someone yeah I think I think dags are great Tool uh for conveying the vision and and translating some of the modeling modeling assumptions uh to to people who are not necessarily technical people and even to technical people who are just not familiar with yeah with causal thinking um because if you throw something into a a big fully connected like feed for Network right you don't know how all those features are interacting and really the dag is saying like you're limiting if you build a structural CA model you're limiting how certain features can interact and some you're not allowing to interact at all in the model right yeah that's uh that's a great Point um I love a thing that I love one thing that naal Weinberger told me he told me like sometimes you know people tend to see a complex a more complex model and a less complex model and they assume that a model that looks less complex also has less assumptions MH but this is not necessarily the case so that's maybe like another side of the same coin where sometimes somebody just says like hey we put everything in this blackbox model we don't make any assumptions but this is actually not true you are making very strong assumptions about the structure of the data generating process you just make them implicitly and I think that's one of the uh one of the huge challenges here that people uh many people will tend to think that this is data driven which means this is assumption free uh but what we in fact doing we might do we might make implicitly very very strong assumptions that will impact the way the conditional Independence landscape is is represented by the model and another problem is that we are limiting our ability to learn because if this model fails of what in my experience most organizations will do they will either retrain this model or say like let's retrain more often mhm or they will try to find a more complex architecture andc increase the capacity of the model essentially yeah yes thinking and the thinking here is that hey maybe it was uh not expressive enough to capture all the details in the problem and Let's find let's find something that is more expressive and this can be very very detrimental in especially in the long run of course yeah you're essentially going to overfit um and then as soon as uh anything changes slightly if you intervene you're you're representing the completely wrong yeah distribution right yeah so mlops can be helpful uh but but but sometimes especially if it's a decision-making scenario maybe approach like called caal ops that's this excellent paper by by Robert Meyer when we think about open source packages uh today we have we have a significant e ecosystem uh of of Cal packages in in Python they might not always be very easy to learn in terms of differences in apis uh and all this stuff but people can generally build pretty Advanced systems with them what do you feel are the most uh significant or important contributions that you and your team uh brought to the to the image uh when it comes to your product yeah absolutely um yeah it first maybe start saying I think the open source stuff is great um we actually so what our product is to maybe give the viewers and listeners a brief overview is it's really a cloud platform for decision making so what the users presented with when they start is an ability to say launch a Jupiter lab session they can use all of our packages but there's nothing limiting anyone from using open source so I spoke a lot earlier about various uh causal Discovery techniques we have implemented and maybe some modifications we made but like there's nothing stopping anyone from using like huawei's g- Castle right and then plugging it into other parts of our Pipeline and we even have a template showing users how to say maybe discover a graph using g- castle and then take that graph and go into our own structural causal causal modeling I think on the product side of what we do we try to make our packages as modular and use case agnostic as possible so there's not just one big like causality importa from causality import you know solution it's it's very much uh modular for for a lot of different techniques so I think what we try to do is yeah understand like the breadth of um the the landscape what looks quite promising and offer uh an implementation of that that has that's well documented that's well tested that's maintained uh that's updated at like a fast schedule I think I mentioned before that we have like a monthly release Cadence so we're really trying to add new functionality dealing with feature requests from customers doing improvements bug fixes um at at a Quick Clip we also have like make sure our packages support python 38 to 311 the main images on the platform are are 311 which kind of gives you a bit a speed increase um but on the modularity side yeah some of the stuff is like we'll have a a package of what we call it decision intelligence engines that might take one of our trained structural CM models and and do something else with it so one on algorithmic recourse which I mentioned before another one on root cause analysis so there's a few different techniques we have to to try to find the root cause whether it be a source node or intermediate node within the graph um so it's mod but it's kind of plug in place so there's like a shared API between everything where you know if you discovered some model earlier in your your workflow you can plug that into to one of the other components but there's really nothing stopping uh you know a user from bringing in open source stuff so uh we have um you know a set of uh metrics that might be like more predictive based so think of your general error terms but also graph based so like structural intervention distance shd stuff like that but then we also have some uplift metrics and we we use like psychic uplift directly right create open source package for um like Keeny curve and and stuff like that that people use for uh yeah uplift modeling essentially so yeah I mean we're definitely not trying to compete with open source or anything it's really a a platform for people to use the tools they're they're used to and the use the ones that we provide as well it's really a a Melting Pot for for what data Sciences want to use beautiful so it sounds like like there are two sources of flexibility one is modularity and another one is that you also allow to plug in anything from the open source domain into the system so people can build a workflow with as you said something that maybe they are familiar with and they really like this method or the API of this package and and they can seamlessly integrate this into into into the workflow yep yeah definitely so like if you have a graph already built with network X you can construct an instance of the graph we might use to build a structural CM model directly from that so you can call that um if you have a graphical CM model already from from DOI uh it would just really be writing like a a small wrapper uh the platform you know if you're in jupyter lab you can open a terminal window and you can just do pip install whatever's there right so anything that you're you're used do um you can do it so we have pre-baked images right that might come with like the latest pandas I think like 2.1 you know numpy networkx a lot of the stuff that we use to develop our packages pytorch pyro everything I I mentioned earlier but if you want to install something new it's just a pip install away basically Andrew what would be your message to Cal python Community um I think the people should uh you know it it's a big community and we should continue to communicate I think you're doing a great job putting this podcast out there um I'm always happy to talk to people uh we were at nurs this past year and we also attended clear and it's just great you know just for like brainstorming and stuff and just being able to to meet up with everyone um I uh one of the things I think I spoke to you at the dinner was um there was this like Laton uh hierarchical caus of Discovery paper that that spoke about this one thing called uh a rank deficiency and that was uh trying to estimate how many latent confounders are really in the data looking at the the the deficiency of the rank of the covariance Matrix of your observed variables uh this is obviously making some linearity assumption but I had never seen this applied before and we like took that to put it into like another part of our our product and that stuff just comes up naturally when you're speaking with other practitioners other researchers other developers and stuff and I think uh you know constant communication is always a a great way to SP on new new ideas is the future causal absolutely yeah I think anywhere where you need to to make some decision uh figure out some action choose an optimal treatment um in a safety critical system a high value system uh where any errors are are detrimental to to life to to you know Revenue business like I I think it has to be if uh entertainment value or like um less you know risk heavy use cases I think there's there's tons of methods out there that are are totally valid um but yeah when we when we really need to apply this stuff in scenarios that are are make or break life and death it has to be Cal where can people find more about you your team and and caal lens in general yeah so um we uh we have a research page on our website so caal lens.com um we publish our papers there um we yeah tend to uh you know our main kpis are kind of more product driven so we don't maybe share as much as everything but we've attended some conferences where we don't have stuff um we are we put some talks up and some learning videos on YouTube as well I think we're going to do some some more of that um we uh recently open sourced our like front-end package Dara so we're we're kind of moving a bit more into the open source and want to you know collaborate with the community so uh just to give a brief introduction um Dar is kind of like our our frontend app building framework so kind of the data scientists will build build stuff you know maybe directly in code and then then want to build an app to ingest some of the stuff but also for decision making and part of that we also open sourced our CLE graph library because one of the bigger D components is a is a graph viewer um so yeah those those are available on our our GitHub I think it's just github.com Cazal lens uh the code from that time series CLE Discovery paper is there the code to reproduce the results from um Marcus's unsuitability no tears papers there so yeah slowly adding stuff uh stuff there as well these are great resources we will link to them in the in the show notes cool Andrew it was a pleasure me as well thank you for having me thank you for your time I I really enjoyed the discussion and I'm I'm I'm confident that the community will love this as well cool yeah thank you it's great meeting you cheers thank you for staying with us till the end and see you in the next episode of the causal Bandits podcast who should we interview next let us know in the comments below or email us at hello caal python.

stay caal