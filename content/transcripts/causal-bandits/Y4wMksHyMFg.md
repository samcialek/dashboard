causality large language models explainability generalization and fairness this is Cole Bandit extra at triple AI 2024 part two enjoy hi uh my name is Kevin sha I'm a fifth year PhD student working with Professor Alise Baron boy in the caal AI Lab at Columbia University and here I I'm discussing my my colleagues work transportable representations for domain generalization we may want to solve a a statistical task like classification for ex you have features X label y you're trying to predict y now the issue is you have a source Dom where you have your data and then the target domain where you're trying to use the model is different from your Source domain so if you blindly use all of your uh features X you might not get the right answer in the Target domain right now how do you the the point the the whole idea of the paper is how do you get get some classifier that is capable of generalizing across environment right one naive way of thinking about it might be okay let's use theal featur for now while that might work the issue is that it's not often the best sometimes you can leverage more information uh than than the causal features in order to get a better represent a better generalization the concept that is used here is the concept of transportability we say that something is transportable if it is in variant across domains so we would like the classifier to be invariant across domains now while sometimes the causal parents are invariant across domains we can often get something that is more informative than that and is still in very across domains um and that's generalized through uh this function here so we can have five of X which maps to a representation R and now we're trying to predict y using our representation R the paper discusses um many results on how to get this R um and the ultimate goal is for this prediction of why given the representation R to be transportable to be in Buried across domain and um the paper discusses many ways to guarantee that um Can Can we also do it without the caal graph yes yes so while the first half explains of the paper explains how you might do it um knowing some information about the caal mechanisms uh the second half of paper discusses what happens if you don't have that and you need to make some other assumptions uh in order to work entirely um on the statistical level um using using just your available data and even with these assumptions um uh and and without the C diagram you you are still able to get some guarantees on when you can get a representation that's in variant across domains in the end um what you what you get what you end up with is you may have some representation that's more informative than just the causal parents and is still in is still transportable it's still invariant across domains such that you can get a better prediction that still works in the Target what impact do you think this work can have on in the real world the issue is that like often times in the real world um we simply can't collect data in the in the domain that that we intend to use our model in right in healthcare for example we want to study humans but all we have is like a setting with a lab for example right there's so many of these cases where um the the area of study is just different from the area of application and um if we ignore these cases if we ignore the differences in these cases um we can't truly hope to get accurate results right so this will basically this type of work which essentially leverages some sort of causal understanding of the system I think is is very important for applications in these domains where uh we we still want to be able to have some powerful predictive performance even when the the domain's different what was the most interesting causal paper that you read last month o oh I see I see oh I I I don't remember the name but uh but there was a paper um that studied uh how how large language model behave on in solving causal inference tasks uh I think the the paper um the paper produces a data set uh seather yes yes yeah sead yes that one yes there's it was basically a data set and I think the uh the the takeaway was basically that llms are really bad at do at solving these tasks right but I I think it really paves the way for like a better understanding of like how how uh llms understand the world and having these data sets could be great for having benchmarks for for future research so this work is about um counter explanations in in the in the broadest sense um there's been a lot of work in this in this space uh C ofure explanations essentially try to explain uh the behavior of blackbox models without actually opening the blackbox so the they work under the premise of just looking at how uh inputs into a model uh need to change uh for it to produce different outputs and in this context counterfactuals they can also have a causal meaning but in a lot of the literature the the causal meaning is is a bit more implicit in this paper what we try to tackle is is faithfulness uh so a lot of work in the past has focused on on a different Des which is called plausibility of course in the context of explanations of human decision makers of algorithmic recourse people are interested in generating explanations that that actually look intuitive and that makes sense uh and and plausibility uh the way that we Define it uh in our work uh essentially means that the counteract that you generate uh is consistent with the true data generating process so we have a little image here if you want to just slightly uh T the camera here so here we show uh the counterfactual path going from one class to the other so from Orange to Blue and Contour just shows the the colal density estimate for the true distribution of the data or rather the The observed data so it's only an estimate of that but plausibility in this context means that the counter factal should end up in a in a region that is characterized by by high density but if we go from plausibility to faithfulness we note in our work and this is very much the the motivation that if we focus only on on plausibility of counterfactuals we might kind of lose track on on our primary I guess uh objective which is to explain blackbox models and to illustrate this here we um we look at a couple of uh simple illustrative examples using Mist so here we have a factual image which is correctly classified by a simple MLP um as as A9 and the the task for the contrafactual generator for the explainer is to to see what is necessary in the eyes of the model to go from predicting nine predicting seven uh so that's that's the the counteract the target label in this case and all of these different conections that you see here um using different approaches all of them are valid so for all of these with with high confidence the classifier predicts that this is now indeed a seven what you can see is these two here water and shoot they look very much like oser attacks that's not unsurprising because just methodologically C explanation and and adversarial examples they they're related the only really plausible explanations is is this one generated by a very interesting approach called revise here would the Au author propose is to use a a surrogate Model A variational auo encoder under the hood to try and understand what actually makes what is a plausible factual or counterfactual to understand the data generating process and that's that's great that's that's a plausible explanation everyone would probably agree that this can pass as as a seven to this two one exactly exact yeah so these are just you know the Ness of those High Ness exactly but to me there there's a bit of a friction War so again since we're in the business of explaining models how can we sort of confidently show just this this plausible explanation if these other explanations are also valid uh in my mind we are yeah committing the risk of whitewashing a blackbox model because we're showing something that that's plausible an explanation that that pleases us but it doesn't necessarily reflect accurately how the model behaves and that's what we're trying to to tackle in this work we want to have faithfulness first and and plausibility plausibility second um because in in my mind I see very few practical cases where generating plausible but Unfaithful explanations for blackbox models makes makes much sense yeah and we should Mark here as as this is in the context of people will be watching this video in the context of K that faithfulness is understood here different differently than the Assumption of fiful faithfulness that we use in causal Discovery okay yeah that's a that's a good point maybe to to since you mentioned causality there are also interesting approaches in the context of counterfactual explanations most notably by Kimi at Al and and B B shop um is also involved in this work and Isabel Valera um they've they've basically shown that it is possible given cause or knowledge to generate counteracts that are causally valid and you can actually use that that causal knowledge to generate counteracts more efficiently at smaller costs to to individuals so what we try to do in our work instead is to to Simply rely on properties that the model provides so instead of using some some surrogate tool derive better explanations we put all the accountability on the model itself and it turns out there's actually a fairly straightforward way to to still get plausible counterfactuals provided that the model has learned plausible explanations for the data and to do that we we borrow ideas from energy based modeling and also from from conform prediction the the intuition here is that we want to be able to characterize or quantify the generative capacity of the the classifier in question and the predictive uncertainty so that's Jo energy modeling and prediction respectively both of these approaches are model agnostic so we can do this with pretty much any uh differentiable model classifier hello I'm loish a PhD student with I Bombay so this is our 24 on continuous treatment effect distribtion using aing interp potion and kernel smoothing so we address the problem by augmenting new treatments in the data set and estimating pseudo outcomes for them we observed that estimating pseudo outcomes for close to observe treatments is easy and can be and can be obtained by performing A first order T expansion however for treatments that Li far from the observe treatments we need to have uncertainty measures so that we can scale down the loss contribution of UNR relable suit outcomes we found that uh this kind of uh data augmentation helps break the conf confounding that exists in the observational data set and leads to better performance we applied our method on vcet as a back so vcet is weing coefficient neural network that is kind of stateof theart neural network architecture statment distribution and we found that that simple data augmentation technique boost the performance of V by a significant marget for more details please refer to our paper in20 thank you so much how would you summarize the main contribution of this paper the main contribution of this paper is to have uncertainty estimates for treatments that are far from The observed treatment and it is very important to skill down the contribution of such under level Su outcomes what would be the best uh application for for your res so the best application we want to apply eventually our goal is to learn uh recourse in recourse uh we want to find an optimal treatment that works well for the given patient now one intimidate step in performing recour is treatment effect estimation where the goal is to estimate a difference in outcomes as we give different uh treatments to the individual efficient methods in estimating treatment effects finding the optimal treatment is kind of you know difficult so we believe that uh this work will help in addressing the challenges that we have in three both problems what is the most interesting caal paper that you read last month that you read last month so currently I'm working on treatment effect estimation given post treatment cerates so I have been reading papers on self supervised learning there are some impossibility results that state that uh you know unless you are given contactual data finding representations that will make the treatment effect estimation approaches that otherwise handle that that are applicable on observation data set kind of you know wi I mostly refer to the papers by the author Berard HOV so he has lot of papers on uh you know impossibility results that uh yeah great thank you so much yeah so my name is gold and I'm an assistant professor at Mel University and you know responsible literature often the causality robustness and fairness are considered a separate topic so they are studed depend while when you're training the robust kind of responsible AI model the robustness and fairness part the properties that we need to be forced in that single month so this work we are trying to see whether it's possible to actually look for a model that is adversarial robust and fair while we also considering the causal structure that's the motivation of this work if you're thinking about adversary robustness is that we are looking for perturbing an individual data point to the points that like the label is changing so we want to make sure that the model is robust with this kind of perturbation and if you're thinking about that the individual fairness we are saying that two individuals should be similar two net should be similar and receiving a similar decision of the model so these two Notions are very connected to each other so they can actually think of like the robustness in terms of of the fairness that the perturbation is on the sensitive attribute so you want to make sure that the two individuals with the perturbation and sensitive attribute they are similar to each other while in the other way around you can also think of like this perturbation to define the similarity between data points right so here we are accounting for this metric that we want to define a similarity between data points that are accounting for perturbation but also for the sensitive attribute perturbation and also know knowing that like there is a cause of a structure that the senstive and non sensive features are related to each other so you are not able to perturb one feature while kind of other features remaining the same which is in the adversarial robustness this kind of perturbation are uniform right so you're always kind of like if you're having thinking of the shape of that perturbation you're dealing with the ball so this the main idea of this paper is just like we are creating this metric that we can actually create this kind of bation with the idea that are coming from the causal literature which is contactual and you're looking for the twen perturbation in the robustness the teature and also in the fairness for the Sens of attribute that if you are pushing all together we can actually have a model that are fair and robust that cause all what I would do one or two main lessons from this word for you so the app show is possible right so you can actually have a model that is robust and fair and CA aare right right so they should not be defined independently because they are very much connected right and if you're looking for a model to be employ life right they should be accounting for all these Dimension right so this work I think for me is kind of like looking at this kind of intersections right and getting one a step further of like creating a model that have more than one property in terms of responsib what impact of this work would you like to see in the real world impact of this work just showing that it is possible right so we can train a model and the performance like here we are showing that like you can account for all this thing but the is not changing so you can of same optimization same structure optimization that you were using also the performance is not changing too much right so you are not paying so much of a price to account for this I think this is kind of like if you are deploying these kind of models they have a better model in real or that is not going to be wor expensive in terms of computation but also not more expensive in terms of the performance but much better in terms of respons hi my name is Marcus Blazer I'm from Sal University and presenting a joint work with aan Gupta from I Bombay it's an interesting problem right that's my motivation So stru Cal models some we assum the gra is given and we have this random variables which are linear combinations of of the other variables with um ARS which are normally distributed and Zer me and now essentially we have to observe Co variances of the random variables and we ask ourselves can we identify these parameters problem is solvable in principle for instance by grou basis approaches these um have double exponential running time by group basis they are complete which means that they always identi when when it's possible to identify however they have double exponential running time which is prohibitive there are also other algorithms known like here with an instrumental variable or generalization of this these algorithms are more efficient however they fail to be complete which means that they might not be able to identifi parameters which in principle are identifiable and what we do in our work is we look at the restrictions of the structural for models namely where the underlying graph of directed edges from the tree and for those structur C models we're able to give an algorithm which randomized polinomial time and which is also an addition complete three shft structure CA models we solve this problem M what impact of this work would you like to see essentially what I'm have the serious perspective on this problem and I would like to understand the complexity of proper mhm and this I think is a very nice first step towards this because we have a natural graph of structural C models we have a complete and efficient algorithm hi I'm D singi and I'm assistant professor at the indoan University of Technology of the Netherlands how is the conference so far uh pretty good uh we started with the bridge program on the 20th and the 21st we organized a bridge on continual causality which is the fields of continual learning and causality as the name suggests a pretty good very nice talks especially this time we invited speakers uh who are not very senior and they made some very good effort to bridge the two things together the two Fields together so that was Prett interesting nice contributive talks as well and after that the main conference has also been good some really nice invited dogs the papers have been good I mean the causal papers have been I mean I'd say it's like uh 50/50 some have been really good some have been underwhelming but overall I think I'm satisfied what are the main insights or lessons from the from the breach that you have added in the beginning of the conference that combining continual learning and cality is difficult I think what both continual learning and cality lack are these real world applications right and uh what we realized from the bridge and from the invited talks as well as several discussions with the participants was that in order to actually scale causality or even scale Contin learning to real world applications you need a combination of both so I think that's very interesting I hope that people will take it Forward uh based on all the discussion that what are the main challenges that the community that you believe that the community should um should start addressing when it comes to causality and intersection with continual learning uh there's several uh the first one that comes to my mind is benchmarking so we don't have specific talking from causality point of view we don't have these specific causal benchmarks that we can test models on you'll see your paper might have really nice theorems uh you might even propose very nice methods algorithms but then if you see the empirical section they are on synthetic data or at best they on this Asia data set or right so which is I mean I can understand why but uh I think it's also time to move on what's next for you what research programs are you planning to focus on in the next one 3 5 years so maybe next one year will be causality and large language models so for example a workshop is called causal pars large language models we talk causality but they are not exactly causal right and you'll see as you have also mentioned in your podcast that there's been recent spurt in the papers that talk about llms and coal which is nice but you'll see that people stop at a point where they say okay there are the these several open problems and we don't know how to solve which is nice I think it's very important for the community to bring forward what of problems are there but I think you have to go one step ahead and try to solve these problems so for the first yeah first year and couple of years I think causality and llms will be a major uh goal and long-term I think uh scaling causality is the long-term goal for me and specif specifically for that I have something called probalistic circuits in mind uh so probalistic circuits are these generative models uh but the inference is linear in the time in the number of network parameters that you have uh we have already some Works where we have bridged coal with problemistic circuits and now it's time to actually apply to a real world prop of course also come up with new ways of this combination because there have been several new approaches in problemistic circuits as well so basically taking causality marrying causality and problemistic circuits so that both can benefit from each other is the long-term goal what is the most interesting Cole paper that you read last month ah interesting I think it was uh this paper by Microsoft Amit sharma's paper where they talk about this uh correlations and causality and I'm forgetting the the title of the paper uh but where they kind of said that under some assumptions actually your large language models can learn causality which is kind of uh different to what we propose in the causal Paris paper so they have I think two major findings where they say okay sure there might be correlations of causal facts but if you make some assumptions then you can actually have large language models learn Kaz I don't remember the title of the people the top of but that was pretty interesting to see and uh recently Amit Sharma who is one of the co-organizers he has tweeted a few papers that I saw again in the realm of causality and open world uh next on the list they seemed pretty interesting I of course I just read the abstract and they seemed pretty interesting so yeah what's your message to the Cole python Community you're doing amazing work but now it's time to scale the models and somehow I also feel that uh again we are lacking in terms of benchmarks so maybe libraries are are are important but then uh keep in mind that at the end we want to scale these models to large models in substance right yeah keeping that in mind while developing these libraries would be interesting