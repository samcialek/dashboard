do hyperparameters matter for caal Discovery and how to tune them for your double machine learning model join us for four short conversations with leading coal AI researchers on hyperparameter tuning coal representation learning and causal effects of publishing your preprints on paper acceptance this is coal Bandits extra at Clear 2024 let's go hello everyone I am damansky from University of Essex I'm happy to present you uh recent work on robustness of algorithms for caal structure learning to hyper parameter Choice what is your work about I'll start with the motivation of the paper so uh it it's fairly simple actually uh we go from the place that we recognize that parameters in supervised machine learning are very important essentially there many methods are very sensitive to the choice of parameters another point is that at the moment caal structure Lings of recover of caal graphs from observational data don't really work that well at the moment especially not in applications like with real data sets because those methods rely on machine learning so much I I was wondering if those struggles in application can be due to have a parameter choice and also something very important to note is that structure recovery structure learning is unsupervised in nature we don't have any information about caal graphs beforehand we have the data set and that's it right um so we first explore very simple uh graphical example uh we have two variables X and Y so very simple B varied case and the idea is that because we have only two variables what we can do as a very simple solution is to fit two regression functions one is uh X on Y and Y on X right so we have two separate estimators and then we can measure prediction error of the two and then how this very simple Cal algorithm works is whichever regression line has lower prediction error we conclude that oh this must be the correct coal Direction whereas if the order has lower error then conclusion is otherwise now what we have on the x axis is number of regression parameters in the in the example the general idea essentially is that as we change the number of uh regression parameters of the coefficients in regression the answer uh of the caal direction will change right so for certain values we will conclude that the caal direction is the right one and for other number of coefficients the answer will be the opposite which is in incorrect right uh and that number of coefficients we because we can control this as analysts it's a hyper parameter right like one simple parameter so that that's the main message as we change that hyp parameter the answer differs and mistakes are possible because of that right so that motivated our full study that we conducted many different simulated settings but also one real data set s and also very fairly popular data generator Sy uh we measure three main matrics structural humming distance but also false positives and false negatives of the edges right essentially for all of them lower is better in terms of H parameter selection we cannot really do H parameter tuning because those Methods at the moment don't exist or if they are they are very experimental so we are just um in a way we are in an abstract world and just we assuming that we can perform some uh selection decisions but we just kind of abuse the fact that they the are they are simulated settings right we have four choices one is the best H parameter values which we can just call like Oracle we we assume that we have access to Oracle H parameter tuning uh on the other end of spectrum we have the worst possible Hy parameter values and then com in between we have either default values proposed by authors of algorithms or within packages and we also have default values that we derive across all simulations right uh and now the main message well actually there are a few very important messages the first one which is very surprising um especially when we compare it to to standard machine learning is that default values perform actually surprisingly very close to Oracle cases and that's surprising because in standard machine learning usually default values perform very poorly that was very surprising fighting here another one with respect to Oracle cases is that first of all even when all algorithms have access to this this Oracle they still differ in performance uh so that means that uh model select algorithm selection is actually very important regardless of our parameters uh and another important point is that even with this Oracle there is still room for improvement for all the algorithms we we still see that there's still more work needed even if we have near Optimum parameters uh another Point respect to the worst possible H values is that we can see that again um the extent to which the methods can perform badly also varies which means essentially I I'd like for example to interpret this worst case scenario as safety of use of the methods right so essentially if certain methods lower values in the worst cases they are essentially safer to use than than the otherwi like for example new network based methods uh appear to be fairly risky to use in those cases which is kind of expected because new networks are known to be fairly volatile and sensitive to two hyper parameters uh and lastly in this in this particular place we are very important um message is that yeah even though we still uh have mistakes even with the Oracle cases the number of false positives reduces pretty much to zero so the only remaining mistakes we have are due to false negatives which means in Practical terms that in those fairly well specified parameter cases whatever answers we have from the algorithms the graphs and the edges even though some of some of the edges can be missing but the ones we have can be trusted as like this this is true right I think this is this is very useful uh can be useful in many cases right it's not at least it's not totally false what we get what I Al like one of the messages I I like to pass along here is that when we compile a list or a table of the best hyper the best algorithmic choices across different settings and we can see that the best choice will vary not only across data set settings right so like size of the graphs and Edge density but also the choices will vary we consider different Ty of parameters or different quality of parameters right so for example certain methods uh will be preferential if we consider that we can perform tuning somehow uh whereas other methods will be safer to use if we consider the worst possible cases in terms of Hamet specification right so I think that's something worth U uh keeping in mind in terms of choices what impact of this work would you like to see in the real world I think the the singular thing that I would like to the message I would like to send is that uh to practitioners perhaps to as I said just just a few seconds before whenever people try to select the best the best algorithms for their case they should uh consider not only the data set that they have at hand but also how H parameter tuning can impact that that optimal choice right say for example uh are you confident that the values that you're setting parameters are actually like Optimal or maybe if if your risk is um your case is um risk sensitive and maybe it's better to focus on safer methods that that's that's my main message what should people type in Google in order to find your work uh okay uh you can find me on Google Scholar uh d malansky uh you can also find me uh on X on LinkedIn uh usually it's at the the mcklansky that's my usual handle what's the best CLE paper you read last quarter I don't fully remember the title but it it's it's about uh upper parameters and umist performance on the bounds decision bounds between parameters like it just it just matter perfect visualizations thank you that thank you my name is Oliver sh and I'm a PhD student at Martin spindler's Lab at University of hamburg what is your work about the one you presenting so my work is about hyperparameter tuning for cal machine learning we are working with the double machine learning framework and our basic idea was um that often the story is told how predictive machine learning is uh failing at causal inference tasks but could it also be that causal inference is failing at the predictive machine learning because in theory uh when you run double machine learning you should get good results as long as you use uh good estimators sample splitting and also nean orthogonal scoring function but what we find out is that it's also up to many tuning choices such as the hyperparameters for the Nuance estimation so for the estimation of the influence functions and also and the choice of the causal model and of course the choice of the learner so lasso random Forest plays in very important role what are key results from this P our key results are that um actually tuning your machine learning methods for the Nuance estimation on the full data um or on the folds of the double machine learning algorithm um are the preferred options um in small sample sizes over doing a train test split um another key finding is that um if you wonder which um causal model and the Incorporated structural assumptions um are the uh right ones um for your particular data then you might want to track some sort of a predictive um loss on your um outcome y um and compare it between different causal models and the direction of Advantage um in in this metric points in many of our tried um dgps so we tried different 16 different dgps and then many of them it points in the same direction as the loss on the causal parameter which is a good result because you don't need an um Oracle estimate to compute this metric and the final um finding we had is um that you can combine some sort calculate some sort of combined loss between both of your uh nuisance estimates so in this case you have one um estimate which is a propensity score of treatment assignment on on D and one is an outcome model on Y and if you combine the loss on both of these models then um look at um a good a small value for that um this might indicate the right parameter tuning to achieve also a small loss on um your causal parameter of interest regarding the particular algorithms like base Learners does this work um leads to any conclusions that could be helpful for practitioners of course the choice of the right machine learning method heavily depends on the sort of the data generating process this is really something we can see in our paper we have the full results for all dgps and it's very different of course in linear settings lasso might perform very well while in heavily nonlinear settings boosting algorithm is preferable something which we found um novel on this is that we used um Automated machine learning algorithm um in terms of um Flamel uh uh the Flamel framework and we found out that um this is maybe something that fits well for all data generating processes this might be also interesting for a practitioner to look at this um framework for tuning what impact of this work would you like to see the a small level of course I hope that uh yeah people who apply the double machine learning framework but also causal machine learning in general let it be in business or in Industry have a better hint um on um how or better guidance on how to tune um their choices such as causal models uh hyperparameters machine learning methods but on a bigger scale of course I also hope that I can have an impact on that yeah causal machine learning is applied more in Industry because right now I think um there are many question marks uh and maybe also skepticism and um when there is more work on the practical application I hope that people uh will use it more what should people type on Google in order to find your work um they should type in hyper parameter tuning for causal inference with double machine learning but uh in more General I also uh recommend looking at um the website of our software package so uh I'm part of the team that develops the dou ml package um you can check our website docs.

l.org and you will find many exciting examples and um also um documentation for double machine learning and for this specific project what is the best conso paper you read last we so uh recently um a colleague of mine had uh yeah they did some work about how to implement deep learning into the double machine learning framework the paper is called doubl ml deep and you will find it archive by typing in that and um I think it's very exciting because um they can now also use as the newsin not only tabular data but also images and texts and of course when you have for example price elasticity estimation using images of products or product descriptions are very exciting so I'm not involved in that but I read their paper recently for the first time and I think it's totally an exciting project and uh I hope I will learn more about it soon great thank you so much thank you Alex my name is y laal I'm a post at the Allen Institute for AI and the University of Washington the work you're presented here what's what's what is it about estimating the caal effect of early archiving on acceptance and the idea is to um you know we have papers and we put them off in an archive that's the main way of putting a walk out there besides the actual conference I'm coming from the NP community and until very recently we had this anonymity period and the Assumption of the or the the main um reason for having the anonymity period at least at the time of having it having starting that was to not have some certain people with certain affiliation on you know more famous people have an advantage over other kind of groups when they put the paper on archive uh so they won't get any other treatment when making decision on paper acceptance and we had this policy from about 2017 until very recently and we actually wanted to check whether this assumption holds in practice basically what we did is uh we took papers uh so observational data where we had data about the acceptance the Autos uh and different other kinds of variables and we try to answer that question whether different groups have uh different acceptance rates uh based on uh you know those different variables the difficulty was that in practice there's hidden confounders like paper quality right so you could assume that uh people that really believe in their paper but if they did high quality work would like to put their paper on AR because they believe it's already in high quality and want other people to read it uh on the other hand maybe people that less quality paper they may perhaps mainly submit to the conference in order to get feedback and to improve that so this is a very strong uh confounder but it's also hidden and uh and besides the uh besides the fact that how to uh quantify it or to Define it it's also how to quantify it in practice uh so what we did is use framework called the negative control outcome which basically you means that you need to come up with another variable that has the same confounders like quality um and we can use that in order to DEC confound its effect so in practice what we used is uh the variable of the number of citations of the paper after n years this variable with different in difference framework we were able to DEC confound the variable and get some estimation about the exact effect that we were interested in for the results we there are different setups and different assumption that we had to make but um the first one was that without taking that quality confounding factor into account we saw that there are benefits of putting your paper on AR valy on the other hand when we did use negative control outcome and the difference in difference framework we were able to show that uh there's actually not much of an effect and even when in the cases where there was an effect the effect was not statistically different between different groups and those groups were number of citation and institution ranking which were the main reason for having the anonymity period for the first place the funny bit was that the day that we submit the day that we got the results uh of accepting to I was the same day that uh the anonymity period was removed from the a community what impact of your work would you like to see in the real world well things like removing the anonymity period no but more seriously I think it's uh an important problem and something that's important to continue monitoring and there are good or there could be potential good reason of having that I think that it's important that to Monitor and check if those things happen in practice more generally I think this was like more of a side project uh in general I'm interested in the connection between data and Model Behavior and about the interpretability of model through the data uh and we would like to see more work on that will allow us to better understand models and understand the you know mechanism what should people type in Google in order to find your work my name uh probably can find out my website and uh all of my work would be done what's the best CLE paper you read last quote probably one of articus uh articus Geo he presented here one of his work about caal obstructions thank you thank you hi I'm uh Simon Bing uh my affiliation is Tu Berlin in Germany what is the work you're presenting here about uh so the work here is about um an identifiability result for causal representation learning where the main contribution is that we um Can soften a very ubiquitous assumption that previous Works make where we consider multiple environments for calls of representation learning and usually these environments are characterized by different interventions on the latent variables and uh previous Works a wide range of them assume that in each environment we can only intervene on a single variable so we have these Atomic or single node interventions and uh we present one of the first results where for unknown intervention targets we can allow for multi-node interventions so that means in one environment we can intervene on many nodes at the same time and still achieve identifiability yeah main results the main results are that we um Can characterize a specific notion of sparsity and kind of uh operationalize this notion of the sparse mechanism shift hypothesis that if we go from uh one environment to the next and we posit some kind of uh underlying Cal CM model that the change in terms of the mechanisms that are affected um by interventions from one environment to the next is uh sparse and by kind of using this basic principle in our setting where uh the key assumptions that we make are we have a linear mixing function from the latent variables to The observed variables and the kind of uh coverage of the uh of the interventions across the environments is sufficiently diverse and sparse in sense that I outlined before uh that then by regularizing precisely for this um for this notion of sparcity uh we can recover the underline causal variables up to the indeterminancy of permutation and element WIS rescaling what impact of this work you like to see in the world in the real world uh ideally hopefully in the long term um my group we are um thinking about how to apply causal inference and also causal representation learning to uh large scale climate data so we have very high dimensional measurements and how we can um yeah extract some uh C information or formulate CLE models about macroclimate phenomena and uh ideally I would like to see this being one of the um one of the bases for for which we can build some work uh that that can then be applied in the real world especially because we we have heterogeneous data but we collect it from different environments and we can't actively perform it to rentes on the climate as you might imagine um and so I think this moving away from the single node to the multi- node uh Interventional environment setting uh could be an important First Step what what's people type in Google find this uh multinode Interventional C representation learning what's the best Cal paper you read last quarter so last quarter uh one of my uh favorite CLE papers that I read was unsurprisingly about uh representation learning and it's called Uh identifying representations for uh intervention extrapolation by uh Sor s um from ET dererk where they have a a real use case for C representation which I think um could still uh yeah or was still missing before this thank you so much thanks a lot