we don't need more and more and more mthods if they all have the same failure mode we keep on focusing on one aspect of the problem um and ignoring another one because no one ever takes a step back and looks at what are we actually doing that's actually a question that I like to ask myself about every new project or every new setting that we study i' like to ask hey causal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine learning on the internet today we're traveling to Cambridge to meet our guest as a child she loved math wanted to do something more applied the first machine learning paper she read in her life was the one about Cal trees she loves rowing and swimming and has the courage to ask why machine Learning Works researcher at the University of Cambridge ladies and Gentlemen please welcome Miss Alicia court let me pass it to your host Alex molac welcome to the podcast Alicia thank you so much for having me a where are we today today we are in C uh UK um in my office in the CMS the center for mathematical Sciences in the department um of Applied math and theoretical physics which is where our group is based before we start I wanted to share with you that I'm really grateful for your work your papers and also your coding for for kets uh was was super helpful for me uh when I was working on my book um and I think the approach that you present in your papers um is something that we need more in the in the community looking at what has already been done and testing it from more perspectives gathering this knowledge um in my opinion IM immensely helps to move the field uh forward what was the source for you of of this approach taking this perspective of looking at trying to combine what we already know and and try to maybe stress test it against um different cases oh well first of all thank you so much that's a huge compliment I think part of it was just trying to understand what's already there because I think if you want to if we want to build more new methods that work better we kind of need to understand what the ones that we have already work well for um and what do they fail at and also to understand what is a viable strategy we kind of need to stress test different ones um to see what our promising Avenues and why so I think most of my research has just very much focused on the word why my machine learning is very good or the the literature on treatment effect estimation in machine learning specifically um had been very good at Building Solutions and I think machine learning as a literature often focuses on the showing that something works but never so much on why or when um and that's the the questions that I'm always most curious about I think just by background like from the more statistics ecometrics heavy backgrounds that I have in in these kind of literatures people often don't Focus so much on a single method but more on understand or like on defending a single method but more on understanding why specific methods work and when and that's something that I've been trying to do in my research throughout your most recent paper uh the one that will be presented in nurbs this year is not focusing on causality but it also based on the on the on the why question uh can you share with our audience a little bit more about this paper oh yeah um totally so indeed my last paper that we've published is the first paper of my PhD that really doesn't have anything to do with cality at all um it's it's called a u-turn on double descent um and we're looking at the Double descent phenomenon which kind of gained popularity in recent work because you you see this very counterintuitive finding that what we know from statistics that usually as you increase uh model complexity you have some kind of a u u-shaped trade-off between um model complexity um and model performance um but in this like new um double descent regime um You observe a second descent as um the train as the um number of um parameters exceeds um the number of training examples um and we found that somehow counterintuitive together um with the co-author we kind of asked ourselves all the statistical intuition that we've been taught um in our undergraduate or graduate level courses can't just stop holding right here at this interpolation thresold so we wanted to ask why does this happen uh cuz I think that there's been a lot of work on showing that it happens um but we've been trying to kind of find the root cause of this phenomenon to understand it better um and that's what we did in this paper we kind of found we found out that there's actually a kind of a mechanism shift happening um in the parameter increasing mechanism at um this interpretation threshold and that it happens naturally in these settings but the threshold itself isn't the cause of the phenomenon actually there's a different root cause and you can find that out experimentally and you can move the peak and you can kind of get back to statistical intuition uh and yeah I absolutely loved writing that paper it was super fun and it was just really cool little statistics Rabbit Hole to kind of go down mhm to give a little bit of a context to those in our audience who are less familiar with what's going contemporary machine learning this double descent is this phenomenon that model the loss let's say model loss is going down and it goes up but it drops drops again and and you gave a very very beautiful geometrical in a sense or geometric explanation for for this phenomenon um C can you give us an intuition how how you looked at this yeah um it's going to be kind of hard holding a microphone because I usually like to do it with my hands but okay so let's let's maybe dool um so usually this is this is kind of the shape of the curve that we've been observing so basically test error first decreases then increases and then decreases again um so this is this is kind of the double descend curve as as it's been presented in the literature um but what we're kind of a were're able to show is that for the examples that we studied in our paper it's not so much this one 2D curve but actually the underlying thing is that there's two separate complexity axes oh yeah it's better there's two separate complexity axes um and it's a 3D plot with two um orthogonal complexity axis and double descend only arrives because you Traverse them sequentially so actually you're basically you're taking a 3D plot and you're presenting it as a 2d plot yeah and that's how this um appears and that's actually you're really looking at a um 3D plot with two complexity axes that each have this convex this normal um convex shape that we're expecting and when you're like unfolding them that's when the peak appears um and like this intuition is kind of powerful because it also shows you that you you don't there's there's different points at which you can like unfold um this curve and depending on where you unfold that that like determines the shape that you're getting so yeah that was a really cool intuition to kind of come up with cuz once you're seeing that like once you've seen that it it almost becomes a trivial phenomenon so yeah that was really fun I had a great time writing that paper and yeah it was something very very different from what I'm used to working on um where normally it's mainly treatment effect estimation where is this which which is kind of a niche within um machine learning um whereas this is as a phenomen on kind of applies to all of supervised learning um so that was a really cool experience um working on something slightly more broader um and in slightly like a wider context than what I usually focus on MH I think this is a beautiful example and um correct me if I if I'm wrong but but I understand this as um the phenomenon that is possible because we are trying to project down threedimensional information to do dimensional space would that interpretation be correct yeah I think so um I think like the reason that that that it happens quite naturally to present it this way um is that there's something very interesting on so I say there's two complexity axes um and the two axes as far as we see it in in our work kind of play different roles um and basically there's always one axis where you have a limit to how many parameters you can add so the double descend phenomenon kind of appears because on one axis you're only able to add n so the number of examples that you have parameters um whereas on the other axis you can kind of like indefinitely increase parameters like the fact that you you actually need to switch axis at some point because you you can no longer actually add parameters um in one way so the my favorite example is the first example we give in the paper which is um an example about trees because a tree can only ever have as many terminal leavea nodes as there are examples in your data set so once you've reached full depth trees if you want to add any more parameters you're going to have to do something different um and so like in the original double descent examples what they did is to add more trees so basically increase the number of parameters not through adding more leaves to a single tree but by adding more trees M um but quite obviously that's a very different parameter adding mechanism um than adding single leaves kind of once you realize that that's what happen what's happening in trees and we then went back to what the what was happening in the linear regression example where this isn't obvious at all where you need to understand kind of some connections between minor Solutions and singular value decompositions and PCA regression there are once you realize it you realize oh there is actually also some kind of two-dimensional space and you're only transitioning um once you've reached n but you could do this at other points as well I think it's a great example showing us that we don't always are very good at understanding what is going on in within those models yes in your work on uh conditional average treatment effect estimators or heterogeneous treatment effect estimators you also looked into different mechanisms a little bit deeper uh in particular things related to the function of how how complex the function of one outcome versus the the outcome is and how complex the difference between them is now could you share what would be the main insights from this work um yeah I think the main Insight from most of like my early work on heterogenous treatment effect estimation was that what type of treatment effect strategy works best really depends on what you what your assumption is on what the underlying problem structure is so like broadly speaking you can estimate so a treatment effect is just the expected value between the difference between someone's outcome given treatment versus not given treatment that's kind of our treatment effect um and you can estimate this in two ways you can estimate this either by first estimating the expected value on under one treatment and then um estimating the expected value under the other treatment and then take the difference between these two estimates um or you can do something slightly more involved which where you're basically estimating the treatment effect directly without as having as byproduct um these two expected values separately which of these two strategies work best in all the experiments that I've conducted and I've done quite a lot of simulation studies on this usually mainly depends on whether the learning problem of just learning the treatment effect itself is technically speaking of simpler structure um then learning the two functions separately um whenever you have a simpler treatment effect um then the potential outcome separately is that like direct targeting a much better strategy because you're you're being much more efficient kind of and direct targeting is again for those of of of us who are less familiar with the term is this idea to looking at the at the Cade so the average effect directly rather than through the um differencing between the potential outcomes exactly exactly you came to cality uh through a path that is maybe not very usual uh for for most people in the field so your first machine learning paper that you read in your life was a causal machine learning paper by by Susan Effy how did this shape your perception of the field later on in your in your career oh that's a very interesting question um yeah indeed so I came at um I came to machine learning via treatment effect estimation from like the policy economics background that I had from my undergrad so I I indeed I learned I I kind of learned about machine learning methods as tools to infer treatment effects so it's basically the other way around and what you normally do where you first learn about machine learning methods and then you see oh how can I apply this to causality for me was more here's a causal here's a problem that I have heterogenous troping effect estimation how do I solve that with methods Beyond linear regression which is what I think what we usually use um in econometrics and I think maybe how how this background from causality has shaped my approach to machine learning is I think specifically in econometrics you think a lot about what are the assumptions that I need to make things work um and under what scenarios do things work um very well because often the the questions that you're looking at in policy economics or but also in BIO statistics are often they often appear in kind of safety critical um environments where if you're making policy or treatment decisions based on something you've estimated you better be sure that you've estimated it correctly um so I think that's kind of that that's influen very much um how I go about evaluating machine learning methods I like to look at I like to be sure why things work and when things work to be confident that they will be working in practice in under different scenarios is that something that also inspired your your recent work on on non-causal machine learning the paper that we discussed uh you definitely definitely I think in because I have a bit of an applied statistic background I think there was a lot of emphasis always put on building intuition as to when and why you expect things to work and I think I don't like this modern machine learning regime where we're currently in and we say well statistical intuition doesn't seem to hold um we need something new but I like statistical intuition um I like what we've built throughout the last years and years and I want to get some of that back because if you have this intuition you can maybe be a bit more sure about when things will work when things won't work I don't like seeing machine learning as like this magic bullet that defies the laws of Statistics um that we know because I think that cannot be that to me cannot possibly be to provide there is no such thing as a Freel lunch nothing will always work there's so much value in understanding especially failure modes um of machine learning methods be it for causal inference or for prediction or something more generally I think that's so important and I think we we're like falling behind a little bit um on that part you work on um worked on causality in in health in the context of health and as you mentioned before this is one of those safety critical um applications where the cost of of mistake or the cost of mistakes was can be very very high from Financial ethical maybe first and foremost point of view do you think that um building reliable automated decision systems that for building those system causality is is necessary yeah I think in some form or another some kind of knowledge on how a system behaves under intervention is necessary MH is it sufficient I guess depends on what you mean by causality if you have a perfect model of like the underlying SCM of the world like I think it would be it would be because as soon as you've like captured everything possible in your model about the world you should be able to make decisions on any type of intervention that you'd want which is I think what you kind of need but I've no never thought about this before it's a good question what would you define as um as a main challenge in in your work with uh with Kate models challenge in what context or in what sense technical or applied or or all of them I think I think I want to leave it open yeah just what what seems the what was what seemed seemed like a biggest challenge for you in this in this work that's actually a question that I like to ask myself about every every new project or every new setting that we study I like to ask what makes this problem setting unique what are the unique challenges that appear here that maybe don't apply in like a purely predictive setting um so in the the Kate estimation context just from like a um like a technical standpoint I think the there's two very interesting things that are happening there um on the one hand if you have treatment of assignment biases you have this kind of coar shift between two the two groups so if the the group that receives the treatment looks very different from the group that doesn't look doesn't receive treatment if you're kind of fitting models to that you you might suffer from some um effects of coar shift so that's been a huge theme in the literature looking at this effect of the coar shift between groups and then the second thing um that I've looked at mainly in my work is this problem that you're that you're not only have cerative but you actually have a label that's missing like the label that you're really interested in is the difference between potential outcomes for an individual so basically the difference if I were treated versus if I weren't treated but in reality I can only ever observe one of the two so the the actual label that I'm interested in is unobserved and that makes learning quite challenging but also quite interesting um so that like as a technical challenge I think that these are the two unique aspects of the like treatment effect estimation problem relative to a standard prediction problem um but I think what then it comes in in addition if you actually want to bring any of these models to practice I think the like the evaluation problem um is the one that I think kind of brings the biggest challenge for actually deploying these models because you never have access to these two potential outcomes in practice and because the assumptions that you need are untestable um it's much harder to validate whether a like a heterogenous treatment effect system works um when you want to deploy it in practice um and I think that's like that that's like the big hurdle that I see for taking these things these kind of systems and like deploying them in practice in like safety critical applications um that it's so hard to be sure that it works very well given that the label is unobserved and you have coar shift in your data in in CLE Discovery literature we have this paper by by Alexander rar and which is titled beware of this simulated dag in this paper he shows that the way we construct synthetic data sets may impact how the models perform and the clues that come from the simulation process might be exploited by those models and the problem or the challenge here is that in reality those Clues might be missing you find you in one of your papers or in one of your research projects you found um something similar in spirit I would say regarding uh heterogenous treatment effect estimation can you share a little bit more about the insights from from this work yeah so this is uh an Europe's um 21 paper I think called really doing great at estimating Kate uh where we just have a critical look at the benchmarking practices in the field and I think that the findings actually very are very similar um to to this project that you just mentioned um which is more in the context of causal Discovery Because treatment effect estimation because you have this missing data problem you kind of need to simulate your um your data um to have some ground truth to evaluate against if you're doing benchmarking um so people have have been kind of relying on the same simulated data sets um and what we what we showed in that paper is that basically the problem characteristics encoded in these simulated data sets very much favor a specific type of estimator over another which I'd say would be fine if these problem characteristics were at all really like rooted in real um data generating processes that we expect in the real world but I think this is not the case so the some of the some of these Benchmark data sets have what I consider kind of random not necessarily realistic outcome generating processes there that very heavily favor one type of estimator over another that I don't necessarily think reflects the types of data generating processes that we'd actually see in reality so I think there's like a big lack um in this context of having some authorative statements on what are the um what are the likely data data generating processes to be observing in reality to have some kind of better benchmarking test beds given that it is so hard um to um validate these models on real data so you mentioned um untestable assumptions that we need to deal with in in causal models in general and in in many Real World Systems uh it would be very difficult to especially the open ones and and complex ones to validate that the graph that we are passing to the model has a good um correspondence to to to reality at the other end of the spectrum we have this idea of testable implications coming from from Judea pear if we have a structural causal model uh we can generate the data from this model and see if this data resembles what we observe in in reality that in particular might be useful when we are able to even Min minimally intervene on whatever the the real world system is because then we cut off the space of possible solutions right and so on what are your thoughts about about this direction um in particular in the context of model evaluation yeah I think this is a really great Point um the so there's at least parts of the the the treatment effect estimators that we can indeed um evaluate so for example in this like K estimation setting you can indeed use like if you have a model that also outputs potential outcome predictions you can at least check these potential outcome predictions against the factual and have this as like a first step of model evaluation to at least say okay well the predictions that it generates at least resemble the real The observed data to some extent um I think the only slight problem that I see with this is at least in my own work like slightly more recent paper we had an icml 23 Paper on model selection different model selection criteria for treatment effect estimation um and what we found is that the models that perform best at predicting outcomes aren't necessarily the ones that perform best um at predicting treatment effects um especially if you're in like low data aines sometimes there's a bit of a trade-off between getting doing very well at fitting a possibly very complex regression surface for the potential outcomes uh relative to um fitting a treatment effect itself well because the a treatment effect is the difference between two predictions if you're making the same error on both predictions that will cancel out so there's a question of do errors cancel out or add up once you take um the difference between these two predictions that just doesn't really have testable implications but I still think it is a very good idea to um if you want to use any of these models in practice to indeed take as a step one the testable implications and look at that you're at least in the right ballpark um of getting the predicted outcomes right so so what you're saying is that uh even though we can learn the functions of potential outcomes pretty well but still with some error this error can actually uh be magnified when we compute Kate based on those on those those yeah or cancel out or cancel out and that's the thing that we just don't really know having said that of course once we're in like very large data regimes like once we get the potential outcomes perfect perfectly right um then the treatment effect is obviously also perfect so like in in the limit doing well at treatment effect estimation and doing well at potential outcome predictions is probably like the same it's just like more like in lower data regimes where you have more Pices I think um it becomes harder to judge from how well like potential outcome predictions do to how well treatment effect estimator would do you started your journey with with causality from the grounds of potential outcomes framework um but I know that you also uh familiar at least to an extent with with pean framework what would be the best tools from those two Frameworks that you found you found the most helpful in your in your journey so far um yeah I think so I think causal graphs tags um are just incredibly useful as a tool to represent your assumptions um especially also to communicate um with stakeholders that maybe are not machine Learners or statisticians um I think that they are like very appealing to go talk to a doctor for example to talk about if this is my outcome if this is my treatment what do you think potential confounders that there could be and do we have them measured um I think it's a great way of communicating and it's a great way of depicting problem s that have slightly more complex structures so any types of biases that arise by due to causal structures that are not your simple treatment outcome confounder kind of three variable setup and also as soon as you get into like as your problem becomes gets more Dimensions so like if you um um uh at time for example um then a causal graph is very very useful to understand um I think the different patterns that you could have so we for example we looked at um treatment effect estimation with survival outcomes in the presence of competing risks so if you're interested in let's say a cancer survival outcome um but a patient could also die of um a competing um cause for example a cardiovascular um cause in these kind of settings without a causal graph I think you're kind of lost in understanding what all the different paths that there could be of effect so I think that's where the uh where that comes in extremely handy um but I still very much like um using potential outcomes to just reason more about like the estimators itself what would be the helpful features the most helpful features of potential outcomes that help you think more clearly about about estimation is it about the fact that that potential outcomes are like Primitives in in this framework and just like maybe yeah I just like to think about like the a treatment effect as the difference between two potential outcomes I think that's just a very very useful way of depicting things um I personally don't really think that the potential outcomes framework um and the per and dag World stand in any contrast to each other at all I think there's also there paper showing equivalence right so I I I think I just like to write in terms of potential outcomes um but I like to I think it's very useful to to depict um problem structures um in terms of causal graphs what was your original motivation to go into into to economics and econometrics I'm not sure how much thought was put into that at the time obviously I made this decision when I was 18 years old I just wanted to study something related to math I knew I liked uh math a lot um and I wanted to do something that also has real world applications so I just looked into like applied more like applied mathematically oriented programs um I came across econometrics I don't think I knew what I was getting myself in when I chose it at all actually I think I was expecting like 50% economics like 50% math what it actually was was about 80% statistics and yeah a little bit of Economics a little bit of other mathematical Sciences but it was really mainly statistics um I don't think I would have chosen it at 18 had I known how much statistic it was because I thought I didn't like statistics just of because how it was taught in high school um I don't I don't necessarily think my like in my experience at least statistics was something that was like particularly attractive um in school um but I loved it I'm so happy that I that I went down that path turns out I love statistics I think it's a very cool way of understanding the world so yeah I just kind of I ended up in econometrics a little bit by accident very very happy accident and yeah I just I just liked it a lot I think um there's there's just something to me there's something magical about statistics in where you you learn you learn why things work but when you actually see when you actually see that they do work in practice I always think it's cool like things like the law of large numbers you under like you prove why this works but when you like in in reality see um something actually converting to the mean as expected I still think there's a little bit of magic to it I like it a lot do you think mathematics is a accurate description of reality or is it just a useful language that helps us structure our experience great question um I guess I also don't have such a strong mathematical background per se but um I think statistics specifically is a great way of dis like gives of giving us a language of describing processes in the world like statistics probability Theory I think it's a very nice way um of describing how certain processes just work in the world um and I like that a lot when you studed uh economics or econometrics does the experiences from uh from from this time of your life um was something that was also useful for you when you moved into working more in the context of of of Health I think so yeah I think I mean ultimately like I consider myself more of an applied statistician where the applied can can apply to any um application and I think the I mean the problems that you encounter are the same you just give them different names like the VAR Ables like whether I'm inferring the effect of a policy as you do in like policy economics or whether I'm in inferring the effect of like in Medical Treatments the like statistical properties of the problem that you're trying to describe are the same um so I think there's actually there's many many many parallels um between the two Fields there's also differences in assumptions you usually place on um problem structure which is I think where it becomes very interesting um or necessary to have domain experts on board to have you understand like but what are the most likely ways in which certain assumptions can be met but I think just like overall I think everything I learned in econometrics is still relevant in what I'm doing right now because it's it was indeed just a way of describing understanding the world um and whether I'm trying to describe an economic problem or a medical problem the the like statistical skills that I need are still exactly the same what do you think is the the future of of machine learning and and causal machine learning or causal analysis in more general terms it seems like the causality is slowly making its way into the more mainstream um machine learning literature where I feel like it was a bit of a niche area um but it's I think gaining quite some traction because I think if you look at questions of like generalization people are slowly starting to realize um that there's a lot of causal thinking involved in going to new settings um and similarly any anytime you want to build any system that takes actions like taking an action in some ways is performing an intervention on your environment so I think there's there's huge potential to take ideas from cality and I people are obviously doing it to take ideas from kazal to make any kind of autonomous systems better if you could imagine that we could solve uh just one challenge in causality today uh which one which one would should that so for me personally um I think it would be the biggest challenge that I see that I would love to see solved is is coming up with better ways um of evaluating that things work in practice um so coming up like if I could build a model and then put it in a test box and someone tells me this is safe this works um that would be great um and I think this goes kind of hand in hand with just something like because ultimately what I need is someone to validate the assumptions because these are untestable assumptions either on identification or on like the problem structure um what I need is someone to tell me the assumptions that you make are very likely to hold here um so to have some way of like mapping um domain expertise into something that can validate my assumptions I think would be great I had a conversation with with Steven S recently recently um was a statistician as well working with experimental data and Drug development and so very very much uh safety critical applications and we had a short conversation about Fisher about randomization and how people and myths about randomization that um some of them are maybe more some of them less prevalent in the community mentioned that when Fisher was thinking about randomization he didn't think about it in terms of how not to be wrong but rather on quantifying how wrong we are do you think that this idea could also somehow be translated to the challenge of caal model or Kate model heterogeneous treatment effect model evaluation maybe in in a way um this relates to being able to at least put some kind of bounds um on on how certain we are um of of treatment effects being in the space of where we think that they are so kind of some like maybe sensitivity analyses go in this kind of Direction uh where you put some kind of sensitivity model on your data sorry not on your data on sensitivity model on your dat models um and then um use that to inform at least some kind of bounds on say if if my unobserved confounding was this or that strong this is how much that would impact my estimates and whether that would like flip their sign I think there's been some very cool recent work over the last like 2 three years on um using modern like modern machine learning ideas for example some cool work on using conformal prediction intervals um to bound the effects of um possibly unobserved um confounding I think it's very cool you you did your uh Masters in in in Ox at Oxford and now you finalizing your PhD here uh at Cambridge what were the most important uh things traits or characteristics or habits something you you've done that helps you to go through the cause of all those challenging uh studies oh um I think curiosity is probably uh definitely one of them I think often when I when for me when I don't understand why something works I really want to know I I need to know I need to understand it so there's like I think a healthy portion of just C I to get myself through it's like I don't think I could do what I did if I didn't love it I think there's a lot of like you you need to love what you do especially in a PhD um I think um to keep yourself motivated yeah I I also just love statistics I think it's very I love learning new things I think it's very fun um so like especially throughout the Masters I think just I found it fascinating to learn about things I didn't know anything about I think especially coming from econometrics where you have more it's a very parametric approach to s statistics um and then in my masters in Oxford um you learn I learned a lot more about nonparametrics and about U machine learning that was fascinating to me just kind of a parad joint di shift um and then during my master thesis I learned a lot about semi parametric statistics which really cool and yeah then coming here I think I always in every project that I did I just tried to learn something new about some some area um that I didn't know anything about say in um there there are lots of synergies between different fields um in like treatment effect estimation I first learned a lot about domain adaptation because that's where the coar chift problems um appear then you learn a lot about multitask learning uh because there's lots of architectures that actually just built on multitask learning learned a lot about survival analysis competing risks bio statistics uh there's just there's just a lot of things to like learn new things about and I think I just have enough curiosity to be kind of yeah to keep going you mentioned this uh intersection of many different fields of field yes related St Fields when you look at the causal Community today what would you think would be the most beneficial thing for the community to learn outside of causal analysis itself that could spark maybe um more inspiration that could help move the field forward that is a great question uh I'm not it's a very heterogenous field itself right and um I don't I'm not entirely sure if like so I'd say other fields have a lot to learn um from the fields of quality because people working on quality are so head heterogenous it it's very difficult to tell what like difference people need to take from where um yeah I don't I don't have a good recommendation how to average them how to average US yeah yeah I know but it's right CU there's there's it's such a broad field you have people that work more in estimation you have like statisticians econometricians bio statisticians computer scientists um like causal Discovery who am I to tell them what they don't know yet um so which of those fields were most helpful for you for yourself um so I learned most um I mean obviously from econometrics I think I took a lot um and then I just been very fascinated by um learning more about bio statistics and like the semi- parametric statisticians like the way that they formalize problems I think is very cool especially as an econometrician I come from a world where every where everything is highly parameterized where we usually like a treatment effect how I was taught it is a um as a regression coefficient in a linear model um and then when I started learning more um about how the bio stat in BIO statistics how things are presented especially by like like semi- parametric statistics literature where you think um of treatment effects more as like a functional of a statistical model um where you think in in terms of like expected values of differences between potential outcomes and stuff that that was to me that was mind-blowing um because it kind of detaches that it makes it so much easier to reason about cality if you don't also have to go this step through this this this weird parametric linear model that you no one really believes holds anyway so what's the meaning um so that was really cool for me um the the like the bat literature I think um there's so much interesting stuff in there what two or three books uh had the most impact on you and has changed your life the most I'm I'm going to answer this from an academic standpoint now um so I think so I read the book of why uh I think in 2019 as an econometrician who had only learned about cality within an economics um like with within an economics framework um so that was super interesting to me because yeah I think I was predominantly taught in like a potential outcomes kind of setting it was also the first book that I've read where I kind of felt a little bit offended by the book itself because obviously Pearl makes a couple of little digs um at economists and whether or not he thinks they are able to reason about kazal well in that book but it was just so interesting to me um to think about cality on a bit more of a philosophical level like counterfactuals for example like true rung three counteracts is not something that we had visited at all um in the courses um on like impact evaluation that I had taken um at University so that was definitely uh super super interesting to me then while I was doing my master thesis in 2020 um I started to get more into like this like the semi- parametric statistics yeah way of thinking uh and I read these parts of targeted learning by Mark folan and um Sheri Rose and I just love I absolutely loved it blue my mind how they introduce how they introduce a completely different way of thinking about like Target parameters like the esans like causal causal quantities how they could be of interest to you I think that that completely changed how I thought about statistics um I think that was just really really cool um and then I think just my favorite book on uh statistical learning where generally must be elements of statistical learning uh I think the way um like as a textbooks goes I think that's like the my goal with any of my research would be to to write papers and anything down in such a way that it's as intuitive um as they make it see um I think it's a great great book what would be your advice for people who are just starting with causality and they maybe feel a little bit overwhelmed that there's so much to learn in order to make any progress in this field for me it was kind of like I started quite naturally slowly in it just from like Eon like economics I didn't even know how much stuff there was out there so indeed I think it's it's about finding a way to get the intuition right first I think so I think the book of why is actually a very really good like gateway drug to cality um I think like because it's just in a popular science way like written in a way that it's actually appealing to a more General audience um I think otherwise other um introductory books by people that have actually taken the time to possi come up with a way of presenting these things that are um that are a bit more intuitive this book for example uh thank you um I think that that would be I think a great way uh because I think there's an overwhelming amount of literature out at this moment um I think it's very hard to go in like at what the state ofth art is yeah but I don't don't have a like great advice yeah except for maybe looking for some courses as well um that actually teach you the the bit of the the basics who would you like to thank who would I like to thank um oh uh wow that's a great question um if I like for Life uh like academic life um I have big big a big thank you to uh my undergrads institution uh in econometrics in Rotterdam I think they did such a great job at teaching us about intuition um and I think they uh they put so much focus on making sure that the students are happy um which I think we didn't appreciate at the time but like um I had a great time Ram so more more generally but like specifically that was a our uh study coordinator or well the person who who really took care of like the education committee and everyone um Professor Christian high who also wrote what we call the hyu econometrics uh for business econometric methods for bu business and economics I think it is called um great book he taught us like econometrics one great great professor as well as um the I took a course in rdam um that's called impact evaluation um by professor professor from kipl um who said in the opening lecture he was like he studied econometrics here a couple couple years ago um and he kind of felt he was missing a course on on actually talking about impact evaluation like how do you use the methods that we we've been given to actually evaluate policies so he made that course and that was the best course I've ever taken um and yeah I had a great friend in undergrad his name was Thomas Thomas vman who's now doing a PhD in Chicago who introduced me to all of the machine learning uh for treatment effect estimation so he kept on sending me the papers by uh uh Susan a and specifically uh um on what kind of how you can use machine learning for causal inference uh and without him I would not be here today and then obviously without people like Susan Ando invens who've written these really cool papers on um using machine learning for economics I also wouldn't have started on machine learning for lots of people to thank but these are just some of them of like in terms of early uh um like just early academics I think Alysa before we conclude yeah I would like to take a step back and and go back to to to your research now in another research project that we haven't mentioned before you are looking at from the causal point of view at models or situations that have like a higher complexity so for instance if we add time Dimension what happens one of very popular methods in in the context of Health um and um and policymaking as well is sensitivity analysis what are your main insights from from from your work regarding sensitivity analysis as seen from the causal point of view I think yeah since sensitivity analysis are like a really interesting tool that I haven't used a lot myself um like not not massively uh much but I think there's a huge opportunity um to use them more to kind of perturb our the assumptions that we're making and and seeing what would happen if the our assumptions are held a little bit more or less so yeah I think there's lots that could be done in like KY but also in machine learning more generally by playing around a bit more instead of focusing on like Point identification like making individual predictions like Point predictions saying like oh here's a set of predictions that I think is most likely given certain um assumptions so I think that's a very interesting direction that I haven't made a lot of use yet myself but um I mean in treatment effect estimation you usually put like sensitivity on like some kind of sensitivity model on the um amount of like hidden confounding but there are like there's many other um aspects that real problems have that you could also um put these kind of sensitivity models on so I've worked a lot on Survival data um and in survival you have an additional complexity which is like the presence of sensoring and similar arguments and questions apply there is like is is your sensoring ignorable so is are is there no kind of hidden confounders that affect both answering and your outcome variables um or missing this is another is another question like we in all of our problems we assume that data is completely observed um but often actually we don't record everything so putting some kind of model on um how data is observed it would be another way of looking at this so so looking when we think about sensitivity analysis uh even if our causal model is well specified in the beginning of the of the time interval that we are looking at it might get confounded with with time yeah I think overall looking at some kind of like of like error propagation over time um of like how bad is it if my assumption isn't met in the beginning versus how how how do like errors compound um over multiple times I think is a very interesting question that I haven't thought about yet myself at all um also have not worked a lot on time Ser setting um particularly anyways but I think it's a it's a very interesting direction to think about what are like additional complexities that you can add to models cuz I think that in like the treatment effect estimation literature in in machine learning specifically I think we focused on like a very very small part of real problems with this characteristics that could arise right we've like picked we picked this like teeny tiny area um and we we we keep on looking at how do you estimate heterogenous treatment effect when you have like a single treatment yes or no and an outcome that's kind of simple and continuous maybe in your static data but there's so many more problems out there I think most most real problems actually have five six more axes of complexities you have time you have different outcomes you have multiple treatments uh treatment combinations U missingness like informativeness and how things are sampled informativ and when things are measured um so I think there's overall a huge um opportunity to look at kind more realistic problems wider classes of problems and um that is also something we've tried to do over the last couple of years to like one step at a time like look at one extra level of complexity that you could add to these problems what would be your main insights from this work in general very interestingly most additional problems that we've considered be it survival analysis or um competing risks missingness um censoring informative sampling all of them ultimately come like come down to a missingness problem um you're just you don't observe everything you want to observe and the more layers of complexity you add the more missingness there is actually the less OB like if you if you have censoring missing outcomes and treatment selection you you actually of of all the potential outcomes that there are you actually observe less and less so it becomes very much like a sparcity problem of I'm observing a lot less than I would want to and there's like coar shifts that are induced by all these different missing missingness mechanisms so I think there's some very interesting opportunity in looking at how do you tackle all of these missingness problems jointly um because that's something we haven't done yet um like I like to look at problems one at a time because that makes it kind of easier but actually there's probably huge value in looking at them all together um because they all in some ways are kind of the same problem that manifests in slight different ways so I understand that that that you are proposing to to look at all those challenges as a missing data problem of treating the missing data um framework as a unifying perspective that's that's a little bit in the spirit of of what Donald Rubin EXA exactly I mean there there's definitely there's a reason why the why Donald Rubin worked on causality and on missingness I'm assuming not that I not I know but like I'm assuming there's a big there's definitely a reason that like in BIO statistics these are usually all treated as like unified um unified problems because they like ultimately they are all missing this problems where you maybe assume slightly slightly different causal structures um like you you change what causes what if missingness causes like if if basically if treatment um if missingness because in a in a like a treatment estimation problem the missingness indicator is kind of your treatment assignment mechanism or your treatment assignment actually um so in that case the thing like your treatment indicator you do you do assume that it has an effect on outcomes whereas in a typical missingness problem you usually assume that there's no effect of missingness on outcome um so there's slight differences in what you assume of like what causes what um and what you slight differences in what you need for identification um and like what the what the like Target parameters of Interest are but ultimately like statistically speaking if you look at part of the structure they're all very very very related which is is why some people like Donald ruin um Robins or um Mark fan why they've all studied so many of these problems because once you once you realize how related they are why not treat them all as like one class with all of those uh challenges you you speak about some structural information information about the process or how one thing or one variable in the model can impact the the outcome or or another variable and so on and so on the structural approach is very far away from what we usually what we got used to when it comes to uh the machine learning uh community and the culture of machine learning publishing we rather look at empirical results and we have very very fast publication or pre-publication Cycles feedback cycles and so on and so on what are your thoughts about these two I feel like slightly contrastive approaches yeah so I think the what I really like about the worlds of statistics and econometrics the in the publish like in in Publications it at least seems to me that it's often a lot less about like performing horse races but where you have a horse in the race yourself where you don't you don't usually bring your like machine learning research seems to be a lot about like achieving the state-ofthe-art my attention horse is running faster than exactly exactly exactly so it's bringing something new and showing here I can make I can make this line bold in my results table CU I've beaten everything else whereas at least how I feel about like like literatures and like econometrics statistics um you focus a lot more on understanding structure of a problem and what kind of solutions you need to solve it so it's a lot more about like the problem itself and understanding it um and I think that's like what machine learning is missing hugely and like I try I try in the papers that I write I try like I'm mainly interested not in building methods but in understanding the methods that already exist and why why they work because I think that's kind of lacking but it's been really hard as a PhD student writing papers like that because I think I'm yet to send a paper to a conference and not have at least one reviewer question the novelty um of what I'm doing just because then the specific novelty that like this community is often looking for is like in the architecture it's in the method itself whereas I think there can be like novelty in else where um if if you're getting new insights into a problem I think that's also novelty that's worth publishing but that's kind of very hard to get through a review process because we're kind of as a community we're on the lookout for um novelty like in methods very specifically and I think that's a problem um because we don't we don't need more and more and more methods necessarily like if they all have the same failure mode for example if we keep on focusing on one aspect of the problem um and ignoring another one because no one ever takes a step back and looks at what are we actually doing here I think that's that's kind of what I see lacking and so there there's a paper from a couple of years ago called troubling Trends in machine learning scholarship by Um Zack Lipson and I think ja steinard um it's one of my favorite papers I've ever read um because it talks about it talks about all the things that are kind of going wrong in machine learning research um well at the time and I think kind of still true today and one of them and I think this is the one that I've looked at the most is um that they talk about how people don't really think about sources of gain like you show okay you show I beat the current state-ofthe-art um I beat The Benchmark but not necessarily why so if you're if you're proposing a new architecture and let's say it has like it changes five things from the previous architecture if you're then not like a blading the sources of gain you've not really learned anything maybe it was just the way that you were optimizing like maybe it was just the step size um you know um that you were using like how you were optimizing hyper parameters that's something that I dislike about how research is going at the moment um is that there's such a big focus on methodological novelty but not so much rewards for kind of trying to understand problems better is the future causal hopefully at least partially at least causality like I think the near-term future is probably causality inspired you know like at least I I I um love the types of research that at least taking some ideas from kazal like um for robustness or transfer learning um taking some ideas of like the um like invariance of causal mechanisms these kind of things to build better um or like more stable models and I think yeah at some point maybe it's fully causal um where can people find more about you uh your team and your your work I mean I have a website myself where you can find something more about me um I mean obviously all luckily machine learning research I think if there's one good thing about machine learning research is very public right there's because we're publishing like usually both on archive and on conference venues everything the the work that we do is usually not hidden behind paywalls which I think is kind of nice so like our papers obviously very um very public um so there's like my own work is on my own website but then we have I'm obviously part of a much bigger um lab supervised by Professor Mahala fanar here in um Cambridge and our lab I mean we there's there's a part of us that works on quality but we I think look at machine learning for health much more generally um and there's obviously the website of our group gr as well that has like all these different research pillars where I'm kind of part of the research pill looks at quality um but there are also people looking at other things so if people are more interested in our work on like machine learning for health more generally you should definitely check out um our website I think it's the funer shar.com we'll link to to initial description yeah um yeah what what's your message to the Cal python Community uh keep asking why I think the I think there's a maybe one the reasons um that in my in my own work I look so much at why do different methods work so well that's also a causal question ultimately right I'm looking I mean I'm looking at treatment effect estimation as a causal problem but then also in most of my work on like simulating and evaluating methods I'm also trying to find the sources of gain so like root causes of why models perform better so maybe there is something to it that um one of the reasons that the the like machine learning literature on cu it is slightly different than other areas of machine learning is that we're very trained to ask why like just distilling uh correlation from quation and yeah I think it's good good to keep asking why uh examining our assumptions IA thank you so much for your time that was a great conversation thank you so much for having me thank you thank you congrats on reaching the end of this episode of the caal bandits podcast stay tuned for the next one if you like this episode click the like button to help others find it and maybe subscribe to this channel as well you know stay causal