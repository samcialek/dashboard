Let's imagine your friend Paul decides to supplement vitamin D. Paul is in his 30s and the doctor recommends him to take 15 micrograms of vitamin D per day. Next week I visit my 97year-old aunt. Should I recommend the same dose to her? Popular recommendations that we can find on the internet would suggest that the answer is no. And it intuitively makes sense. My aunt's much older than Paul. Her body size and metabolism are likely very different from his. A different recommendation sounds natural.

And yet, in some other areas, we tend to ignore similar differences between people almost entirely. For many years, manufacturers designed cars to be safe for the 50th percentile male. According to a Stanford University report, we introduced female crash test dummies in the 1960s, but they were just scaled down versions of male dummies. Moreover, they represented the fifth percentile of females in terms of body size and mass. In other words, the smallest 5% of women in the general population. Heterogeneous treatment effects are effects that differ across contexts or people.

In today's episode, we will discuss whether they exist, what it means to researchers, and whether we should talk about them more often. Ladies and gentlemen, please welcome Professor Richard Han and Professor Steven Sen. This episode was recorded during the 2025 edition of the causal data science meeting and takes the form of a semiructured debate. I hope you will find this format useful. Enjoy. >> My name is Richard Han. I'm a a statistics professor here at Arizona State University and before that I started my career at a business school uh where I was working with econometricians and that was when I got the fullbore uh interested in causal inference um and recently I have been interested in applications to medicine and I'm sort of branching out um reading Steven's work and um I'm really glad to be here.

>> Thank you Stephen. >> Yeah. Hi. So I'm Steven Sen. I'm a medical statistician. I originally studied economics and stats and decided economics was not for me and then I decided computing and stats and decided computing was not for me. So I'm sort of a statistician by fault of having eliminated the things I didn't enjoy but I really enjoy statistics and I'm looking forward to this session. >> That's really great. Thank you so much for for for the introduction. We'll start with the first question.

Yeah. But before that I will also run run a Python script here that will pick um one of you who will who will answer any given question uh first. So the first question is very general and it's uh it's a question that we've that we've seen in the title of the session. Do heterogeneous treatment effects exist? And our pseudo random number generator picked Richard uh to to answer this question first. Yeah, we started talking about this a little bit in your absence. Um, and I was saying that I I hope that the talk the talk title is hyperbole because I think that to some degree we all agree that they exist.

Uh, and really the question is how useful are they? So in your advertisement for this you talked about automobiles um being designed for an average person and in fact that's not a good idea. There's a a book called I think the end of average that uses the example of uh fighter planes. It's very similar sort of example. Um they were trying to design a cockpit that worked for everybody and in the process they design designed one that worked for nobody. So yeah. Yes. I I think they exist and as statisticians a big part of our mandate is is reigning in our enthusiasm for searching for these things.

as Stephen mentioned, we get um we see patterns where there are none and that can be really wasteful in terms of resources and all sorts of things and I'm just I'm kind of interested in finding them anyway. Um I'll give you a quick personal story uh sort of how one of the things that launched me on this was that a couple of years ago at age 38 my wife was diagnosed with breast cancer and she's doing well now and everything's fine. and it was not a tragedy in the end. But when we were researching tmoxifen, which is one of the main drugs that is used for the prevention uh after primary treatment, um we were looking for something that was appropriate to her, sort of the what's known in statistics as the reference class problem.

And so we had survival based on tmoxifen, but we couldn't find it based on substrata basically. So we could find it by age, we could find it by tumor size, we could not find it by age by tumor size. So even when you just go two levels deep, the data is just not there. It doesn't appear in papers. It tends not to be analyzed or broadcast. And for patients, our feeling was that was the information we really wanted. Um and so the interesting thing is that that data is available. Uh meaning it was produced at some point um and yet it was not around.

Um so anyway, I always have that sort of example in the back of my head. If I'm a patient and I've given a diagnosis and it's ambiguous because these drugs don't work 100% of the time, I want to know the the right reference class. And so that's how I've been thinking about them. And in my experience, this is not an uncommon thing. >> Great. Uh thank you so much. Uh Richard Steven, it's it's your turn now. Yeah, of course I agree that um heterogeneous treatment effects exist, but the problem is um finding them and treating them appropriately given that we have finite resources and we don't have infinite data.

Um and I think that medical statisticians have failed to take a cue from the quality control movement >> there. It's well known that if you interfere in a process and you continually try resetting a machine based on very very small samples, you just add to the variation of the process. And we become obsessed by the fact that patients might vary. Actually, there's a much much bigger source of variation within the whole healthare system and that's doctors and getting a handle on doctor variation would make a very big contribution towards treatment.

You will find if you have a look at particular regions within the United Kingdom that um the rate of mastctomies varies very much to take the breast cancer example from one particular center to another. It's very difficult to explain this in terms of patient characteristics. It's probably in terms of physician surgeon preference, but they can't all be right. So this is the problem you're up against. So my feeling is that naively going too quickly to trying to get yet another smaller and smaller and smaller and smaller reference class can actually lead you to estimate things worse.

The estimate may be unbiased in some sense, but the variance is so large that you're actually doing very badly. >> Let me find out who will be answering the next question first and this will be you again Stephen. Uh the next question is regarding the disco discovery of heterogeneous treatment effects. How do you think about it? You gave us a little bit of a of of an insight into your thinking about efficiency and and constrained resources. In a in a broader perspective, more general perspective, do you think about discovering heterogeneous treatment effects primarily as a scientific necessity or rather luxury constrained by opportunity cost? >> Well, I think first of all, you have to appreciate that when we carry out clinical trials that we are usually very happy just to prove a treatment works at all.

And if you look at standard power calculations, then you'll actually find that that's what they are targeting. they're targeting um some sort of mean treatment effect. Now, that doesn't mean that in doing that that you believe that it has to be necessarily the same treatment effect for everybody, but that's your starting point. Now, if you want to start having a look for um individual effects, then basically you need replication at the level at which you're claiming an effect. Ideally, you'd like to repeatedly treat the same individual patient.

This is possible for some chronic diseases where you can use so-called N of one trials but otherwise what you're doing is you're doing replication at the level of the subgroup but then if you work as I have done in drug development another question arises does this make the cut if you look at the development of HIV treatment during the late 80s and 1990s was a rapid rapidly developing field in which yet another drug yet another drug was added and gradually survival al improved in the particular cohorts being treated.

Now what's our priority here? Is our priority using the resources for the next drug to come along so we have yet another treatment for maybe resistant cases or whatever or in general for for people who are further down the line or do we actually say well wait a minute maybe it works for men but uh you know gay men but does it actually work for hemophiliacs and does it work for women and you know and so on and so forth. And this usually turns out in my opinion not to be a wise way to spend your resources.

>> Thank you, Stephen. Richard, what are your thoughts here? >> Yeah, so one of the interesting things about discussing with Stephen is that I very rarely disagree with him and yet we come to opposite conclusions, which seems like a paradox, but um I I I think that the I think that the emphasis is different and the concerns are different. And so there's concern about um using resources and whether or not you can prove that something works versus whether or not it works in small subgroups. That is all true.

I I agree with that. Um what I am concerned about is that we are missing potentially and this is an empirical question. Maybe I'm wrong, but um Andy Gellman on his blog many years ago said we're all looking for the the biggest effect hiding in plain sight. Um, and I think about that a lot in this context because I'm wondering if there is a subgroup for whom a treatment effect is very very different, you know, to the point where power calculations are sort of beside the point. Stephen mentioned, you mentioned recently there was a study that had good results even though it was a bad study.

Um, and this can happen when the effect size is is big, right? Um, when it comes to subgroups though, if we have not measured the things that define the relevant subgroup, then there's just no way we can see it, you know, full stop. Um, and so my interest, which seems to be not not too common, and maybe it's because I'm missing something, but is that while we're running these studies, we need to be measuring the things that would allow one of these effects to be observed in a surprising way. So serendipity is is what they call it in the quality literature actually.

Um and in particular I think when we're doing that sort of exploration the notion of power and false positives and all the standard things that we think of when we think of confirmatory analysis. I think we need to hold it to a different standard in the sense that one of the reasons that these subgroup analyses are so expensive is that we want to power them the way that we would power a primary analysis. Um I don't know what the right answer is. Of course, you know, Steven's correct that if you if you change the machine in small ways and then, you know, you're injecting noise into the system, but as somebody who has worked on software for responsibly and soberly analyzing conditional average treatment effects, trying to find them, it's only as good as the stuff that you measure.

Um, and if you haven't measured the right stuff because you don't think it's relevant because you don't plan to report it in your paper because it's not in the FDA plan, you're never going to have the opportunity to to be surprised. I don't know what the right balance there but I think it's a slightly different direction which is I'm not talking about doing secondary studies actually I'm talking about measuring more and richer covariants almost right on time Richard a great sense of timing next we have Steven starting if a large average treatment effect may mask a small subgroup with extreme benefits or harms is it ethical to stop at the PTE well I mean it depends on the practical situation if you uh there are circumstances in which we choose to use the average treatment effect I mean I'm no expert on the field but I did once do a little bit of work on antifungal therapy and one of the issues there is that um there's certainly no point in giving any particular patient uh antifungal therapy if they don't have a fungal infection but usually what you'll find is that the symptoms of a fungal infection will uh emer emerge before you actually can get a laboratory confirmation that they have fungal infection.

Unfortunately, the symptoms are such that there could be other causes. But if the cause is not a fungal infection, there certainly is no point in giving them the antifungal treatment. Problem is that if you wait for the clinical uh confirmation, you might be two days further down the line and that two days will be too late for some patients who have got the fungal infection. So there you have no choice but to treat everybody using the average treatment effect. And I personally am somewhat skeptical as to whether all of this labeling of different treatment effects is really helping us understand this particular point.

People ask me are we interested in the conditional or the average treatment effect. The answer is the conditional effect obviously. But it turns out that in many circumstances, the best estimate of the conditional effect is the average treatment effect. And this is an issue of practicality and resources as to whether you can get beyond this average effect for treating individuals. And this is the heart of the problem for me. >> Thank you, Stephen. Richard, uh, what are your offs here? >> Yeah, I I think that there's well, there's sort of two directions for this.

Like one is whether or not there's a small group that is benefiting. um and then you know sort of hiding it. Um but I think that the other piece of it is is harms. I I mean you know one of I think that the treat with the at logic makes a hell of a lot of sense uh sort of unambiguously when there's no negative side effects. Um and it it becomes much more challenging to think that we're giving people drugs that have adverse effects um and it's not helping them. it's helping some other percentage of the population.

So that's what I think about when I think about the ethics. Um as Stephen's point about the AT being the best estimate of the condition average treatment effect um that's certainly true. Uh it will it will depend on features of the you know of the problem and um it just brings me back to my point about measuring more more attributes. um in particular if you do have a subgroup that has a large positive effect then it will not be the case that the average treatment effect is the best and we but we won't know if we don't look.

So in the case of the tmoxifen example again I would have liked to have been able to check for myself uh whether or not a particular smaller reference class had an effect uh that that popped. When you estimate these things you do want to do it right. And in fact, you know what I advocate for and in the software that my collaborators and I work on, we're effectively doing partial pooling. So at the end of the day, what we end up getting is we get a blend of the average treatment effect and the condition average treatment effect.

>> You know, I think I think part of this goes away when you if you're limited to unbiased estimators uh and you think about I'm going to estimate it as an AT or I'm going to estimate it as a conditional average effect, then the question is a little more potent. But in practice, that's not how we suggest doing it. And in in general, what we do is we suggest looking for these partially pulled effects. And then if one of them rises above the level of the AT, we we see that. That's the hope. Anyway, I have two questions.

Uh now in my head, and I'm not sure which one to pick. Uh but I I'll I'll pick the first one. We talked a little bit about both of you mentioned statistical power which is obviously important from this point of view. Do you think that we should that we should look at power as something that deters us from exploring subgroup effects or rather it would be better for us if this state of affairs motivates better data collection or different or ch cultural change sort of when it comes to data collection.

Stephen your turn is first according to the computer. >> Okay. So um I don't actually think the power is the best way of looking at things. I like to think of it essentially in terms of prediction error. uh and there I'm completely with Richard on this partial pooling although I didn't call it that was something that was proposed back in the first edition of my book is rev development um in the chapter on subgroups um and the idea would be that it might well be that if you had a disease in which uh there were very few female sufferers but you wanted to prescribe for females you might use some pulled average of the female and male results um in which you somewhat downweight the contribution of an individual male to the finding and you upweighted the contribution of the females and you can look at that in terms of minimizing the maximum mean square error if you have some idea what the maximum possible biases or you can use it in a basian term so I'm all in favor of that I like to pick up Richard on one particular point he said the reference class but I wouldn't think of it that way and I'm somewhat surprised by the story he gave of uh wanting to know how many uh women of a particular age group were particular had a particular receptor positivity status or whatever because typically the way a medical statistician would look at this is in terms of regression model and what Richard is then talking about is a model in which not only do you have age and not only do you have receptor status but you have the age by receptor status interaction and such effects are plausibly less important than the main effects that to which which are marginal to them and therefore it's not necessarily the case that the way in which one should look at this is in terms of constructing reference classes.

Um, and I I'm, as I say, I I think take think of it as a as a regression problem in which you're trying to minimize the the predictive error. I'm not sure I've answered your question now because I sort of went off on a tangent, but >> I I I think you answered it um in the spirit, not in the letter maybe, but in the spirit. Richard, uh, how do you think about it? >> Well, let me respond first to the to that example. So, first of all, I'm trying to I'm trying to speak the same the lingo of a bunch of different people and and so reference class I agree is not actually how I think of it.

Um I think it's a fairly accessible one for people that are used to Excel spread sheets and and cross tabs and stuff. Um in that particular example, what was notable was and the reason we wanted to know was that the main effects were in opposite directions. actually it looked like it helped uh for the age group she was in and it looked like it didn't help for the tumor size she was in. >> Um and so we were like well okay like that's that's kind of ambiguous. Anyway, as to the question of of like this prospective view of statistics and and guarding our resources and whether we should change the way we think about finding conditional average effects.

The thing I've been saying for several months now to people because this comes up a lot is that we forget that this is iterative. Uh the whole idea of a power analysis in particular is sort of focusing on one study and people's resources and if you think about the string of it, the long string of science, it's much harder to do the correct sequential decision theory to get the the correct power and stuff like that. Um but nonetheless, it is it is iterative. Um so one project that I worked on recently was reanalyzing some data from this large randomized trial called uh the vital study which was looking at the effects of vitamin D on heart disease at 25,000 participants over a 5-year period and they were looking at whether or not this supplement this cheap omega-3 supplement helped.

It was vitamin D and omega-3. It was a 2 by two treatment. Um anyway, uh when you analyze that data, it it looks as if there's heterogeneous effects. And that's kind of why I got involved with it. Um and what we found was that it looked like that there was a a a racial distinction between it worked for black patients, it didn't work for non-black patients is sort of the the point estimate there. Um when you go in and look at it though, what you find is that um there is confounding within the subgroup analysis by age.

In other words, the age of those two race racial groups were totally were different enough that you couldn't tell if it was the age that was the the problem or or not the problem, but uh so that was an interesting case where that was a well-designed study that was powered to find main effects. It looked on subsequent analysis that there was an interesting subgroup effect. When you go to find it, you realize that there's not enough power to say anything definitive about the subgroup because it's a different kind of thing.

It's a different kind of confounding. You need everything just needs to be different. and it needs to be designed different. I think that I learned something from doing that analysis, which is that if I want to find out more about this racial difference, I need to do a study in which I balance things differently or try to balance things or randomize differently. So that's I I guess what I'm trying to say is that even if you don't find the condition treatment effect you're looking for or you cannot prove it, you have still learned something which is that the next study that you go to do, you need to do it differently if you want to find that.

>> Thank you, Richard. uh are modern machine learning tools for HTT discovery or quantification, heterogeneous treatment effect quantification or conditional average treatment effect quantification rather extending or undermining traditional statistical rigor. Steven, >> so I have a a comment on a recent paper in the journal of Royal Cisco society series A uh talking about using historical control data uh and proposing that a causal lens is the right way to look at this and I think the paper's very nice in many ways in that what it does is it it outlines a number of assumptions that you make but then in the particular example that they use all the assumptions are violated and the particular authors don't notice that they've violated all the assumptions and it's very easy to show that in actual fact the example they have could not possibly satisfy these assumptions and it's probably doubtful as to whether any example could really satisfy them.

So there's a matter of judgment there. So I sometimes worry that the wrong emphasis is being placed on uh things and from a statistical point of view and to pick up something that Richard said one of the things I'm baffled by is why people are so obsessed by point estimates. Richard said we have to learn and we have to be able to synthesize studies to a certain extent and learn about new studies and so forth. And what I wonder is this how can you possibly combine different studies unless you know how precisely you have measured everything in so studies in these studies and why is so much of causal inference about identifiability and ultimately maybe estimability.

Um but in actual fact the issue of how you would validly estimate your uncertainty is never addressed. This particular requirement that uncertainty is central is in my view the traditional statistical way of looking at things and I think yes there something has been lost if we're no longer concentrating on it. >> Thank you Stephen. Uh Richard what's what's your answer to this? So the in the econometrics literature they they take great pains to get their confidence intervals for their causal identification stuff.

So I the world that I come from is sort of the opposite. They'll spend 50 pages of math to show that they can get an asmtoic confidence interval on on one of these things and but then not talk about the assumptions uh that are needed for the identification. So um you know the thing that I personally have focused on recently I got interested in this because I like the idea of deconfounding intellectually. I thought it was an interesting problem and all of these strategies instrumental variables and difference indifference and uh but I have kind of come around and and part of it is um I think the the lowhanging fruit which is not at all easy so it's not actually low hanging fruit but if we like if we can't find heterogeneous effects from our RCT data where things have actually been deconfounded uh or you know randomized appropriately then I think the possibility of finding them in observational data sets is really is is just that much harder.

I don't think that we should not use that data. Um, but the assumptions are heroic in in many cases. Um, and my favorite way to play around around with this stuff is simulation studies. If trying to simulate data that is non-trivial that looks like a nice clean difference in difference is so hard if if anybody wants to try it. Um, same thing with regression discontinuity designs, heterogeneous effects in that context. So anyway, there there's more exotic things that you can do, but I have personally picked this spot where I want to find heterogeneous effects from RCTs first.

>> Mhm. >> If that if that makes any I don't know if that answers you know your question was pretty broad, but that's sort of that's where my mind goes on. We can also uh look for for heterogeneous treatment effects in RCT data using machine learning, right? >> Well, no. So, I mean that's that's that's what I'm advocating for, right? Like I I I want to get my hands on our my dream scenario is an RCT that has all of these really detailed covariants that I can then run my machine on. >> I don't have to worry about confounding.

I don't have to worry about identification considerations uh primarily. uh and I can just look for those those partial pooling estimates that pop above the midline right above that pop above the AT. >> One point about that you don't have to worry about confounding of the coariantss with the treatment. >> Correct. >> If you're interested if you're interested in the individual effect of the coariantss you do not assign patients their coariantss and that is a much more difficult issue. So you might find in a particular clinical trial that the diff there's a big difference between men and women but actually it's just that all the women are old and all the men are young and the difference is one of age and it's not one of sex.

>> So I I want to I want to park it for now and I'm I I'm hope I hope that we'll uh be able to get back to this. Now I would like to ask you the last question that will come from me and then we'll move to the second part of the discussion where Richard you will ask you will be able to ask your questions to Stephen and Stephen you will be able to ask your questions to Richard. My last question in this uh in this part of the discussion is what makes observed heterogenity truly actionable? Uh let's start with Richard.

>> Well that that was perfect because that's the exact question that we were just talking about. to some extent. Um I I think that one thing that casual people in this field don't realize is that heterogeneous treatment effects are not themselves causal, right? Not only can they not be assigned like Steven said, but but even if we find a pattern there, that heterogeneous effect is probabilistic. It's purely associational. Um, and so you cannot make policies based on HTTE generally because all that you know is that one subgroup on average has a a higher or lower treatment effect.

If you're lucky enough, you can show that, right? You don't know if if that's because they are different in this way or that way or if it's just associated. So, for example, you might find a gender difference, but in fact, it's body weight. um that and which of the two that it is it would one make it actionable and two just change policy in general. I think it's a first things first kind of issue. You you have to locate it first and then you have to interpret it second. And this is why the iterative nature of it is so important because in the secondary study if you wanted to do randomization on one of those other attributes or study it some other way you uh you could.

So I think that there's a whole lot of work to do after uh you have isolated these things. Thank you, Richard. Stephen, >> yeah, I think one particular covariant which is probably actionable to some degree is severity of disease or risk if you like to call it that. And there certainly are some particular circumstances where plausibly uh a side effect or the probability of occurrence of a side effect is not easily predictable and so it's not actually associated with severity of disease. So basically anybody for all we know is at risk of this particular side effect.

But the benefit uh might be additive on the uh let's say on the hazard scale the log odds the log odds scale. Um and that means that the absolute benefit you will have because in the end it's always probability that matters not the odds the probability. The absolute benefit would depend on how ill you are. The the the sicker you are the greater your benefit. And so therefore maybe some sort of a policy can be developed in which one says well this particular treatment is not indicated unless your the severity of your disease is such and such.

So that I think that's one and this this particular thing has been discussed a lot in the literature at what point you know a baby aspirin uh is plausibly reduces the odds of you getting a stroke whatever your risk but some people are at a huge risk and some people are low risk and there are other things that might happen like uh you know side effects of aspirins so therefore it's not really indicated for everybody even though everybody would have their risk of stroke reduced the net effect is not beneficial and that's a particular way in terms of decision anal analysis that you might try and make things actionable otherwise it's it can be quite difficult.

Pharmacocinetics is certainly one of them but we choose not to do it. You know we could we could decide to measure clearance of particular pharmaceuticals before we decide on a final dose for a patient but it's just much easier to say well you know adults this is what you get full stop. >> Thank you Stephen. Can I just can I can I let me just pin onto that that those are those examples are exactly the sorts of things that I would love to have measured in my data sets severity clearance right right these other things things that are more proximal to the disease process would be beautiful >> yeah um clearance is plausibly an example of something where a marginal effect might be of interest because it might well be that in a particular clinical trial you can measure the clearance of all the patients but you know that in practice in actually um prescribing the medicine, you're not going to be able to measure the clearance of any of the patients to whom is prescribed, but you might have some idea as to what the distribution is in a target population.

>> So then you can actually think of the marginal effect in that particular target population, which essentially becomes the sort of conditional effect for any patient whose individual clearance you don't know. You simply know that they belong to a particular population. >> Thank you. Uh thank you, Stephen. Thank you Richard for for your followup. And now we will move to the second part of our conversation where you will be asking questions to each other. And according to our pseudo uh pseudo random number generator Richard you will be first.

One thing I noticed and I as I have been self-eing myself uh from Steven's textbooks and and others uh is that uh drug development and drug drug discovery and drug development are sort of two branches of the same palace and uh as an outsider when I first learned that it seemed like there was a it's a pretty big firewall. And um one of the things about that that I learned was that um on the pharmacometric side where they are studying the drugs initially um there's a lot of attention to things like clearance and um metabolism and body weight um and Steven has been on the record as mentioning that he thinks that there should be more randomization to dose and uh body weight based dosing um and it hasn't happened.

So my main question is can you just talk to us about why the field the industry has not latched on to body weight or body mass as an actionable um heterogeneous thing and secondly how that relates to this division between the two branches of the drug you know statistitians in statistitians in pharma pharma are on these two sides right and so what what other effects does that have >> I'll pick up on the division division first of all I worked a lot on crossover trials when I was in the in uh the drug development industry itself as opposed to merely a consultant for it.

Um and I became very rapidly convinced that what statistitians were doing was wrong. Um that in actual fact carryover was a phenomenon of the direct treatment effect but simply residually so and therefore we had to understand it in terms of PKPD modeling. and I abandoned what statisticians were doing. And since then, I've been a vocal critic of all of the very very complicated, mathematically beautiful, practically useless papers being produced by statistitians on optimal crossover design. These designs simply don't work.

Um, and the the pharmacatricians in my view had this already understood by the 1970s and statisticians still haven't understood it. So, the firewall is a great shame. As regards body weight, well, I think there are a number of possibilities. Um, it's baffling because uh the randomized to concentration idea was an idea of Carl Pex who was actually in a leading position at the FDA at one time in a position in which it could notionally at least um influence everybody in what they were doing and it didn't really catch on.

There are some practical difficulties with body weight. Um, one is having taught medical students I can tell you the following. If a dose has to be calculated by a medic, chances are you'll get a better dose, but the probability you'll get a catastrophic one is not negligible. Um, and we know from the history of radiation therapy that there are cases where children died of leukemia because they were given the wrong dose of radiation for treating ringorm for instance. So these things are it's not quite as simple as one thinks.

We need foolproof way of doing this. There is a practical one and that is to to really fine-tune dose. You need rational doses that are quite small or you need lots of different possible pill strengths or pill sizes. If you want to differentiate between 60 milligrams and 70 milligrams, you either have to have a 60 milligram and a 70 milligram or you have to also have a 10 milligram pill as well as a 60 milligram pill or there's something like this. So it's it's not it's not as easy as we assume always.

But nevertheless, it's clear that it's underutilized and it's baffling as to why farmer became so obsessed with the omics. You know, marketing people, wow, wonderful, you know, biionics, metabiomics or proteomics or, you know, you name it. But the bathroom scales were ignored. I I wanted to propose a science of ponderomics, which was weighing people. It's a it's a great example for people who study HD to have in their head that there are these plausible sources of hetrogenity that people can agree on that are not sophisticated and that are still not utilized and that should be a background that people have when they talk about this stuff.

>> So let's move on with the second question. Uh Stephen, now it will be your turn for a question to Richard. >> Okay. So, um, Richard, uh, I admire what Richard is doing because basically he can work in in two worlds. Um, the causal stuff and the and the statistical stuff. Unfortunately, I'm limited to the statistical stuff. I don't really understand the causal stuff at all. Um, and usually when I find myself trying to decode a causal paper, I end up eventually understanding it in terms of linear models.

And so then I see, okay, what they want is they want another equation. And this has given them another equation. And if you can believe that, this makes this particular thing estimable. It's the way I think about it. But in fact, there is a very peculiar feature of all of this and that is that a central feature of the way in which the design of experiments and its analysis was developed in terms of statistics did in fact rely on quite a sophisticated way of deciding how an analysis should proceed.

And this analysis this particular way seems to have had absolutely zero impact outside the narrow world of agricultural statisticians. >> Mhm. And this is based on John Nelder's uh theory of general balance. And the idea is that for a particular design, you define the block structure which is what would vary even if there was no treatment effect. >> And then you define the treatment structure which is what am I as an experimentter applying to this particular block structure and then you give the design matrix.

You then show how you would map your treatment structure onto your block structure and the analysis then follows automatically. And some of these analysis can be wonderfully complex and complicated and it follows automatically from this particular principle. And what I don't understand is why has this never been picked up by anybody else. So what I want to know is have you ever come across this? >> So so I am not deeply familiar with Nelder's work as you are. That is certainly true. the designbased causal inference stuff is out there and I suspect that I mean that's where I would go to I would look at Rosenbomb's work and um uh Pongden has a a a book out recently that has uh a bunch of design based stuff um the causal inference stats in general okay so I don't have a great answer to your question because I'm not as I'm not that familiar with it I suspect that there is stuff out there um okay in that in that world I I will take this opportunity to just say that the the history of causal inference as a as a youngish field um is complicated um and a big part of I have come at it as a basian statistician by training who then worked with economists and then got interested in biomedical problems.

uh you come to it from uh statist traditional statistics background who got into the drug development study and had colleagues that did agricultural experiments in the early days but there are people coming from econometrics there are people coming from computer science so I I think there's a lot of mutual unintelligibility and um there has been some people trying to do translational work I think that people get lost in the technicalities and that there are sometimes really basic things that are between the ways of thinking about it.

I'll give one concrete example for people and then I'll this is a non-answer but um the IV analysis when when you look at the instrumental variables that the economist developed and the specifically the local average treatment effect or the complier average treatment effect in that world. If you try to write that down with a causal diagram, uh you run into a a immediate problem which is that the the line that you need to connect the instrument to the treatment does not exist essentially for people that are not compliers which means that the graphs are not actually homogeneous.

It's like different individuals in the study have different graphs. There are ways you can modify this. you can elaborate things and so forth. But that's that's a nice accessible example of where somebody working in the diagram paradigm and somebody working in the potential outcomes paradigm would not just have a difference of notation. There would actually be a difference there that was substantive and I suspect that there's cases like that with the agricultural design based experiments and some of these other things.

Um so it's an interesting line of work for all of for those of you that are students and are interested. Um, and Stephen, maybe you and I can look at Nelder's work and u try to >> try to connect. >> I think there'd be a great PhD thesis for somebody to combine NDE's work with the uh structural causal modeling. >> Yeah. >> Recently uh with regards to marginalization is is when I was reading Nelder's work um and you and I had a brief exchange about that with respect to main effects and and partial effects.

So it's certainly still relevant, his ideas for sure. >> Great. Thank you, Richard. Um, now it's time for your second question. >> I think that most of us are animated by the challenges that we face on a daily basis. And one of the things I perceive in Steven's writing is that he has been beset by people wanting to spend resources on things that he thinks are dead ends. Uh, and um, and that that animates a lot of the tenor at least of the of the discussion. and he and I agree on OMIX um in terms of when I got my PhD, gene array analysis was the big deal and everybody was hoping to find subgroups based on gene expression and that data is just a mess.

It it's it's super noisy. It's super highdimensional. You need heroic assumptions. It doesn't work. Um so yeah, my question is Stephen, can you just tell us any stories about how you think Omix has diverted resources in concrete? I don't know if you're at liberty to do that, but I've been sort of wanting to know the backstory of that. >> Well, I mean, I I Yes, it's a good question. I think back in 1997, Sir Richard Sykes, who was at the time head of uh Clax, welcome and also I think the um the recctor or the principal or whatever the the phrase is of Imperial College London, so one of the great British universities.

He he said it will soon be possible to have smaller um cheaper clinical trials, but in actual fact, exactly the opposite happened. The trials did not get smaller and they got more expensive because all of a sudden people discovered more and more and more and more things to to measure and we started measuring these things in my opinion without having an idea of how we would use them. >> Mhm. in some cases in some cases very interesting things were discovered but it's very easy to just concentrate on the success cases and I do think that possibly um the idea that other people working in public health I think of people like Richard Pito large simple trials uh would have been a better way to go rather than um huge huge you know if you think of the array of data uh not just many rows but huge number of columns in the in the data more than you could ever possibly hope to decode without heroic assumptions.

So I think I think that has been a bit of a a problem. Yeah. >> Great. Uh Stephen and thank you. And that's uh that will be your last question in this part to Richard. >> Yeah. So um my question is about the thing that I see over and over again in looking at the causal literature is that a sampling paradigm seems to be being used by people. And as someone who's worked in experiments, I find this very strange. Uh I don't think we should think of in terms of uh probability sample or even quot sampling from target populations and that's not the way in which I I think about uh about experiments.

So am I wrong? Is this in fact well understood in the causal literature or is in fact this sort of this sampling paradigm deeply embedded in what everybody is doing? This is a great question and I've seen you mention this recently and I've thought a lot about it. In some sense the question is like what wh why why are other people thinking and doing what they're doing and I don't feel totally at liberty to answer that but um my own thoughts on the issue are that um one yes statistitians shoehorn too many things into this because this is the way they've been trained.

um the great undergraduate textbook statistics, Freriedman, Pasani and Perves, right? Starts out in chapter one with drawing things from a hat and the whole rest of the book is sort of built up on that and thinking about that. Um so I think that some of it is just inappropriate, you know, using what you're use using what you know. Um the substantive side of it is that I I I do think that there is value in thinking about prospectively your target population. um and the transportability problem of taking data from an RCT and moving to a population.

So at some point you you have to wrestle with that or you ought to wrestle with that. Um but I think it it's as you to your point it gets real muddy in the middle where where the primary analysis is sometimes done with reference to population based thinking or sampling based logic when in fact that's not appropriate. the textbook that I mentioned earlier um uh ding g peng d i nng uh this is a a great statistician at Berkeley his textbook is brand new I think it's called an introduction to causal inference first course to causal inference something like that I have it it's two feet away I was looking at it yesterday and he has a chapter that's called bridging the designbased and population-based perspectives for causal inference >> all right okay >> so so some some people are aware of it.

Um I think for this audience what I would like them to know um in answer to your question is one your point RCTs are convenient samples or you know they should not be thought of as representative of of hardly anything. Um and it's kind of a a a big question about how we should think about interpreting that result even if we find it. Um and this is the internal versus external validity question that you have written about and my approach to that is I I do like to think in terms of super populations.

Um but yeah but but it's hard um but anyway I guess that's the point like yes people more people should recognize that RCTs are not really there's not really a well- definfined population there and because of that they should think hard about their population and what it means to take that result and move it to their population. Mhm. >> Yeah. Yeah. No, I'm glad you asked that because that's it's an important point. >> Okay. >> Thank you, Richard. So this closes our uh structure part of our discussion and now we have five to seven minutes um free for a free range free range discussion and I wanted to kick it off with with one thought regarding the on one hand the machine learning perspective and on the other hand the identification perspective.

I think it's a very valid point that you raised before Stephen regarding the very strong focus on point estimates and I think this is this is a problem that we face not only in causal inference but in the broader practice of data science. I can talk about industry. So I will say in industry this is uh something that for whatever reason probably probably reasons related to uh what was easier for people to um to really process has been embedded in the in the data science culture. So I'm working with I've been lucky to work with some of the biggest companies in the world as a data science consultant and I see this pattern repeatedly that when you start talking about quantifying uncertainty and saying well if you only have a point estimate it it may be very misleading for your for your decision- making process.

But when we start talking about uncertainty uh many people will will see oh there's complexity arising there. So it will be more difficult for us uh to use this data the outcome of the of the modeling process to influence other people in the organization to make a decision to get us funding and so on and so on. So I think this problem is is really much broader. It's a cultural problem that is goes beyond only the uh this little subset of causal of causal inference. The second sub point that I wanted to make is that I think in the community we have a good awareness of the importance of uncertainty and we can see it in papers uh that on one hand look at uncertainty from the identification point of view.

So we have this entire it it's not even one it's not even one literature stream but for simplicity I will call it one literature stream of partial identification on the other hand we have all the works regarding sensitivity analysis so this is uncertainty that is grounded in in the concept of identification and on the other hand uh we have we have some works with bootstrap and u with bootstrapping methods for for estim estimation uh quantifying estimation uncertainty uh and also conformal prediction as applied to estimation uncertainty in causal inference problems.

So that's a note I wanted I wanted to make and by making this note I wanted to lead lead it uh lead us to to to a question that I wanted to ask to both of you as you said earlier there is a fragmentation in the community right so we have people coming from the econometrics background computer science background uh medicine drug develop and so on. And for each of the subcommunities, there is typically a dominating perspective through which they look at the world and the problems we are discussing today and other important problems.

When you think about your own background and your own journey, to what extent do you think your own history influences you? And uh to what extent do you think some of the problems and I'm asking this because it's a part of the mission of this podcast. Some of the problems that you might be looking at uh maybe they they have uh there are different perspectives out there that could be helpful uh for you as well to frame them in a way that would be useful in in solving them. >> Let's start with you Stephen.

>> Oh okay. Right. Well, I I certainly think within statistics I think different frameworks are useful and I often although I usually work as a frequentist, I often actually um try and ask myself what would a basian do here and I usually find the basian perspective very interesting uh and valuable as regards uh particular things that we would at first sight we would treat differently such as sequential analysis for example um which tends to be a sort of lipmus test between frequentists and and and basians.

So I think that's valuable and if by extension I I dare say the point of view of other people would also be valuable other disciplines non-statistical disciplines. Um although I do think that if you're a statistician it's very hard to imagine a world in which uncertainty and certainty are not at the central and I recommend anybody who disagrees to have a look at Cox and Hinckley's book on famous book on theoretical statistics in which the chapter on point estimation comes after the chapter on interval estimation >> and it starts by considering whether there's any justification for point estimation at all and they basically come up with the following well it could be that you have no prior knowledge.

You're never going to look at this problem again. You have to make a decision. In that case, a point estimate could be useful. Or it could be that in actual fact, the uncertainty is fairly regular, could be described by a normal distribution, and you're going to have a point estimate and a standard error. But basically, the conclusion is the point estimate on their own, no, nobody should ever be interested in that. So, I do think that's pretty fundamental, and I find it rather strange. Um so Richard mentioned Rosenbal's work on experimental design but the propensity score if it's the same Rosenbal is deficient from an experimental design point of view because it actually you have the same propensity score for a randomized block design and a completely randomized design and the point estimate will be the same for those two designs and yet one of the analyses is wrong and the point estimate doesn't point out that it's wrong.

Whereas if you think about uh the only reason I balanced by block was because block was predictive therefore block must be in the model then in that case it falls out immediately. So the standard statistical way of looking at things seems to give the right answer. So I'm somewhat skeptical. I'm going to fight my corner as hard as I can before I before I get down on my knees and say yes I've seen the light. I'm going to I'm going to use dags all the time. >> Richard on the on the point estimate question.

Stephen, I presume you've heard the quote. I don't know who it is but that um you can't inject an interval. >> Ah right. >> Uh which is sort of the the same same conclusion. So my background I I was a philosophy of science undergraduate major and then I got an operations research master's degree uh and then I went to statistics and it was basian statistics and then I played an econometrician in my first job and while I was there and throughout all of this I was sort of doing computer algorithm sort of stuff.

I feel like I've got a I've got this is more feet than I have but I've got a foot in all of these fields. um master of none maybe. But what I like to do with these is I like to figure out what the motivation was behind the perspective. That way I can ask myself, is that motivation valid in my case? So for example, if you're studying observational data, power calculations are just irrelevant like like you still need uncertainty estimations, but you're not doing anything prospectively. Um, you know, likewise, if um if you're doing an RCT and you care about the average treatment effect, you might not care about measuring all of these extra coariantss.

Maybe you will. Maybe they're uh prognostically important and they can reduce the variance. But I like I guess the point is I try not to think about it in terms of methods and teams so much as problems and you know hurdles because generally speaking people working in these areas are all very smart and while there is some inertia based on who your adviser was a lot of times the techniques and tools that are dominant have to do with what the thing is. So econometricians one that I know well is a great example for many years they were not doing RCTs.

There was an RCT revolution in econometrics in the last decade or two supposedly but uh prior to that they had observational data and you can see why identification was so important then because the point was even if we collected as much data as we possibly could what could we learn you know that that bias doesn't go away >> right and so I think that explains the the identification focus at least in that community um the uncertainty bit is a is a harder one the philosophical debates I think are are more of a distraction.

Uh getting people to acknowledge it is the is the bigger thing in some form. So you and I have talked about conformal inference versus basian versus frequentist. Um I think that in an applied context those are all valid. Often they will agree to first order. Uh what you don't want to do is pre pretend that it doesn't exist. >> Um you know that's no good. So as far as like is there anything that I could learn more about specifically and I'm trying to learn more about I mean recently I discovered the pharmacometrics literature and I feel very at home there.

So it turns out that the way that I think about things is sort of natural and copacetic to the way that it's developed there. Uh which was a pleasant surprise. And so um I'm trying to learn I I know that they are big on random effects models that allow people to deviate but then parametric models. So parametric models augmented with random effects. And so I currently am trying to learn whether or not semiparametric methods would be any benefit there or whether or not there's something about those PK curves that make them essentially fundamentally parametric.

So that's what I'm doing right now. Mhm. >> Okay, we have 11 minutes left. We have four questions in the Q Q&A uh chat. Okay, let's start with the first question from Bruno Spelton. I hope I pronounce it um correctly, Bruno. If not, my apologies. thinking about study designs and considering that the final goal is to see if people respond differently to a given intervention, eg keto diet and LDLC response. Should we run more common RCTs, eg parallel and crossovers before going for replicates NF1 trials if we don't know anything about the outcome we want to measure or do we just assume that humans will have some heterogenity to some degree although not necessarily MCID? I think Stephen that will be probably you will be probably the best uh person to answer that.

>> Um yeah it's an interesting question. And I have suggested that it might be a good idea to try and discover if we can in certain fields whether um particular there's scope for personalized medicine. Can can we actually establish whether there is scope? One one way possibly is by comparing variances in treatment arms. Um if there's heterogeneity plausibly it will show up in some sort of v difference in variance in in treatment arms. Crossover studies are a possibility. Um but there's an analogy here I think to twin studies um carried out in genetics and I think it's one of the things about twin studies is you can tell what proportion of variation in theory is heritable but you've no idea where the heritability lies.

So it doesn't actually they don't actually tell you anything about uh about where the heritability is lies. And for that reason for example I believe I'm no geneticist that we know that there are many many factors affecting height as genetic uh >> variable but we really haven't identified anything like a half of them. There are still huge areas of variation which are missing but yes it will be a start. So if I interpret that question in that particular way, maybe um by doing smarter parallel group trials, we could begin to uh establish whether it's then worth doing more in the way of targeted subgroup trials or end of one trials where that's possible or whatever.

>> Thank you Stephen. Richard the second question uh I think will be will be for you question for both speakers in particular for Richard the co-author one of the co-authors of BCF. How much do you uh how much do you think call uh can basian methods help in identification of heterogeneous treatment effects? Basian methods I think have the advantage in general that it's an easy framework for putting prior information in where it's available. Um, one way that we do this is via weak but informative prior on magnitudes of treatment effects.

I don't think that my treatment effect is going to be, you know, I don't think that it's going to raise somebody's heart rate to 300. A lot of times responses are bounded, right? And and so we can put that sort of information in and sometimes that really helps in being able to tease out these differences. basing is not the only way that you can do these sorts of things, but it's a convenient way and it's one that I'm familiar with. Um the one of the big practical advantages that I like about bay that is related to this is that when you fit a basian model that you worked hard to make plausible and honest um you can ask lots of different questions without invalidating other questions.

So one of the things about a frequentist analysis is that if you ask a different question, you do a different analysis. And um sometimes there's famous examples where you're doing hypothesis testing. And if you phrase it as a difference between two groups versus a difference from a baseline versus um you can get different conclusions. One will be statistically significant, one will not be, even though they're kind of answering the same question. In a basin analysis, for all of its potential flaws, it has the virtue that you can ask a question about the ratio of two parameters or the difference of those two parameters or a subgroup and they're all coherent within each other.

And so I think for subgroup binding, this is particularly nice because you want to keep that uncertainty front and center, but you don't want the extent of that uncertainty to change depending on whether or not you're looking at a ratio or a difference. And B sort of handles that nicely. So sometimes people will talk about bays as being coherent. And what they mean is this very high theory reflection principle, but there's a a more pedestrian version of coherence, which is just that you fit a basin model once and then you've got the posterior and you're free to ask as many questions as you want and there's no multiple comparisons problem because it's all based on the posterior.

All the uncertainty is there. Um, so we see this a lot with Kate with subgroup effects. you get a a conditional average treatment effect and there's a a posterior around that there's uncertainty around it and then we can aggregate up partially so right you could be looking at the maximally disagregated conditional average treatment effect but then you can say what if I want to aggregate that up to a to a coarser subgroup you can do that and oftentimes the uncertainty there is going to be lower because you have more units inside that analysis and those two things will match basically.

So um that's kind of a a secret weapon that we have by by using basin and it's makes it particularly nice for exploratory subgroup analysis. >> The next question comes from M Matt Gerishov and Matt asks is the last comment basically that we should care about PAT estimates rather than SAT or rather that PAT isn't really possible. I assume that what uh Matt meant here by PAT and SAT is population versus samp sample sample average treatment effects. I think you have to have some puditive relationship between the two other otherwise why are we doing the RCT you know one one easy assumption is that effects are basically homogeneous that makes it easy because then you've learned it in the in the trial and then but I I think to the extent that we think that's not true we we're obliged to think hard about it and that's the uh generalizability question >> I think uh I think I would agree with that but what I would uh warn about is assuming that therefore sampling ideas are relevant.

They're usually not. >> I'll give you the example of bioequivalence studies. Bioquivalence studies are carried out in healthy young male volunteers. The results will be applied to elderly frail women. So there's a complete mismatch. trial sample has nothing to do with the population in terms of representativeness except one thing and that is on the scale measured the log area under the concentration curve this difference will plausibly be the same in the male volunteers as in the healthy the the uh sick elderly female patients.

So it's a it's a theory which enables us to trans transfer this from one particular sample population to the target population which is completely different. It's not that we regard the sample population as being representative drawn from this particular population. They are deliberately different. This is also the reason why we carry out toxicology um toxicology experiments in rats. What nobody asks is what's the target population? A stupid question. What instead they do is they look for their tables of interspecies scaling.

That's what you want. Nobody asks are these representative rats? This is a completely stupid question. So what I would say is yes, we should be thinking about the target populations, but we should thinking about them as grown-ups. We should take our cue from the physical sciences which run highly abstract specialist experiments not from sample surveying. That's not the analogy we should be using. >> I I think some of the I mean some of the the science like using science to do the transfer, right? >> Yeah.

>> Um is I think the right idea. I I would just add that you know where that science comes from is on the in the pharmacometrics wing uh and things that things that modulate uh and moderate the the the science are relevant and I would like to know about those. So I would like to see those PK studies have all those columns that I want which is not the OMIX columns but it's the it's the more pedestrian severity of disease and and stuff like that. >> I'm sorry like I have the opportunity I'm gonna I have my talking points.

Sorry everybody. >> No, no, I think that's uh that's a great point. Paul, did you want to add something? >> No, I think uh I uh switched on the video because we're slowly running out of time. I see there are many more questions uh in the Q&A and also a couple on the chat um that probably we don't have time to address. Uh but thanks to all of you. Uh thanks to Alex for organizing this. Um great provocative questions and uh great discussion among Richard and Steven. I think this was very useful for for our audience uh and to see also the different perspectives.

So um yeah um once again thanks to everyone Stephen Richard thank you so much uh for for a wonderful conversation. I think we could speak for another one one and a half hour easily on on these topics. Paul, thank you for organizing and the entire team. Joe, thank you so much. It was a pleasure as always. >> Thank you so much. Especially Yeah. Thanks. Thanks, Richard. Bye and >> thank you. Bye. Bye.