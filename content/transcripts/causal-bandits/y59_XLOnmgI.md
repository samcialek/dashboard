we today calling it ca wi INF but it's actually a collection of stuff from multiple fields and I feel that reinforcement learning is one of those fields can we do cross validation with C inference model can we do feature selection and this meet grind their way where we try a bunch of stuff and then we take this mod that we like better and I feel that at the heart of this problem is hey caal Bandits welcome to the caal bandits podcast the best podcast on causality and machine learning on the internet today we're traveling to Sal Paulo to meet our guest he thinks of himself as a weird Brazilian because he's not interested in soccer or Carnival he grew up in a rural area and loves the sense of community that he remembers from his childhood he studied politics but switched to economics where he fell in love with math an author and a staff data scientist at one of Brazil's largest banks ladies and Gentlemen please welcome Mr matheus furi let me pass it to your host Alex molak ladies and Gentlemen please welcome Matos fakri thanks for having me Alex so are you enjoying some Paulo so far I enjoy it pretty much uh especially now when the weather become a little became a little bit more Sunny yeah it's it's beautiful and I love the parks and um there's a lot of nature here very beautiful nature glad to hear yeah and the city is huge definitely I learned yesterday from you that it's 20 million people living here yeah that's about right it's like 50 50% of of of the entire population of Poland for instance so that's the scale Matthews uh what was the biggest challenge for you when you were writing your book I think let me pause a little bit here but probably working around all the family duties Plus writing so I was lucky in some sense that most of the content of the book I have already written in some shape or forms of because of the the the open source the online book and I thought to myself okay this is going to be a piece of cakes I'll be able to do it in no time I have a kid coming along the way but it's going to be easy it was not that easy I didn't take that into account it would be so much harder like so hard so taking care of the kid the wife that just had a kid plus the book doing all of that was insanely difficult I'm very blessed because my company gave me four month of paternity leave on top of that I added an exra month of vacation but still was a lot of work so like working around the schedule and making sure uh I was not behind what I proposed to do and making sure that the quality was right for me and I was happy with the material while taking care of the family definitely the hardest part by far it's not the context not the complexity of the content yes it is a fascinating and complicated topic but it's the sheer work of it and I think that's the hardest part when was the first time when you when you realized the value of coal Coal inference or coal modeling in general it was back when I was mostly working with predictive models and uh well a traditional predictive model essentially but have a bunch of data and you predict some outcome and it outputs you number that's interesting in some cases but it's not the end goal like you have a number you have to do afterwards like the compan is usually not that interesting a number they want to take that number and transform it into a decision and that part is a little bit fuzzy if you don't take into account stuff like CAO inference for instance at some point uh I was working with uh dab collection and essentially we had to figure out how to make people who are late PID their deps to the bank and we had a very good predictive model that told us what's the probability of someone paying us okay so we have a very good predictive model it tell us okay this is the probability that this person will pay us and that Motel was very good but how do we use it like should we target the people that are very likely to pay us or should we target people that are very unlikely to pay us like what end of the the spectrum of the model do we target how do we transform that probability from the model into a decision an optimization for the company that's not entirely obvious and then a predictive machine learning approach doesn't answer that fully so then I started to work with Okay I need to think about I need to specific specifically think about the actions or or the strategy that I want to employ on top of the model and how do I evaluate and how do I find the best actions and the best uh decision to make in that framework so it was in I think in this approach like we had predictive model we were trying to force them into a decision making process and I didn't like to force it I went to I wanted some sort of formalized framework whether I could take the predictive model because it was very interesting but I didn't want to force it into a decision-making process and CA inference came into a very natural way and a very natural formalization to evaluate and decide and optimize uh decisions and actions that you can take within a company that's a very specific specific context but it's morally afterwards I realize that it's much more General than just depth collection it actually happens a lot of places in Industry when you when you speak to stakeholders um and and you talk about causality with them you might sometimes hear a question from from people uh why should I invest in caal in in Cal models if we already have machine learning in our company uh what would be your answer to to this question question yeah great question again I think it ties back to to what I was mentioning like it's hard it's hard for you in in most situations that business want I can find exceptions but in most situations they don't care about purely prediction like prediction gives you a number but they don't care too much about it they care about making a decision that is bring more customers increases conversion they increases turn increases profitability Cuts costs so they usually want to do some sort of optimization where the predictive model is just a tiny piece of the system and most of the convincing that I had to do or that I usually have to do is usually in the sense of you don't even have to get that technical in terms of mentioning CAO inference and being okay so you want CAO inference because cause is nice it's more like okay you have a model you have a predictive model it makes prediction and you want to make decisions on top of those predictions like how what's the best way to do it here it's it's a framework that you can make this decision and you can see okay is this the best decision for this type of customer or is this best decision for that type of customer so I usually frame the problem a sort of a personalization this is a very easy way a very easy sell for people because managers and product managers project managers they're usually very interesting in personalization they want to treat users differently uh treat user how that specific user wants to be treated and personalization is the let's say the selling point but we can translate personalization into a CA of inference approach very naturally it has a very ugly name we call it treatment effect heterogenity but it is personalization in the end of the day so it's just just sort of translating the technicality into a way that sells more but the idea behind is very easy to sell like again companies don't want predictions they want to make better decisions sometimes a machine learning model predictive model is good enough for this I can think of examples like fraud if you just predict that a transaction is going to be fraudulent that's good enough but most cases you want to actually execute decision like you want to figure out a price you want to figure out who to Target an ad for you want to figure out who to call stuff like that when you have to deal with decisions then cause inference become very natural uh formalization of the decision- making process so I don't I I feel that if you understand this then it becomes much easier to sell it what was your path towards towards understanding this and and learning about heterogeneous treatment effects yeah I learned the hard way so again I I come from economics and I like to think that I knew I understood inference from the beginning and what happened is that I when I started working I wasn't working mostly as an econometrician I was mostly working with traditional data science predictive modeling and there were a bunch of problems where we were tackling them with predictive modeling that didn't solve the problem I gave the example of the ADT collection one so we had many problems okay we have a model that's getting better over times in terms of performance of predictive metric or Au or cross entropy whatever predictive metrix you like like are squared so we had very good models in terms of those predictive metrics but the decision itself were not bringing more money or not a profitable or we're not improving so we took those models we put them in production and we saw okay where's the return like we're not seeing it essentially we figure out that we we were misclassifying the problem we treating as it was a simply predictive problem where it was it was not it was again a decisionmaking process that you can frame it as a cause of inference process but mostly was the the hard part like we tried something it didn't work we had to learn something that did and that's something else that did was cause it was actually very like for me I think it was very easy at first to see that those problems they reminded me of stuff that I saw in econometrics course and I said okay oh I know what's going on here I know why this is not working this looks like a cause inference problem we're treating it wrong but it took me around two years to actually learn how to phrase this problem so that the people from data science can understand it so there's also this challenge of CAO inference is relatively new to data science so you have to talk the same language for them so you have to and they understand exactly okay how can I make this econometric stuff sound like a machine learning or data scientist stuff so that we can communicate so a bunch of the work was actually figuring out the communication and how can we speak in the same language so that we can frame the problem in a way that we can solve it but also at the same time both of Economics econometric like myself and traditional data science could could understand it you mentioned this this language barrier um with in causality itself uh we we also experience this so we have people with background econometrics like like yourself uh some people in back with background in epidemiology who For Whom the natural language to think about causality is is the language of potential outcomes and we have people who maybe came to causality through uh machine learning or through graphical models and they speak the the graphical language uh the language of of Judea Pearl broadly speaking uh how did you how did you deal uh with the differences in framing problems in in those two languages I didn't actually so the thing is uh usually what I gravitate towards more is the potential outcome that we learn from econometrics uh again most of the translation that I did was from causal inference towards people that are not familiar with caal inference not necessarily between different flavors of languages of inference that's a very diff difficult thing to do again I feel like I don't think I'm qualified enough to do that because I frankly I need to study much more about the judal approach and to understand exactly like okay what's he saying I use a lot of graphical models but usually and I think that's something that we defer a lot like I read your book you usually start from the graph and then you move uh along with it the way I tend to work with the graphs is like okay I have this problem this problem I want to formalize it and then I draw a graph and I use it to explain to people that are going to work on the project usually technical people like graphs are not that simple that you can just show any stakeholder but we use the graph and say okay so this is the graph this is what we have to be careful about so this is the things we probably don't want to control for those are the things that we want to control for but it's only a formalization for us to have a clear picture of what are the assumptions that we're making what we want to estimate what's the question that we're asking and what we should be careful about and from them on we don't necessarily use the graph for let's say estimation or identification then we move to the data and we model everything only having the graph let's say as a background pictures in our head that's mostly of my bias from from coming to econom from coming from uh the econometric but uh still I I understand the value of the graphs I just didn't manage to use it more on a on a more broad uh perspective like apply it throughout the entire project I feel it's more like the beginning when you're trying to understand the problem use the graph but then you leave it behind when you leave it behind you leave it behind explicitly but implicitly you already made decisions based on the graph which variables you will includ in the model is that correct yeah like when I say leave it behind like it's always there in the back of your head but we don't uh necessarily use it for like estimation we forget about it and say okay we have to estimate this relationship while holding this this constant or we have to perform a test or an intervention here and here and here which will allow us to measure this but we rarely go back to the graph and see okay so this is valid this is not valid this doesn't this doesn't hold so it's it's rarely the case where the graph keeps we keep the graph throughout the entire project like we don't go back and do let's say I think uh what's it called it the final stage where you question the graph and you ah you mean like uh reputation tests yes yes we don't usually don't do that yeah I mean if you yeah so reputation tests are are not that strong of of tests I would say but it seems that the entire modeling processes uh is very similar to what people from with with graphical background do when you say we don't go back to the graph uh I initially thought that you mean that you do not go to evaluate the the correctness of um of the graphical assumptions which I think some people are are doing and in Industry depends on the use case right but in especially in open-ended systems complex systems people do this this often um and produce like a human in the loop process what are the main challenges that you're facing today uh in your work okay so just to give a little bit of context I I think that's something we also chatted before it's like for what I do usually confounding is not an issue uh just explaining what this means is that when you're doing CAO inference especially in Academia you're very worried worried about confounding biases so let's say you want to understand if wine is good for you because people that drink wine they tend to live long longer you have to adjust for other like confounding are the other factors that vary together with wine conceptions for instance people from Europe they tend to drink more wine but they're also from a richer country where uh they have better health care so how do you know if it's the wine or is actually the healthcare you have to isolate those factors so this is confounding for most of what I work I don't have to worry too much about those stuff because I get to intervene and I feel this is actually what you see most most like we might disagree on this one but this is from my experience what we see in most cases in companies because usually companies are worried about okay so I have this stuff this action this thing that I can do it might be sending an email might be sending a call maybe changing the price of something and that's something that the company can control if they can control usually they can randomize with some some exceptions of course like marketing is hard to Rand but usually something that they can change so let's say it's price they want to understand the impact of price on sale they can set price here they can set price there they could do some sort of randomization or natural experiment and then they can understand the impact of price so I work mostly in this situation where confound is not an issue because I get to intervene on the variables that I care about which doesn't mean that there are challenges like that's only a small part what I'm mostly concerned about if confound is not an issue is what I what we we said before is uh effect heterogeneity so we have something that we want to do say call a customer because he late we want to figure out who should we we call first like should we call this type of customer should we call that type of customer and for that I want to understand who will my action in this case calling someone will have a higher incremental impact in terms of increasing the probability of that customer paying us so this is one example other examples that we have is for instance we have we have we want to do cross sell so someone has a credit card maybe we want to cross sell that person to a prime credit card so who do we cross sell to who is more likely to benefit the most with this cross sell like who's the incrementality of the Cross sell we who cross selling it to will increase the most the probability of that person converting to to the prime credit card so essentially it's a world where we're not very interested in finding if an action is good or bad on average is more a world where we want to know okay is this action or we call it treatment in cause of inference is better for this customer versus that customer or maybe better for this one and sort of like again uh it's sort of a personalization no it's not sorry it's exactly personalization so that's the world uh ion from and in this world the hardest thing for me I guess I would say it's first when thing things are nonlinear so for instance if you want to understand the impact of credit lines so credit lines on the probability of someone defaulting on their loan this is not linear like if you increase credit line the probability of someone default increases because it's harder to like it's harder to control yourself if you have too much credit so it increases but at some point it saturates and becomes flat so this is sort of nonlinearity that if you increase above a certain point then the Y variable doesn't change that much so this is one very very tough problem to crack and the other one is uh evaluation metrics so there are a bunch of treatment effect hydrogenating models and we want to figure out which one is the best so we can fit a bunch of them and we want to pick one then essentially do model selection and it's very hard to come up with a metric to evaluate a clausal model because clausal quantities are very hard to put your finger on like they're usually non observables well they're always non observables I will say so it's very hard to estimate them and to put a predictive a performance metric which is very different from traditional machine learning like you do cross validation C A if it's better okay this model let's pick this one and apply it to production so what we are trying to do now is take this framework from machine learning we call it the meet grinder framework essentially try a bunch of things see the performance metric whichever has the best performance metric deployed to production we're trying to do something like that for caal inference like can we do cross validation with caal inference model uh how do we do it easily can we do feature selection which is also incredibly hard challenge to figure out what to put in a c model and what to take it out so can we do feature selection in this meat grinder way where we simply try a bunch of stuff see if the metric goes up and then we take this mod that we like better and I feel that at the heart of this problem is coming up with a good CAO inference a good evaluation metric for the coal model I think this is the core the hardest part what are the lessons you learned uh thinking and working on evaluating models in in your company I think the first lesson that we learn maybe the hard way is that it's very hard to trust uh any any evaluation metric if you don't have randomized data so if you don't have any sort of experiment or if you have confounding you'll always have that feeling like the neing feeling okay is the metric this good or is just the bias that is messing up my metric so first I I guess the the thing that I learned most is okay first I need to have a data set that I trust and I know it's completely free from any biases and I trust this data set with my heart and then I can use this as a validation set I can use this set as okay I'll perform I'll evaluate the Matrix and the model compute the C Matrix in this because I know there is no confounding odds forever in this I don't even need to use this clean and pristine data set to train my model like to train I can throw a bunch of of garbage in it but as long as I have a data set where everything is randomized I can make the traditional and confound this assumption and I use this data set to validate and I trust this data set then the problem becomes much easier because I have something that I can trust that I can ground myself and say okay if the model doesn't perform good on the data set where treatments are random it probably will not perform good in production so this is the I think the the more important lessons like as much as possible simplify the decision making process in this case randomize the thing so that the evaluation becomes much easier and does not and do not does not complicate the analysis so in a sense the idea is to have simple decision processes so that you can also have simple analysis I remember that it's some point you shared on LinkedIn that you also worked with reinforcement learning um are there any experiences uh from from that period that you are able to translate to working with causal models personally I think reinforcement learning is actually a flavor of caal inference or they other other way around depends where you're coming from which is actually very interesting causal inference like we're today calling it caal inference but it's actually a collection of stuff from multiple fields and I feel that reinforcement learning is one of those fields because if you look at reinforcement learning strictly from like okay let's simplify reinforcement learning and see what it really is is something like this you have uh information about the environment you have a thing a metric that you care about so something that you want to optimize and then you have actions that uh you want to perform in order to maximize the metric that you care about well if you look at this is very similar to call inference like we just called it them in CA inference we call the action the treatment but we are also interested in finding out what's the treatment that maximizes the outcome condition on a set of coant which in reinforcement learning we called uh environment it's the same thing with different models which is very interesting because it allows us to use a bunch of stuff from reinforcement learning particularly what I like a lot is uh offline policy evaluation where you can look at stuff you did in the past and figure out okay if I do another thing a different policy so if I take other decisions how would that's how would those decisions have performed if I've done that on the data that I have so you can use this technique to evaluate a policy even if that policy has never been to production which is very interesting and it comes from the reinforcement learning approach and I also want to be very careful when I say Okay C INF is reinforcement learning because when people think about reinforcement learning they think about a model that it's self updating like the model see the environments performs action and retrains see the environments perform actions and retrains and that's done automatically I would say that you don't actually need to do that automatically you actually have got to have you might want to have a human in the loop that actually presses the button to refit the model constantly but the if the model always training on treatments that it did in the past is essentially the same thing but with a human performing every iteration of the reinforcement learning updates so I think this is what I've came to realize when I I was working with again the collection strategy we had a bunch of actions we had a bunch of customers and customer had different uh context let's say some customers are have very late delinquencies like they're two months late some customer maybe they just forgot to pay their bill like they're three days late we probably don't want to treat those two customers the same and this fits very nicely into a reinforcement learning framework and it also fits very nicely with uh AO inference framework so I I think it's interesting to look at the same problem from different angles because it gives you different insights into how to tackle it but fundamentally I think it's just different names that we give for the same formalization of a pro of a decision- making problem now on the other hand we know that reinforcement learning agents might be susceptible to confounding as well which means that they might learn World models that are associational in it in their nature uh which might lead to to suboptimal decisions it's not that they will learn if you're not careful which is why I advocate for the human their retraining the model because the human can use techniques to actually debias the data set so if you throw for instance if you naively throw a a model like this in the wild it will learns on the things that it performed in the P right so for instance if let's keep with a collections example so if for instance this by chance this machine learning agent it try to call only people that have very low delinquencies so that they're only like three days late or five days late okay those people are the ones that will probably pay like they're not very maybe they just forgot uh they didn't uh were not paying attention to the billing cycle so they just forgot to pay so they'll probably pay if you look at this data naively what will look like is that calling has a very high impact on the probility of payment because you only called people that pay and you did not call people that didn't pay but this isn't isn't CA it's just like correlational and this will certainly happen if you simply fit the model on the decisions that it made on the P but there are very a m of techniques that you can use to alleviate that for instance if you take the actions that the model did and you make them probabilistic instead of deterministic so the model can decides to do this but it has a probability of not deciding that and actually doing something else suddenly you have a non-deterministic policy so you have a probability Association to Associated to each action that you can take and then you can use propensity Square to correct for all those sorts of biases and again that's why I advocate for the human actually looking at that and say okay this is how I'm going to refit the model instead of just letting this model lose in the while because it will certainly learn correlation instead of causation but again we know that there are techniques to to correct for that b and to make to use that biased data and in a in a way that it satisfy the causal assumptions like in confound us so that we can learn and use the data to update the model let's say or refit the model so that it learns from the biases decision biases decisions that it made in the past coal Ops is is a new area that is interesting to more and more people and organizations that are interested in um using causal inference in their operations what would be your advice to people who are interested in deploying causal model at scale I tend to use uh I think everything that we use for traditional ml Ops can be used for Callo models at least if you're thinking from the perspective of uh Callo treatment effect hydrogen so what you want in those cases is that you essentially want to predict for each person what's the impact that a treatment will have so it's nonetheless a predictive thing a predictive model although it predicts some something that you cannot observe because like you can never see for anyone what's the treatment effects or what how much that people will benefit from having this this treatment applied to that person so it's a odd treat predictive model but from the production aspect is nonetheless a predictive model uh so I would say like most of the recommendations that I would give from traditional mlops translate to CAO effect models so like use standard libraries SL PM stuff that is easy to deploy try to avoid as much as possible like models in Python I know that because the C inference uh Community is relatively new a lot of the models are not implemented in efficiently in let's say C++ it's usually pure python while if you take machine learning models it's usually C++ with a python wrapper around it so for instance in our case although I love the work of Susan a and CAO trees we've never managed to deploy any coo trees in production although I find it fascinating simply because it's too slow like uh it's usually pure python is not implemented efficiently so we cannot use it so most of the recommendation is try to treat it as as much as possible as a standard machine learning model and should be fine I perform this experiment in my book uh for to assess the computation requirements for different models and I think uh compared to sarner uh coal forest was 39 times slower yeah yeah and and that's that's the reason like if you it adds good and bad uh to it like if you want to understand how a cal model Works caal Forest works I don't know I think you I'm assuming you used ECHL eonl yeah yeah so if you want if you use eonl you can just look look at the cin python Cod so you can understand what the tree is doing which is a good thing like if from the learning perspective it's very interesting but for the production perspective it's just uh not feasible to deploy something like that at least not in this a large scale you mentioned before that confounding is not that much of an issue for you because you are able to randomize your treatments and use randomized data either for training or um evaluation both um but confounding is not all like backd door paths where you have a common cause is not the only graphical structure that can lead to to to causal bias have you experienced uh challenges with other types of confounding if we talk about confounding broadly confounding are bias in the yeah so I say about confounding as any back any path that any caal path that is opened in the model definitely I think uh selection what we kind of like I I'd like to make the distinction between like confounding where it's usually uh UNH common causes and I like to distinguish this from selection bias which is if you condition on something that it should not like so a collider or there something uh in the path between treatment and outcome or something after the treatment something like that so definitely uh those are the less intuitive uh things to work on and I confess I still I still think about them on a weekly basis because those are very complicated problem I've posted about this a lot recently so one issue is the conversion issue so I'll try to describe it very briefly so let's say that you want to understand the impact of interest rates on how much a people the size of a loan of a person so you would expect that if you put interest rates down then person would borrow more and if you increase interest rates people would borrow less right let's say a very hypothetical experiment that you want to understand the impact of interest rates on amount lent it's from the homo economic point of yeah yeah yeah yeah you Define interest rates and you have the interest rate treatment and you have your outcome and you want to find that effect but turns out that most of your customers they don't want a loan so they don't convert so regardless like of the interest rates the loan amount that they have is zero like zero so if you plot this distribution if you plot the outcome the distributions of outcome you're going to see a huge Spike on the zero and some loans amount above zero because the vast majority of people of your customers don't get a loan even though they have seen the price seing the interest interest rate so if you try to naively estimate the impact of interest rates on loan amount the effect will be very tiny very diluted by all those zeros so what people usually do when they face the situation this is a Lo loan example but it happens if you want to understand like any sort of pricing where you want to understand the impact of price on sold amount or purchase volume so what happened in this case is okay let's filter out the zeros so let's only look at people that actually took a loan and then see the impact of price on people that took a loan on loan am out but the moment you do that you open a spirous path because you you condition on people that converted and convert conversion is in between the treatment price and the outcome purchase volume in or loan amount in this situation so so we basically lose the benefits of randomization it this exactly exactly so and this is very counterintuitive like on some aspects of the problem if you look at it it's not caal but it's fine because as long as you estimate the effect of prices on conversion so you have the effect of price on conversion and you have the effect of price on loan amount given conversion which is not a CA of quantity because of the buys but if you multi them together then the biases disappears and this is very counterintuitive as with many things related to to collider biases or to condition or to all those sorts of selection biases so it turns out it's not a problem if you're just want to apply and to break down a difficult problem into two easier problems and want to estimate and to make counter factual predictions so the goal here being you simply want to know how much people will borrow depending on the interest rates so you can do that even though you caused a as you broke down the problem into two as you condition on the people that uh converted you introduced bias there but that's not an issue as long as you multiply whatever that is by the effect of prices on conversion and that's very nonintuitive like it doesn't seem that it would work like how can you have a biased measure for an effect and then you multip fly with something else and suddenly the bias goes away it's very contr to it if we say it in words it's easy to see like the math and if you do the simulation it becomes clear but it's not something natural that okay if you explain to someone that person say okay this is obvious it's far from it so this is my current take on it I've looked at this problem from many different angles and my opinion has changed along the year at some point I was like oh this is we cannot do this we should not lacked on this we buy everything we should shouldn't do it but as I studied it and I looked to it and say okay maybe I'm being too harsh here maybe this is some bias that we can live with but I might change opinions in the future like that's where I stand today I think some sorts of biases you just don't have to worry about them let me ask you a technical question so would would this multiplication of those sub problems uh work also in nonlinear cases or only in linear cases they would work in any cases because like it's a simple uh it it has no assumption on no CAO assumption it's a simple let's say a result of how probabilities work so if you take in this case what we're doing is that we take a broader probability so we're estimating the prob the or an expected value so the expected value of Al loone which is the loan amount given price this is the the goal like in this problem we want to understand how much people will will borrow given the price and the issue here is that this is complicated because there's a bunch of of zeros on the Y variable so this expected value will be very close to zero so what you can do is that you can break that expectation into a term which is the expected value of the loan amount given price condition on conversion being one so you got rid of all those zeros and another term which is the probability of converting given price there are no causal assumptions here like you're simply breaking down an expectation into the two terms that it has so it's a simple mathematical truth like and the issue comes when you want to estimate th those terms one of them is going to be a causal a causal quantity because if you random rized prices so probability of conversion given price you don't have to worry about that because price is randomized you are not conditioning on anything perod that that's is going to cause any trouble but on on the other quantity the expectation of loan amount given price condition on conversion that's not call like when you come to estimate that quantity it doesn't turn out to be a causal parameter but again if to multiply things out they work out beautifully what was the favorite uh part of your book when when when you were writing it h the regression chapter I love regression I say it like sort of kidding but it's actually true I love going back to basics and revisiting Basics uh through different angles I think people especially data science they are too quick to this mis regression as a simple and simplistic tool but it's so much powerful than that like uh when you need to control for stuff they don't understand the fact that regression actually is controlling for stuff and they usually want to implement a bunch of group pies and stuff that don't work so I feel that looking at regression from with all the Care that it deserves was very interesting to me was a very interesting study for me because I also had to look back and say okay what is regression really doing what's the assumption that it's making like uh what should I be careful about when I use regression and what should I not be careful what is regression similar to like how can I explain regression so can I say regression is similar to X or is it similar to Y so seeing regression from this multiple angles and giving a think the the attention that it deserves was a very interesting thing from for me to study and also for me to translate it into a language that it's hopefully more accessible and I think Al also the part of uh evaluation CA a model evaluation because that's something that I've been working so long with it it bothered me for so for so much time and it's something that I feel that I have something to contribute on for instance I think that probably what we did in new bank where I I don't remember seeing anything uh relating to caal model evaluations for continuous treatments which is something that we've been doing like for for four years now and I think we have a very not definitive answer but we're more we have a more mature understanding of the problem that most uh think that you see out there so that's something definitely that I enjoyed writing about because again I feel that's something that I can contribute to what will be your advice for people who are just starting with causality and so let me just see if I understand from people that are are starting with causality coming from where like from a data science perspective or fresh undergrad that's want to join data science in general and wants to start with Co like who who are we talking to here so Pi pick the pick the group of the groups that you you feel uh your advice would be the most valuable for I guess I wrote the book for data scientist that they want to transition into to caal inference or to learn about caal inference I think the general advice is to is to be trying to see that the machine learning model is only a tiny piece in the much bigger system that it's making decision for whatever you work for or or whatever your project like yes machine learning is a very fascinating topic and doing predictions a very fascinating topic uh it's not simple by any means you have to be careful with leakages and making sure that uh stationerity holds or if it doesn't how you deal with it so prediction is a very fascinating problem but it's only a piece in the puzzle like after you have a good prediction you actually have to make decisions with that prediction so how do you do it like how do you go from the number that your model is outputting to actually a decision that is close or in the direction of optimal decisions so if you take that broader perspective and you interest yourself in not simply the technical aspect of make making a prediction but actually designing a system that it's overall better I think having this broader perspective helps a lot mostly in the motivation part I think it's something if you want to motivate yourself to to learn causal inference I think that that's it like you should be thrilled by making better decisions not necessarily just by making better predictions like yes making predictions is a part of the problem but it's not the the entire problem what resources would you recommend to to people who are just starting I think that's a very easy selling point so I think any books like for me or for Alex they're very interesting places to start again because we went through all this stuff and it's a very new field and I think for us it was hard in a sense that I don't know your experience but from my experience we had to I had to piece out many papers there wasn't a complete like body of work that summarized all this stuff so now you have the opportunity that this is done you don't have to go through all the work that we had so definitely a work that summarizes caal inference like my book or Alex's book if you want something let's say open source or something that is available for free I really like the American uh economics Association webcasts where they they usually have a course on econometrics every year so every year they they they have someone uh go there and talk about econometrics the one that I really like that I like the most is the one by Joshua angrist and Alberto abedy I think it's from 2020 where they go through the basics of econometrics so I I feel that's something that data science would benefit a lot from learning like the the basics of econometrics understanding what is a treatment effect what's a counterfactual out potential outcome what's a counterfactual how to frame the problem as a decision making problem is an notation problem so those stuff I think it's very a very interesting place to do there's no coding in that so so you only learn the theory is not applicable to the industry it's a course focused on academics but I feel the fundamentals there are amazingly thought like uh Josh is the amazing Professor like of course and learning those fundamentals I think helps a ton uh in learning cult uh cult inference in they apply setting later on is the future Cole I think the present is uh again I'm a very PR practical guy I I didn't got into CLE INF because it was a fashionable like I had a problem and I guess my company had a problem where we had a bunch of models but we didn't want the models like yeah we want Mo good models but the end goal was not a model is is to make decisions and I I think that's why C inference is becoming so popular because there are a bunch of companies that hired a bunch of data scientists and data science are doing a bunch of models that don't actually solve the problem or that that only partially solve the problem like they get to the prediction but then they have to hammer in that prediction into a decision- making process and this isn't very nice like you shouldn't want to hammer a Model A predictive model into a decision framework you would want a a more natural and seamless way to integrate decision making to with machine learning and I I'm pretty sure coo inference is the answer for that so I think in a very natural path forward companies will see okay I care about making better decisions so I want to take this machine Le I want to take this this data science capabilities that I have and I want to supercharge it so that it not only out outperforms numbers which I don't think is the end goal for company comp but actually out outputs decisions and decisions that improve over time and become even more optimal so I think that's the end goal at least in the industry setting I think for sure companies will start to see that they're already starting to see that and that's why I believe like again the present is probably already CA I wouldn't say with that words because it sounds not natural but definitely I agree it is a way to to put it like the present is companies should and they would be better off doing causal analysis what question would you like to ask me I have a bunch of questions that I wanted to ask you like how do you build a podcast for but let's take with uh with with the topic one thing that I want to ask you is that uh you seem can I ask two questions so the first one you seem very concerned with confounding so I'm taking that uh the problems that you face are not the samees that I fa where confounding is not an issue so I I take that uh for what you work with confounding is most definitely an issue like so that's why you're very interested in stuff like uh from your book of partial identification and you're very careful with the graphs and making sure that that works so I'm very eager to understand okay where companies still suffer with uh identification because from my perspective okay if they can control they shouldn't suffer from it and the second question is regarding to caal Discovery this is a topic that I'm not uh well versed in I wanted to know like have you seen any instances of companies using it and applying it into successfully to solving a business problem uh where does COO Discovery fit into the the picture so regarding the first question the first question was about about confounding um I think it really depends on on industry or what companies can do uh if you can randomize that's that's always great even if you don't randomize for the data that you train your model on you can use it for evaluations and that's that's very valuable um but not everywhere you can you can easily randomize so for instance uh in certain settings in marketing randomization would be would be difficult there are certain settings where you can do it there are certain ways that you can leverage randomization for instance little subsamples of of of your your population but it's not always a feasible thing to do so I sometimes like to think about this that there are like open-ended complex systems where we not necessarily can randomize and on the other end of the spectrum there are systems that are essentially enclosed where we sometimes we can randomize sometimes we cannot randomize but because the system is essentially enclosed we can we have very good chances of drawing a a graph that that will be complete enough in order to give us estimates that will have practical applicability I think in my book this comes the the focus on confounding comes from two sources so one is thinking about those cases that are open-ended and complex and the second one is that it comes also from thinking about identification and I think a lot of about identification because uh I think a lot about cases where you cannot randomize or you cannot or sometimes you can control the treatment but you cannot randomize fully so that would be the two main sources of of this thinking and also I think I come from a place where I think about experimentation as a special case of of causal inference so when you look at uh peran causality from the point of view of like a more General perspective in um measure theoretic perspective or topological perspective you can see very clearly that randomization and changing the world physically or factually can be seen as a special case of of Performing causal inference over a system I think what I what I find very attractive in the topological perspective is that you can very clearly see what are the limitations of randomization as well that for certain uh queries that that would be counterfactual queries that you mentioned before uh randomization cannot essentially uh answer those those questions in general of course there are special cases as we also have special cases with observational data where where we can uh answer Interventional queries but this these are only special cases regarding the second question about causal Discovery uh yeah I've seen it in industry and going back to those different settings uh causal Discovery might be much easier when we deal with enclosed systems because many causal Discovery algorithms require that you don't have hidden confounding in your in your data and not all of them but but many of them um and um and so those enclosed systems they are also often well documented so we can we can have pretty complete expert knowledge about those those systems but this is not the only setting where Cal Discovery can be useful um I've seen it used also in different settings that are more open and then often the process is is iterative which means that we collect expert knowledge we combine it with we feed it to the algorithm We Run The algorithm uh and then we either test with with the data that we have or if we have some uh randomized data with Riz data or if we can build a generative model we just compare the distributions to whatever data data we have we confront this with other people and then maybe reiterate with uh with the algorithm so that would be a very general framework for using this kind of methods in in practice can I give an example which I think is causal Discovery but feel free to correct me uh I was talking with this uh this data science and she she worked for a a solar plant company U and the problem they the thing that they were trying to solve is that they have uh solar panels all across the countries and sometimes some of them fail and they have a predictive model to predict which uh solar panel is going to fail next so that they can send a technician there before the thing fails this is good uh but she wanted to to go with even further because once the technician got there the technician didn't know what exactly was the problem because it didn't fail yeah so it could be like overheating could be wind it could be sand something like that so the technician didn't know the cause of the problem and she said Okay I want to help that I I want to fix that I want to give to the technician not only okay this is this is the the pan that is going to fail next and it's going to fail because of whatever I want to figure out what this this is what it is that is going to cause the the the power plant to to go down this seems to me like a causal Discovery problem where you want to understand the cause of of the is let's say a root cause analysis of of of that that problem like is this a way an application of causal Discovery like can use causal Discovery to to tackle this this problem so a great question causal Discovery could be used in this case uh probably not as a as an ultimate solution but as an element of a solution so uh we could use Cal Discovery algorithms in order to find the connections uh within the system which variables are connected to to which ones um if the system is linear we could also using some methods we could potentially also learn the coefficients on the on the edges but if the system is nonlinear of course that would not make not make too much sense that said if we learn the structure which um variables in the system can cause the failure then we can train a a structural model on this um on this on this structure and this structural model can be used to perform root cause analysis so that could be one way to tackle this uh to tackle this challenge now uh when you asked me before about causal Discovery I said that it's often used in a as an element in an iterative process where where people are looking at the graph and so on and so on one of the reasons for this is that causal Discovery from observational data is is is not possible in general without any additional assumptions so any algorithm any caal Discovery algorithm that will take uh will take U and work with will require us to make certain uh certain assumptions but by using Cal Discovery algorithms especially in systems that are uh that are enclosed and I understand that this system with with panels is a system that is like more or less enclos plus some external forces that can be also uh Quantified or measured yeah espe we we know the mechanism from from experts so so this is a case where where this could be a a solution so iter iterative process that involves experts and causal Discovery algorithms uh could be a good first step to build a model that then could be leveraged to perform root CA analysis what was your motivation for for writing your Open Source book your your first book it's a pretty selfish motivation so it was a pandemic till 2020 we didn't know what was happening everyone is in lockdown I say oh I need to do something to go through this pandemic and I think I want to go back to basics and review econometrics so I went and wash watched uh the course that I've mentioned early by Josh and ab and I said okay this is very interesting the way they teach this stuff is very compelling and it's very clear and I want to learn this very well so what I did is okay if I want to learn this I want to take what it's here and I want to apply to my own data so I went and I fetch data and I found or sometimes simulated data but mostly I fetch it from from the internet and I took whatever they were teaching the course and then I thought it again like I translated the course into written material and then with the example and with the data that I had and I used the data I had like to replicate the simulations or the explanation that they gave in the course and this this essentially gave me a translation of their course into python which I then sent to to J and say hey here is the I watched your course I really enjoyed and here is the python version of it if you want to shareed feel free like do what you want with it and then he shared in his Twitter which got me a little bit of traction not much but enough so that some uh people started to comment and to to track the repository the open source repository and this is very interesting because at that point in time then people started to correct me and say okay you did this wrong this is not clear can you clarify this and then I got a bunch of feedback which is exactly what I wanted like I wanted to learn this stuff I think that writing and teaching is a very uh powerful tool for learning but feedback from the community is also a very powerful tool for learning so that's why I mean like the reason is selfish in the sense that it was mostly for me in a sense I think it is it still is for me whenever I see a paper then okay this is interesting let me try to implement it and then I'll publish and then people will comment and give feedback so I guess that's how I tackled the initially the book uh essentially as a translation of Josh course into Python and lat later on uh I tried to take the the stuff that I was doing uh in my company in newbank and I try to to lay out in a format and an applied format and say okay this is what I'm doing which is part two of the book this is not traditional let's say not not harmless econometrics uh uh I certain have to take this with a grain of salt I'm I'm no uh scientist I've never published on this this is what I do in practice this is what works for me this is why I think it works and I started like in the sort of informal science let's say talk about what what uh the things that I was doing and how I was using kadi and that became part two of the book which is essentially uh a huge uh discussion on machine learning and effect hetro geneity and model evaluation which at some point I managed to compile into and to structure into a book format which is also not obviously you have a bunch of content scattered how do you NE them together so that it forms a cohesive structure so it went I that's sort of the point and right now I think the book is still live the Open Source book is still changing I certainly don't update it as frequently as I would like to but mostly because uh again young kid lots of work I need to find more time to do this stuff but that's definitely something I plan to is to keep the book uh updating the the the Open Source book with things that I find interesting and things that I want to learn more about what would be your message to to Coal python Community okay so I I think coo python Community had is in a very good place again because I think coo inference is very practical and python is also very practical so if you want to apply this stuff I think python Cal Community is the the place to be like it's where you'll find models that are easy to deploy that you can go to production unlike with let's say R although I enjoy R but not for Pro for production so you have the benefit of having models that that are very easily Deployable you have now a bunch of materials a bunch of amazing libraries like C I I feel it's ECHL and DUI I feel that they are maturing to to be very standard models for CA analysis and uh all the environment for machine learning which I think is fairly interesting to to use in in caal analysis is also alive and well in Python so I think that's very interesting place to be if our goal is uh applied stuff and Industry stuff mostly tech industry stuff matus what was the most challenging moment in your career so far I think it's conciliating all the the I think it's right now where I have to understand how much time I want to dedicate to work and how much time I have to dedicate to family that's definitely a challenge there's a lot of priority shift that happen once you become a father so I'm having to deal with that it's not a challenge in terms of a technical challenge this problem is too hard is more a challenge of of how do I juggle all those balls at once so I think definitely that's the the biggest challenge right now if if I restrict the the the question to okay what's technical the the technical challenge that I had I would say credit definitely it's hard for me to say okay this is the it's a part of my career that was really tough because I still work with credit I've been working with credit for five years now but credit is a really tough job like uh it's a really tough uh really tough field for instance if you give a loan today or if you give someone a credit card with I don't know five uh 5,000 in credit limits or 10,000 credit limits maybe this person will not default this month nor next month nor the other but in two years time so how do you know like you have to essentially wait a lot of time for you to see what happens to that person once you give the loan so there's a huge delay between you doing something and you actually seeing how that something performs in reality and in a sense it's sort of an explosive business like if you've mess up today you might only figure this out when you're in the past and by then it's too late so it's a very risky risky business you have to be extra careful there is a lot of particular ities and lots of continuous treatments which are very hard to deal with like credit lines or interest rates are both treatments that a bank cares a lot about and they're continuous they don't have linear response functions so it's a very tough problem overall like uh it's it's no joke uh dealing with uh with the problem of giving credit of giving loans to people so it's a very tough a tough n to crack on the plus side is very interesting like I feel like banking is a very nice place to to be if you want to work with with uh with call INF again because there are all the problems that are very interesting and because banking already has a lot of the culture that comes from economy economics the field in the the economics and econometrics is already well established so it's easier to to find literature on and easier to convince people that an econometric approach or hous approach actually makes sense for the problem that that we're trying to solve what skills non-technical skills that you learned earlier in your life do you find most helpful today in your work or when dealing with the challenges that you that you mentioned I wouldn't say earlier I don't know how earlier learned this but definitely writing so I feel that being able to structure your thoughts well and written format is very powerful also in spoken language or power Point whenever you have like an idea and you can structure it in a in a presentable manner I think that's a very powerful skill and especially if you work for a big company or if you have to work with different backgrounds so not necessarily managers right uh sometimes you're a data science and you want to work with engineering or you want to work with uh someone from product so not necessarily your boss or your boss's boss some folks from different backgrounds that work together to solve a same problem you have to to be able to communicate effectively with them and being able to translate the technical difficulties and challenges of a product without dumbing it down like without like dumbing it down and actually capturing the essence of it in a way that it's clear I think it's very a very valuable skill I I didn't Master it by any means I'm trying to but something that I try my best and I I think I'm fairly decent at it let's say who would you like to thank most of the academic c community I think most of them are amazingly kind definitely Joshua angrist for all his work not only his work as a researcher I think mostly with his work as a teacher he's a phenomenal teacher definitely a source of inspiration like the way he teaches is is is amazing again back to the point of communication I think uh teachers have good teachers have that that figured out so we definitely should learn with them and a bunch of nice folks that helped me along the way so for instance I can remember a bunch of people that were very kind to me so for instance Pedro and Carlos Pedro Carlos CIS were incredible researchers in this this this field and also Brazilians they were very kind to to come and talk in a meet up there organized also nick uh who again has a book on causality and econometrics and Shan also when I invited them to talk they were also very kind and lend their their time besides that a lot of people from the CAO Community they tend to be very open and accessible so I generally like post questions and I ask them and they reply and say okay you got this wrong you have to think about this way or they explain something or they give a perspective on something that I plan so a bunch of researchers that I wasn't expecting them to be so open and so approachable so Casper who worked a lot with um putting bounds on tops of uh uncertainty bounds on synthetic controls a Peter H who I learned a lot from in terms of the basics of regression I think learning the basics very interesting so because of I think the researches in CAO inference and economic in general are very kind and I would like to almost all of them I would like to to to learn and everyone that I've interacted with certainly everyone that I interacted with I have nothing to complain like all of the interactions have been very positive that's great uh I think that's a great message to the community as well uh if you have a problem if you want to ask someone a question just reach out there's a there's a chance that uh those people will will answer you and maybe this chance is higher than than you believe yourself today yeah definitely like sometimes the answer will be oh here's a link you can learn might not be a full fledged answer but they will at least point you in the right direction I think that's definitely definitely will be the case where can people learn more about you and and and reach out to you so social media all the stuff related to caal inflence I try to post on social media I try to talk about it I'm not that active uh when I don't have anything new I I I I'm not like I don't do weekly post I usually just post when I have something new or something that I find it interesting or a question that I find that I want some answers for so usually social media also LinkedIn and Twitter I'm usually there you can find my emails also on those those networks and get like if you want to give any feedback on on the Open Source book or Reach Out GitHub I'm also there I'm also active and uh trying to in the the open source Python and CA INF stuff related to python are you planning to still update your your online your online book yeah definitely uh again I already have some some branches that are there uh Gathering dust but I definitely want to go back to them as soon as I have time I want to talk a little bit more about reinforcement learning I think we chatted a lot about it but I've never written and formalized what I'm thinking to a written format so I'm definitely working to some stuff related to reinforcement learning and how it uh relates to what we know as C inference great Matos thank you so much for your time it was a pleasure thank you for having me Alex you're also too kind thank you and hope to have another conversation with you in some time you too congrats on reaching the end of this episode of the caal bandits podcast stay tuned for the next one if you like this episode click the like button to help others find it and maybe subscribe to this channel as well you know stay caal