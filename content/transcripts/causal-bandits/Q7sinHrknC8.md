I can even go further like I will say that like in in some ways dipl Revolution is the worst thing that happened to caal inference in some sense the code is God because it doesn't matter what you describe in your method section and like how well you describe it or not the code is very definitive about what it does and how he does it they don't respect the data generating process it's ineffective it's it's a waste of money to gather all that data but not to analyze it properly and it's also I think it's like it's disrespectful to the patients so I think the hardest part for like machine learning researchers going to caal infant is hey causal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine learning on the internet today we're traveling to Tel Aviv to meet our guest before starting his adventure with computation and causal inference he used to be a bass player he believes in the power of good code and loves to play music with his children creator of calib and research staff member at IBM research ladies and Gentlemen please welcome Mr a hood Caravan let me pass it to your host Alex mol ladies and Gentlemen please welcome Mr UT gavani R thank you thank you for having me I'm grateful for that welcome to the podcast thank you for having me again yeah before you started your journey with caal inference used to be a bass player what is one or two experiences that you had or one of two skills that you learned during your music career that translated to something unique about you as a researcher in causality h that's that's an excellent question so I think um Ed young he was the science journalist for the Atlantic once he wrote he had a piece about like the irrelevance of of Nobel Prize and he wrote like that um science is the teest of team sports and I think there's something about being a musician that prepares you for that especially if you're a best player like that takes like a certain certain character that I guess like I don't know give you appreciation like for Good Foundations having like building a flow that will carry others moving them forward being a basis they can build on um and and and yeah I mean it all exists in music being a part of like a a greater thing and and and and let others build on top of you and provide those foundations I think that the the lip from that to science is is quite small in order to like to collaborate um well in science because in order to make good science like you you you can no longer be a lone thinker like what's good science I think good science is trustworthy science it's science that we know can generalize and we can look at its internal and realize whether it is made up of nothing or it's it has some substance in it good science is science that is communicated properly and and and and and and well to others I mean a scientific discovery worth nothing if other don't know about it like a person can discover tomorrow like what happened before the Big Bang but if it if he doesn't tell anyone else it's it's worthless and so communicating is really important for Science and and and so that's that's the presentation but um but we don't have we we don't want to put like a lipstick on a pig right so like the the essence itself should also be trustworthy and so and so yeah so good science is is is open and it's and and is transparent science MH when we think about causal models especially when we talk about them with people who are new to causal modeling and maybe they come from more traditional machine learning called data science backgrounds those people we will often ask questions wow but how can we evaluate those models in CA of Le that you created there's a module that is dedicated to model validation or model evaluation what was the idea behind creating this module what was the motivation behind it the biggest thing that differentiates machine learning prediction from causal inference I guess like it's at least one of those is like the fun elal problem of causal inflence right the fact that you never have ground truth labels so like you can never truly evaluate whether your model works I guess in a direct way in a direct way yes it's not like you could you could generate like an um an area under the curve and say like it's point8 it's 7 it's trust worthy or not in order to convince people that your model works you need to Market it it's not just the of like showing some numbers and convincing and even if people are aware of like machine learning evaluations then they will expect that and so you need to counter that uh in advance and so causally eval the the evaluation module in Cal really was raised by that need but us needing to convince stakeholders that the modeling process that we did was okay so first of all it's graphical it's there are like many plots with many flavors and colors um you need to make like the presentation compelling um in order to Market it properly and only later did we like converted these uh graphical visual insights into like numeric values to to enable like users to do like automatic selections or selection of models M um based on those ideas what are sub mods of this model give us like a like a you know like a birds eye view yeah so so K generally like it has um it has like a its main components are maybe like an an estimation module which has like a lot of the more more common models that we work with like ipw or an S learner which is named standardization at the time and a t learner or like yeah which is like stratify standardization Co goes way back like it's goes like before the the Kuno paper with those 2019 yeah with those with those naming conventions so like I had to make make those up and so it has like this estimation module um and it also has a a survival um estimation module uh for for survival uh time to event analysis and it also has the evaluation module which again like is is uh is comprised of uh of graphical evaluations it has it basically tries to to replicate psych it learns structure so it has like a metrix module with scores that are like compatible in some way uh which you can like put inside like other components like a like a grid search or or a half space or having healing space um uh uh search objects it has um um like a feature selection module again like it tries to mirror Psy learn so it has like a feature selection module with like confounder selection specific methods and it Al also has a a contri module which as well as like it learn like it it it has like slightly more maybe state-of-the-art models or models that has been contributed like not by the core team so like might not be as tested or um or or that like might be interested uh for for for users to play with you created the at least the first version of coal leap um essentially alone uh which is well I think it's a great it's a great Challenge and a great achievement I I really admire this thank you how was your journey with this uh with this library and and how does it how did it start it started when I joined IBM research so I joined the machine learning for healthcare in life science group when I joined the the the the people already knew they they needed causal inference methods in order to answer the questions that interest them um and so you know like when I when I first like open opened my mailbox there was I don't know if people know but when you when you when you start uh a position in a corporate like you open the mailbox there was lots of like onboarding spam and also like just regular spam spam but there was like one non automated that I receiv was my my mentor isai shimoni with with like a single hyperlink in it to the Hernan and and Robin's book today it's called what if back then it it didn't really have a name it was the ca inference book MH and when I joined um since the the the the team already had some previous projects doing causal inference there were some fragments of code you know like each project had like maybe this that one project like needed the npw and another one needed like nestler know and so it was it was scattered around and well like when a new project came along like people would maybe copy paste those files maybe tweak them a little bit but there was nothing like coherent about it and and then I came along as a student uh in a student position and and I don't know like maybe partly because like maybe coding is like is like donkey walk maybe because I had some basic immature sense about like how important it is to build like tools for other to use maybe it was just like you know maybe it was just a vacuum I came in and filled but I started like uh organizing uh those fragments into one consistent coherent python package with like uh with a reasonable API like I broke down the code because like it was all jangled up together so like I broke it down to like a nice psychic learn like a API and I combined like all the and I combine fractions like from all over the place uh in order to create like a library that will have causal inference methods like for causal estimation that could be passed around a project not not even passed around like be be installed like any other tool that like we use and that's was the and that was like the internal beginning of of of C probably like I don't know like maybe early 2017 and then like once we realize that's like after some like internal tests and uses like in projects that we have I guess like I realize that that this is like something that can be useful to to other people as well to many more people and those are the early days like those are like 2017 again so like do y looks very different from today's d why coal ml is is non-existing I think econ ml is maybe around but like the like the most advanced methods that Eon ml currently has like the papers weren't out yet back in 2017 so it probably looked quite different um than now and so I had this idea that like we will BL and Source it uh it was a idea um and we had some and it required some like uphill battles you know like fighting corporate uh bureaucracy and corporate politics and I was a student like I was not acquainted with with with corporate politics I had nothing to do with it and for me uh my mentor is shy like he had my back and we went along with it and we thought and we were able to publish it uh and make it and make it available for everyone uh to use and that's how we started how was your experience starting in a field U like causality so early in 2017 I mean as you as you mentioned many of the papers for the methods that today are considered being like Classics like double machine learning or or some other methods they they were not even out there and the idea about causal machine learning was something that was incredibly still incredibly Niche Maybe year two years later a year or two years later it started building more momentum with the book of Y as well uh but that was a very very the very very beginning of of model modern causal inference and machine learning so how was your experience how did you feel back there it was tough uh in the sense that as you say um I think there was like relatively few materials available so as I said like I got the Hernan and Robbins book um but that's a hard book to start with like I don't know how other people learn but like that book is so full of it's it's an excellent book it's full of details I regularly go back to it to like relearn things and like get a better understanding but that's not I think like a good first book in some sense I prefer I don't know my learnings you know like starting from a bird's eye view and and and dring down uh and not getting into the math and the details uh very early it's easier to conceptualize like obstructions like when you when you look at at at at at the at bigger picture before then like drilling down to the details so like resources were limited in that sense like you you needed to work harder in order to learn in order to get intuition in order to fully BR like not what the meth looks like but like what it actually does like I don't know like what ipw really does like what standardization really does like how does it balances how does it uh allows us to obtain um causal effect so and and and and also like in terms of resources they were not like digitized and so there was no code like I like code because like code is very direct what he does I mean there there are no second guesses um it does what it does and there will like I don't know if no but like very little available code resources that like would allow you to see what's actually happening M uh within those models how things are actually being done how they are actually being implemented but I think like nowadays is much better like we have more software we have we have more books written like written for for like varying audience for probably more uh computer science machine learning people for like more for econometric econometricians uh or epidemiologists so yeah so so we are definitely at better shape now now it was slightly harder at the start I guess uh but but people had to to to struggle as well in order to make the resources that are today exist today what do you think we as a community should do today in order to make causal inference and causal machine learning causal AI even more accessible to broader public than it is today that's a good question I mean I so we just established that like we already have like the the learning resources um piling up and we have excellent software by now we have excellent software that like given data metrics of covariates and treatments and outcome will spit out causal effects and it will do it nicely however um I guess like as every I don't know practitioners would probably know getting data to look as a metrix of covariates and treatment and outcome is a complex process it takes as much knowledge in causality to organize data to be ready for a caal inference analysis as it is to to develop methods and and applying caal inference methods you need to know like what coari you need to to choose to adjust for you need them to take them at a specific point in time like they need to be like before the treatment happens they can like after the treatment happens the outcome you need to to establish the follow-up time you need to establish a Time Zero all those things are are complex to um all those all those things are are quite complex to implement using like I don't know databases um queries and data processing and I think a solution that will make it easier will be uh will will will provide like a great lip in order to make causal inference easier solution that will allow you to take a database like an event like databases because I mean no one curates data for research for like I don't know unless like you're a research lab that like uh uh uh generates your own data we work like I worked with insurance claims data or electronic health records I don't know like marketing people walk with like events like website events and and and interactions banking um Finance people work like with banking databases like all those databases were not created with research in mind they they have like they they fulfill like a really diff a relatively different need a different purpose in mind yeah a different purpose in mind in in in in the sense that um you know they might be used for audits or for for for for a different task entirely there um and we repurpose them for for for research and practitioners who do that who use like observational data probably know how difficult it is to take the the data that that it's out there and make it and fit it in AC Cal analysis um and so I think a solution that will make make this process slightly more automated will be more beneficial than like another causal inference methodology package or something like that yes so what you say is that you feel that we are actually missing tools for this preparation phase where we can take data that was recorded maybe just for well with with some other idea in mind some other goal in mind and repurpose this data or reshape this data in a way that is um that can be consumed by causal inference Machinery that we already have yeah definitely that's that's the point yes you mentioned uh electronic health records and health projects is something that you worked on like practical applied projects around he at IBM research what are some of the main learnings main insights you got from this uh stream of your work let me give you many people people are fairly interested in understanding what is needed in order to make a a actual real world causal inference or causal machine learning project what are the intricacies what to pay attention to uh when we are in a preparation phase for a project like this so for many people who are just starting with causality they don't have to much practical experience they are very curious about the stuff like how do you look at the data uh how do you communicate with the stakeholders and so on and so on so if there are any insights that you think could be valuable for the community I'm I'm pretty confident that people will be very very grateful for them yeah so I think like the first first and foremost what you need to understand is the domain and for that you don't you don't even need data it's the the the work that you need to do is is before you even take take a look at the data it's it's when you when you try to real to realize what data you even need and so it revolves interviewing the the domain experts like in our case it's probably like doctors and Physicians to set up the problem to know like what questions do you want they do they want to answer um what's the problems what's the what's the complexities in answering those questions all of those can guide you to know what data you want to collect like what's the estimate you want to estimate and extracting that knowledge from domain experts is hard um you need to take their knowledge and somehow fit it in to I don't know to to to a directed A cly graph in some sense right you need to to distill their knowledge to make to make something sensible uh for a causal analysis and so for example like I think after lots of trial and error like we realize that like especially Physicians they know the process process of treatment decision much better than they know the the process of of outcome of what determines an outcome because they prescrib the drugs for example so like they know like what patient would receive like what drugs relatively speaking and and they know where the fuzziness exists which we need to take advantage of in in in in in modeling because like if there are strict guidelines that like everyone aderes to like you have no place to operate within and and make causal claims because I know it's like it's it's that that would be like an overlap issues right you have no variants to to start with to start with yes thank you so we realized that like modeling the the the the decision to treat is much simpler than modeling whatever the terms of the outcome may be and so that's a slow process of interviewing and figuring out the complexities because they will tell like they will start and tell you oh but those kind of patients like they will get something different so you know that's a coar you need to just for I know the what is the best part of working with Physicians and people who are so deeply ingrained in ingrained in the domain their passion they come to us because they have problems that they really seek to solve it's it's and it's a strong driver for for doing good work you worked with on on many interesting projects in your career uh some of them were involving topics like um child delivery you also work with people who are interested in causal effects related to uh s surgical in medical sense surgical interventions what what were your what were the main lessons from from those projects and looking at something uh so close to to human body and human life so I studied computational biology University um because I had a vague knowledge about computers and I know they can be a great tool but I was not interesting in the tool itself I was interesting in what what what it can do and biology seems seemed like a like like a very straightforward domain to do good in I don't know like quote unquote good as I evolved my career like I moved from biology to more Applied Biology right to medicine and and and and and human health uh more directly and so if you if you work on human health it's like it's a very direct way to improve uh people's life I mean they can be lots of other ways that you can uh improve lives in a very indirect way this is what basic research is is is always about right like no one thought we will discover crisper they just like studied some somehow microbiome how bacteria fights against viruses it's like very basic science but it's it end up it ended up being so meaningful I didn't have the patient I want to do something slightly more direct and so working um directly in healthcare seemed like the right opportunity to do that what are the main challenges in working with um with practitioners from your point of view as as as a as a as a scientist or researcher first of all is close as I said as we we said before is like closing the Gap like uh trying to extract Knowledge from them is a very communication heavy like interaction heavy uh it's a communication heavy interaction that you need to carefully listen and like carefully convey and then try to distill what what they say in in in in natural language and distill that into a dag the second thing I think is hard is the disappointment because they are not statisticians and they're not causal inference experts and so they like their world view is uh is based on things they hear from the from like a a pop Media or or po Popular Science in some in some sense and they come to you and they want and and and they and they heard that causal inference is important and it's it now can be done and and and so excited and they're hyped about it and they come to you and they want you to use the tool like they heard machine learning they they heard about deep learning they might even know pie torch or something that then they they come to you and they say like oh you have this coal liip tool come on let's let's fire it up and let's get some causal effects and then you sit down and you start to talk about estimates and causal gaps and a causal Road mapap and Target trial emulation and I don't know time zero and self-inflicted biases and they are buff you can like you can see how less engaged they become as you speak um they came for an answer for a tool and they get a teaching teaching session in some sense it's like in life in general yeah isn't it yeah you want you want you want the easy solution and and you get more homework in some sense and then they become upset because they realize I don't know where where I read it before but they realize that causal inference is a bait and switch scheme because we speak so highly about the importance of causality and we say that counterfactual prediction is so important and and that's the way that we need to operate and yet when we provide the tools we provide a regression model we provide like a like we provide like regular machine learning models or or regression models and we tell them that if they think hard enough and careful enough it will become causal and that sounds like snake oil that sounds like a selfhelp book like the bad kind like the secret was in you this whole time well sometimes it is guess sometimes it is I can even go further like I will say that like in in some ways deep learning the Deep learning Revolution is the worst thing that happened to cause of inference because back in the good old days where like svms were all the agage researchers did mostly did feature engineering they would sit and think about like clever ideas to represent the data so when they pass it through like this simple svm they will get like good results and this has changed yeah and under that climate like I would think it would be much easier for causal inference practitioners to sell what we in caal inference to think carefully about the input data uh what variables you select uh at what time you select them like the temporal relations between variables and how they interact but then like Along Came deep learning like I don't know the 2012 Revolution I don't know like starting Alex net and stuff and the idea that um you will just input as many row data that you have and let the model figure it out uh in a much like more automated way really instilled that Insight in people that like it's you can just take the data and you get something out of it but there is no deep learning framework for causation like identification cannot be automated in such a way or at least like fully automated in such a way it always requires metadata and so when you when you tell so people come with with diff so so so collaborators come with different expectations and when when it hits them that they just like need to be more rigorous about their understanding of the underlying data generating processes they are slightly disappointed but then it's up to you to come and sweep them up and like encourage them and make them partners for this journey with you and and also show them that well there is we we have something that is more than just a regular machine learning model right exactly to debias the effect for instance and so exactly and I think like I said before that like uh it's a betting switch scheme because like the underlying Machinery like under the hood is is is regular regression model or machine learning models but the good thing about about causal inference software is that it makes the counterfactual prediction explicit and so you it it it allows you to conceptually grasp the difference between what the regression model would have done to what a counterfactual regression model does one of very exciting to General Public exciting um use cases for machine learning is is drug Discovery but when we think about the drug Discovery process sometimes maybe repur repurposing existing drugs might be a more efficient way to address certain diseases and so on you worked with uh projects like this where you used causal Machinery to repurpose existing drugs can you share a little bit with our audience about this experience and what those projects were about what were the main challenges and Main lessons from those projects right so I mean first of all just to briefly explain like drug Discovery is the mission to find new drugs um drug repurposing is uh is about finding is about taking existing drugs and finding new usages new uses for them um in some sense that's Pharma companies love it because it it shortcuts the the the funnel that you usually have like from discovering a molecule to testing it on animals to testing it on humans like in varying uh different levels that it's not toxic that it's effective and so on and taking uh existing drugs really bypasses most of this process because because it's already approved so you're like you know it's relatively safe and you just need to prove it's efficient it's efficient for the new disease and there are many approaches you could do for drug repurposing right for from like the molecular level like the proteomics level you could find molecules that bind to the Target proteins and and I don't know like inhibit or excite its function but more broadly in the statistical sense you can even shortcut this process of finding the molecular pathway uh the mechanism itself because you can take electronic health records or insurance claims data um and you can and you can stratify on the people with some disease and you can enumerate all the drugs they're taking and then you can start comparing people who took the drug to people who didn't took the drug and see whether it improves the outcomes related to their disease which is fairly simple conceptually but how do you compare those who took the drug and those who didn't took the drug like you need this comparison to not be biased to be DEC confounded right to isolate the effect of the drug itself and this is where caal inference um can help us that to do this sort of modeling so here in IBM we set out to develop such a system uh which takes like a configuration file written like in very high level abstract level that Physicians can even Define and then it goes through some strong Black Magic database querying of setting up the the Translating this configuration into again like translating into metrices of of confounders and treatment assignment and outcomes which is really a tremendous fit that that the team did here and they did excellent work on it and then provide it into let's say the causal inference engine that will estimate the causal effects the survival difference or the the cumulative incidents used under each drug regime and to do that it also need to be done in a high throughput because we as I said like we enumerate all the drugs patients might have taken that can be hundreds of candidates so there's like first of all like there's in technical terms there's some Docker machinery that needs to run those things in parallel in order to be efficient but in terms in the analysis you also need to account for that later because you have this multiplicity issue uh that you need to account for and doing High throughput analysis like you can no longer tailor the design for each specific drug because each treatment outc compare probably has like a slightly different structure like a dug around it but you can't tailor a specific dag to each outcome treatment pair because you have 100 of those and so you need to to to to have some trust that your data is Rich enough in order to capture some of the internal state of patients like you have all the all their Diagnostics and you have all their tests and you have all their previous prescriptions and so it's not that farfetched that those capture something about the inherent Health stat of those of those patients and together with some variable selection and together with some like proximal codal inference um methodologies you can gain slightly better Trust on your estimates but also like we're very down to earth like we know that candidates being selected by the process I mean that this is a process that generates candidates and candidates selected by their process are not directly translated into like uh good repurposing candidates as in every observational uh analysis like the results need to be triangulated from other sources in order to gain like more confidence in the evidence that these specific candidates are promising for the for for the disease that the test on when we think about triangulation do you believe that technology like large language models can be helpful in this for instance in searching large databases of scientific research and finding candidate articles again right that we could use in order to confront our hypothesis with yes definitely so I guess like loud language models that are slightly more grounded into knowledge basis in some sense but definitely I mean if I would have a large language model um I don't know trained on PubMed for like medical Publications and I find in my Cal inference analysis that some drug with some active ingredient is beneficial and then I go in and I query this large language models and it tells me that there are some articles about that drug being effective in I don't know in mice so that's really strong evidence I don't know if it's strong I mean evidence is a spectrum but that this like further reinforces your trust that this candidate might be promising then it should be moved further along for further testing uh to see if it really if it can be really beneficial when talking about causal Le and then about the system uh that you just described you you emphasize that it's important that those systems are are efficient that they are written in a certain in a certain way now we met yesterday for a dinner and you told me that you believe in in good code yeah why is that if you can trust the code get TR the results in some sense the code is good because it doesn't matter what you describe in your method section and like how well you describe it or not like the code is very definitive about what it does and how it does it and today's research is inseparable from coding software is inseparable from science in some sense because you cannot do Empirical research without statistics without computation without writing code and in order to gain confidence in the conclusions you need to have confidence in the methods so you need to have confidence in code and so testing code is like the most important thing and I know like it's easy it's easier said than done because like when we get to it like people usually don't often like to test their code not just that it's important I think like testing the code will allow you to design the code better in the first place and if the code is designed better if you know if you can break it down into compart into into components it means you can break down the question into components and so those two are really tightly connected in some sense those principles of modularity is that something that you also used when designing caal of course yes no definitely so so designing caal was was first of all a lesson in figuring out how causal inference operates knowing that you can estimate an effect and you can also estimate counterfactual outcomes doesn't have to be the same necessarily but counterfactual outcomes like some models can estimate the average counterfactual outcomes some can some estimate the individual level outcomes some models model the treatment some models model the outcomes and so figuring those out like being able to draw a schematics around it really deepens your understanding about causal methodology like the methods themselves the the theory behind them and then when you put it into code it shines like you see it what are the practical implications of building a modular system like this for causal inference specifically I think and I can speak for caal Le um but I think it allows two things so first of all if it's clean enough and it's simple enough then people who want to learn caal inference caal inference methods can look at the code and learn it also which I think it's one of the strength coal has it allows you to scale very naturally and very easily because using a logistic regression and using grading boosting trees or like a cross valid Val validation based hyper parameter tuning model for gring boting trees is a manner of like changing a single World in your code I'm a very strong proponent of uh of modeling the treatment like even if you want to end up with a heterogeneous outcome heterogeneous effect estimation and you need to model the outcome directly it is still like very important to model the treatment regardless to figure out how the treatment groups behaves like how separable they are how comparable they are whether you have like uh overlap violations that that sort and things and so if you already modeled the treatment and you also like want to model the outcome then like why not combine them why not do a dou robust model right and I think Co allows you to use those components and reuse those components quite easily and do it again like in in a way that scales that that allows you to make to create complex models very easily and so you leave very little residual confounding bias again it's up to the user to to close the the gap between like the causal estimate and the statistical estimate but I think coal Libs allows you to close the gap between the statistical estimate and the statistical estimator really like make them really tight and and really close today you wear an archive t-shirt yes which I I want to take a video of this an additional one so the viewers can see it because I don't know if it's in other cameras what dictated this Choice today yeah so I knew it would be um video recorded on top like it's a podcast but today's podcast are are also video I guess I guess like why not make a statement like if it's already out there like why not make use of it that's like the efficient thing to do in some sense and so preach what's important for you and and uh we spoke about open software spoke about trustworthy open science um an archive in some way encompasses that what's important for you in the idea of open source for open science and open software it's important because it can be checked and it can be assessed whether it is trustworthy and if you wrote the paper in a in a good way and if you wrote the code in a good way way then people can build on top of it much more easily much more quicker and we can move faster and we can discover new stuff much more quickly than we would have otherwise what are two books that change your life I don't know as a teen I really liked to read I don't know Douglas Adams and court vut and Milan kunda I mean those really impacted like uh really affected me but if we're like slightly more on topic so I think can I have three books yes all right so first one will be gak by Douglas hofle really like the most intelligent book I I've ever read I think uh read it during my bachelors it really really made a Mark I remember Concepts like being used so beautifully there and it's written so nicely it's written in in such a smart way it's really a delight to to read and so especially like in in these days and eras of like large language models it's it's nice to reflect back on how on like the basic of language you know like hsky Lamas and such and and and the science like behind it not just like the the billion parameter Machinery that that might be useful for it um the science behind it a second book which we just mentioned um between us maybe the the Design of Everyday Things So when I when I joined IBM my mentor shy um he suggested me that book and it's really an eye opener [Music] for anything you interact with in your everyday life every interface you it's a fast station or free so thoughtful about all the little uh details all the fine detail and all the broad uh aspects and and and specifics and so that's a good book that can teach you about life but it is also highly relevant if you write code because it allows you if you write code and if you do research as again like we said those are really Inseparable so like it really allows you to to organize your thought like around like the most basically like around the API of of code but also like how you organize research and how your research might interact with the world like even even the basic structure of a research paper introduction methods results discussion that's a design Choice it doesn't have to be like that and the third book right so I think the third book is slightly more directly related so I think in in the last like 10 years I think I gained some uh some very specif specific view about science and statistics and how infu to for that how to do it for that and I picked it up little by little fragments by fragments reading books hearing lectures reading Twitter at the time but then I encountered uh Richard Richard meel statistical rethinking and he PS like 90% of the stuff I learned in a single book and so it really resonated with me I think it's an excellent book again about rigorous honest thoughtful way to apply statistics which puts science first and methodology second and Analysis second and and breaks away from the usual Machinery that we so automatically uh go for when you try to analyze data I did some very uh some some some little Consulting also work with some hospitals and and researchers their pis like they do experiment they go and gather data this they spend millions of dollars Gathering data recruiting patients recruiting healthy controls taking measurements some of the measurements are done by doctors by physicians that's they bring them for days and they measure them it's it's an entire operation and they capture all those all these data and when the time comes they treat it like junk they junk yeah they don't respect the data generating process they do a T Test in some sense now they were not taught anything better than that but it's it's somehow it's lazy and it's ineffective it's it's a waste of money to gather all that data but not to analyze it properly and it's also I think it's like it's disrespectful to the patients who gave their time and their hope and thinking like things might be better like they will be the ones who will help make things better and then like being used uh for a T Test even though repeated binary measures in some sense right so I think I think Richard's book is is really uh pins the point on on how to on how to respect the underlying data and analyze it properly what would what would be your advice to people who would like to go into causal inference or causal machine learning research and or practice so I think the hardest part for like machine learning researchers going to caal infant is identification is realizing that the kitchen sync approach we talked about before where you just let the model figure it out it does not exist so that's like the first thing and the hardest thing to understand because people with machine learning background like they wouldn't like it won't be very difficult for them to understand the algorithms and the models that do cause an inference but understanding that each problem has a structure that needs to be respected that's like a slightly bigger jump they need to make and unfortunately as I said like when I started learning C INF there weren't a lot of uh books and resources but nowadays there are high level stuff low level stuff detailed stuff less detailed stuff unfortunately I'm not very proficient in them because I I didn't grow up with them uh but there are and I really do think it's it will be slightly easier for them uh in that regard and I'm very happy for that we need more to jump a ball who would you like to thank my family of course I mean yeah I mean it's probably a cliche but I don't know like I don't know first of all like my parents they like they instilled in me that like learning is important and they put their money where their mouth is like literally I don't know when when I was studying at the University and I don't know if I needed some help they provided they provided help that like allowed me to focus on studying solely just studying and I'm grateful it's it's uh it really did help me um study better and understand more into the present it's my partner and the mother of my of my of my kids um she's a Bedrock she's the most empathic and caring person and and she helps me with our everyday struggles I mean life is much more than just research and so and she's there and I'm thankful for her for that but I guess like more professionally like right it's like like the the mow hierarchy right we talked about life now we can talk about work so I think professionally it will be probably Shai shimoni is is his I mentioned him before he's he was the my mentor when uh when I arrived at IBM he is my manager now uh he's a great person the most very interesting person and he he taught me a lot he taught me about the importance of coding he taught me about design and and he really had an impact in how I perceive uh I guess like maybe research or generally where can people learn more about you and your work with the internet being uh fractioning more and more um I opted for uh my own website recently so we it's uh at a.co Ehud uhco um it has like all the other places I mean at the internet like Twitter and blue sky and GitHub it also has some teaching materials I upload for caal inference that might be interesting uh for people and my blog probably most of it is on medium but some of it not so that's also though what's your message for the Cole python Community keep on rocking I mean it's been expanding so nicely and the tooling is becoming more accessible and more trustworthy and they're doing a great job and I'm so happy to see I'm so happy people can can use it as a getaway to to causality and so just keep doing what they're doing amazing I would thank you so much that was a pleasure so grateful for having for having me thank you very much thank you so much