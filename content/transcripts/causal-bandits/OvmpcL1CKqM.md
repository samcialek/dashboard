whenever you do one of these activities you have to gain a much deeper understanding of the subject matter in order to successfully deliver that kind of content you also lose the temporal relation between the confounders themselves and the treatment so for example when you do that collapsing of data it's possible that some of those features will actually hey causal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine learning on the internet today we're traveling to B delazari to meet our guest he almost became a pilot he studied political science yet ended up getting a degree in statistics just to realize early on in his career that statistics alone is not enough to answer the most interesting business questions he plays cello and is truly passionate about solving real world problems ladies and Gentlemen please welcome Mr I ly let me pass it to your host Alex molak ladies and Gentlemen please welcome Lin hi Alex nice to be here hey hi where are we today well we are at my uh childhood uh garden and the house here in bazari Israel when was the first time in your career when you felt that the statistical approach might be not enough to answer some of the important questions yeah so actually you see it quite early on in in your career I started off at vsat which is a satellite uh Internet satellite internet company based in the states and uh you see folks um running statistics all day long T tests wherever they go and um it's in those instances where you realize that those measurements don't make sense and and you know they they' go off and say that it is statistically significant but you'd understand that there's more than just statistical significance to a result for it to mean something MH that really threw me off and made me realize that there was a whole parallel kind of uh Dimension to analyzing data that isn't about the uncertainty related with just measurement error but rather about the data generating process that underlies it what were your what was your first encounter with with cality or caal thinking so I think that uh if I go back to what I said earlier um working at vasat we we had uh I joined this team um that was working on uh email campaigns uh for our customers uh sending them those discounts in order to retain them and uh I remember uh you know seeing that what was done till that point was to model which customers are about to turn which are not and then sending out those those uh discounts to those who were most likely to churn I went to the literature and read some and and and realized that what we are interested with is not predicting which users are going to churn often times by the way they would churn anyhow and so that made me think well it's not about who's going to turn it's about who's going to respond best to our discount and that might not not necessarily be those high probability churners in fact you can imagine that there would be scenarios where some customers are so upset that you know sending them a $5 voucher would probably not make things better for them and vice versa there would might be customers who are perfectly satisfied with what they have and you sending them that $5 voucher makes them start think about the bills that they are paying every month and whether they should check out the competition why should I enjoy a onetime $5 discount when maybe the competitor the competition is is cheaper so you know these are the slipping dogs the last one I was referring to then there is the uh the first group which you can obviously uh can act on so uh at that point in time I realized that just the statistical associations is not what we are after most of the time time times we are but most of the time as far as decision making goes it's that layer of how would variables in the system react that you are most interested with today you are working on projects that are helping companies make better decisions so right you you you took this path right that day right yes so um you know during that time I started reading up about uh that topic and at some point the topic of cause of inference came up and as you go through their material and you go deeper and deeper into it you you realize that um it's very complex no one around me does it or knows how to perform it so I don't any I don't have anyone to consult and I need to convince everyone around me that it's actually needed so at that point in time I realized that I wanted to do it but I wasn't sure about how what first steps I should be making there was I have never met someone you know um was certified or known for applying causal inference methods and so what I came up with was uh well I need to study this around and at least for me I don't know how most people are but if you just sit and read papers you kind of like chat GPT you are able to summarize them and maybe have an intelligent conversation about them kind of referring to the main topics in a paper but for me to be able to actually utilize those methods in real world scenarios where the settings might be different to some degree than what I've just read I needed some something that will imbue me with a much deeper level of understanding and the way I went about doing it is uh writing a Blog um and I can't say that this blog uh you know got me a lot of attention or a large audience but it's okay because that wasn't the aim to begin with it was was making me uh need to understand the topic well enough to be able to explain it to others and be able to write it down in way that's uh approachable and understandable so if you have to go to these lengths then as you are writing the blog you suddenly realize wait I don't really understand that point to the fullest so you go back and you keep reading and then I'd always make it a point for myself to do code examples like real life examples so again for me for it to be useful and apply it and also come up with you know topic topics I didn't really understand um you know oftentimes I realize that the best way of understanding a specific model is when you want to simulate a model you all of a sudden need to think about all the specifics of how things actually work because you need to code it so you need to be as explicit as possible so in these blog posts I would often simulate those situations and suddenly understand that oh shoot I didn't really think through this part and so that's how I kind of r with it uh and then funny story at some point I was contacted by uh the person who's now the CEO of the company I work for Tom laer and he reached out and said uh you know I came across your blog posts and uh how about you come and uh work in doing causal inference for real and not just writing about it that's a that's a beautiful story like uh you do something that helps you it also helps the community because people can learn from your experience and all the hard work that you did and then finally it also helps a company on the market that just finds a person with face skill set that can help them solve their problems so it's like a triple win exactly and to be frank I didn't expect that scenario to play out because again it wasn't like a huge uh didn't wi fire or anything but you know if you post constantly uh and you make it a point to yourself even though it's not directly walk related and it is timec consuming to not neglect it and post every once in a few months then it does generate the amount of awareness needed to facilitate that that kind of connections so it was a very happy scenario for me because during all that time um I knew that the next way to level up would be to actually walk in doing cause and inference you know so that's the next step in the ladder of getting into the field is actually applying it and then you again go through this real I izations where you know um you get tough questions asked because this is not just a blog post this is your work and your boss comes to you and he tells you well how do you know that it works why should I believe it um what what good does it do and you know you have to answer it you can just say well you know I don't have time for this right now this is my dead job so I need to be able to answer it you got your degree in statistics um how much attention in in your statistics cause was devoted to to causal thinking and causal INF well you know um zero uh there was no attention given to causal thinking whatsoever and you become very surprised in retrospective that that was the case um because in essence causal inference is a sub branch of Statistics um you know it's the world where you you need to walk with assumptions like you do in statistics and build some model of the world and then you apply the model and you you analyze your results so it's all it's in the same state of mind and also a lot of that literature isn't that new I mean you know ruin work from the 70s parls from the 80s so you know it's been there for a while um so it's a good question as to why that is the way it is I mean for sure there is some overlap or many of the statistical tools I learned through my studies were very useful um but in the end they only were uh con uh constrained to the statistical associations field and never really touched about the data generating process and how things actually work under the hood in in the real word scenarios we uh often hear that it's difficult to control for all possible confounders for instance this might differ between different settings right for maybe for a manufacturing process it might be much easier to control for all the factors because this is a system that's essentially isolated from the influence of the external world not completely but but substantially while for a marketing campaign or a political science research those systems tend to be very very open uh and it's very hard to contain all the influences that might be might be relevant uh in your work uh you found a way to debias uh the problems that you work work with at least to a certain extent using the time domain can you share a little bit more about this yeah sure so um just to give some context um I work for a company that uh provides uh an analytics platform and for product and growth thems and so the um problem setup we are dealing with is a product where product managers need to make decisions such as should they sh change the all theing of the menu um should what features in the product Drive conversions uh for example to to PID users and what we so so if I am to State the problem fully let's imagine that um we have this freemium product where you have a trial period and we want to understand by the end of the trial period which of the features in the product Drive um conversions to paid the most I mean that's one of the main money drivers for these apps right and the second one would be uh churn right after that so um and then usually what what companies would do is well um compare um the conversion rates among users who adopted a certain feature versus those the conversion rate among those who didn't and the problem here is and we it's often realized very quickly that those users who adopted a certain feature are usually much more engaged and come up with an initial intent that it's much higher than those who didn't and then the question arises well the differences I'm seeing in conversion rates are those due to the feature adoption or just due to the intent that brought a user to adopt it in the first place so what you're saying sorry for interrupting you what you're saying is that uh users might be adopting features because of the features themselves but they might be also adopting features because of some properties of their I don't know Pro personality exactly or maybe their preferences uh for for novelty or or just by butons exactly or just by how valuable that product is to them right it totally makes sense that users different users find products uh value different and so there's a strong sense of confounded in that area that you know all product managers feel that those users adopting the feature probably just found the entire product more useful and so how can we disentangle uh that effect now one approach uh that's you can take is say well how about we try and control for the Baseline attributes that users arrive at when they join the the product so for example you could measure country device marketing campaign on which they arrived but essentially all these metrics are very coarse I mean you can can imagine that even within certain let's say the US you'd still have a very wide range of personas or user types so what we did here is we used a method that enables us to measure users activity within the app as a confounder or as a proxy to that initial intent or user personality and in that way generate much strong stronger uh uh conditions or much stronger confounders on which we can um debias the results we're seeing so you know just to be to be uh specific you'd might uh say something like within users who have already adopted or interacted with at least five features in the product the difference between those who additionally adopted the feature I'm interested with had a 10% higher conversion rate versus those who didn't now when you say that it makes the comparison much more I'd say um believable or you'd understand that it's much less confounded because those users who already adopted five features cannot be very low intent users or users who don't find any value in the product and so that additional difference between them is probably more due to the additional feature that was adopted so what you're saying is that if somebody adopts five features in in some period of time it says something it reveals something about them so that would be like a an equivalent of a personality test exactly instead of asking people questions we just look at their behaviors exactly which every psychologist would tell you that this is a better test than asking a question yeah so um just to be more specific um there are certain challenges involved with measuring the effect of time wearing treatments AKA feature adoption and time wearing outcomes such as conversions and also using time varing confounders such as activity in the app so right off the bat you know we were up for a challenge because just the way you organize your data isn't uh straightforward and just to give you an idea let's imagine that users usually um adopt the feature by the end of the first day and let's say 50% of them do that and also 50% of the users convert to paying within the first day and that's very common in certain uh products like casual apps for example so now if even before we are talking about confounding if you were to want and measure um the out you know conversion within those who adopted versus those who didn't you'd need to specify some time frame for the uh adoptions and that time frame would have to precede the outcome you're interested with because otherwise uh you know it doesn't make sense to say that the adoption affected the conversion so adoptions have to come before conversions and then in order to cast it as a simple classification problem you'd have to say something like okay well um let's measure first day adoptions and compare it with conversions after the first day so that kind of um settles the problem with the ordering of the events but what ends up happening is that you have to discard half of your conversions because they happen within the first day you'd also have to discard half of the adoptions because they happen after the first day so you can't really frame it as a regular classification problem without losing a lot of data one of the most um problematic phenomenas we saw is that often times if a feature is very strong then the conversion would happen right after the feature adoption and then what would end up happening is that either both happened on the first day or both happened after the first day in both cases you would throw them right because either the conversion or the adoption didn't happen in the correct time frame and what you'd end up thinking is that that feature is actually not very good because all the cases where users adopted it and actually converted were discarded so this discarding of information is not just about the quantity of data points it's also a very bias way of discarding information now um one way you could go about um fixing this problem is uh using a method which I feel is very underrated um that's called survival analysis I was lucky to get introduced to it via my statistics studies and but it's not very well known outside the statistics community and it's actually a third branch of supervised learning so you have classification regression and then the third one is survival which kind of looks like a mixture of both it has both a continuous a binary part and and so using survival lets you um solve the problem I was mentioning earlier You' go about and use what is called time varing covariates to encode those instances where users have adopted the feature but haven't converted yet and those instances where users haven't adopted the feature and you would build a surv survival curve for both groups and then compare the um you know the survival or one minus survival which is the conversion rate at the end of the period And by doing that you'd get um an equal footing for both groups in terms of time they had to convert and also you you wouldn't need to discard any information so really cool methodology I I uh I recommend everyone to get acquainted with regardless of Cal inference um and then the way it ties back to what we were talking about earlier is that um you you can also um model the confounder we were talking about the behavioral ones in the same manner so you treat feature adoption as a sensored time to event uh Target just like you would conversion and your covariates would be the usages in the app or in the product and that way you are able to uh find those um user profiles who convert the fastest to adopting the feature or who adopt it the fastest versus those who usually take a lot of time to adopt it or not adopt it at all um so that's the regist of of the meth so that sounds like what you're trying to do here is you're trying to preserve as much of temporal information possible and leverage it in order to uh one get more get more accurate estimates but two also to exclude the possibility that you control for for a variable for an irrelevant variable in in in certain certain cases yes exactly and to make that last statement more clear what it helps you do is make sure that whenever you um analyze the effect of a certain treatment on the outcome the way that the data is structured Mak shes that it actually happened before the outcome and not for example afterwards and same goes for the confounders the user profile always precedes the treatment adoption and regarding what you said earlier about uh time uh then that's exactly right the way the data is structured enables you to not discard the timeline and but rather encode it itself so you know you could get millisecond precise description of users's Journey I mean he used that feature on that specific second and then he used another feature on this specific second and this is how we traversed through these states so again to make this less ambiguous I would recommend anyone read about survival analysis and time varying coar Ates survival analysis and these are the methods that enable you to um preserve the time Dimension which often times you know it just being collapsed into into one dimension like I said earlier you would say let's look at all first day adoptions or like uses of the app and see how those relate to an outcome sometime afterwards by the way even in that what what I said earlier was the problem that you had loss of information because you know uh the temporal relation between treatment and outcome but you also lose the temporal relation between the confounders themselves and the treatment so for example when you do that collapsing of data it's possible that some of those features were actually interacted with because of the treatment mhm and it would be wrong to you know to condition on them because they are the result of the treatment not preceding of the treatment they don't cause the treatment they are caused by it so when you squash your data and you eliminate the time Dimension then you can get all these backwards relationships that can really bias your your analysis and you know in that case for example if you are controlling for actions in the product that are a result of adopting a certain feature you actually uh biasing uh you are making the estimate look lower than it should be so why what I what I hear say is that you're saying that in certain cases the same variable uh in one case the same variable might be might be preceding the treatment so adoption of the feature in in certain other cases the same variable the same let's say Behavior might be in a result of adopting uh the feature and in the second case if we control for this variable we would actually control for the mediator exactly exactly blocking the blocking the the flow of information from the treatment to the outcome exactly to talk you know in pearls language it would be like you said a mediator or a blocker where you'd be biasing your effect estimates down one way of thinking about this in a in a more um intuitive way is saying that um let's say that this feature is so good that it makes me use the app much more now by controlling for that further activity I'm I'm actually punishing my treatment I'm telling him even within that certain group of users who are highly highly engaged because of the treatment you need to show an extra effect MH and or to talk you know in mediation effect you really remove the mediation effect and you only look at the the remaining effect right so you have total effect that's you can H partition it to mediated effect and direct effect so here you'd be looking only at the direct effect and from a decision- making Point standpoint H you don't really care usually about that break you you care about total effect be it mediated or or direct so it's very important to not control for these activities and by keeping the time Dimension you are able to to overcome that naturally whereas if you were to collapse them you'd need to use further methods to understand what's the directionality you know of of the of the flow so you know talking in pearls terms you you know you need to know which way the L should be pointed yeah in purse framework we can we can show that um the time Criterion is not always enough to exclude spirous relations between variables so we can think about anti-causal direction of of confounding uh what are your thoughts about this in the context of your work that is a a valid concern what you're referring to um for example what is known as the amb bias where you'd have um two uh users who are uh connected through mutual interest and you could show that even though um their self- preferences are preceding the actions that they do in terms of what to read they actually confound that relationship once you control for the for them being connected and so it is uh it is a possibility so what I would say about this is that setting the uh or keeping the time dimension um resolves some biases that can arise in data um such as for example blockers or mediators this is the most prevalent type of biases that um sorting variables on time can fix um but in the presence of hidden confounding um then you'd still need to further develop you know the framework or the model that you're assuming in order to correctly handle those uh apparent biases if we are in a scenario by the way where you assume that there are no hidden confounders which often times is an assumption that is made you know in Practical terms even if no one's really willing to admit it m and then you you then sorting that by time should solve everything because in those scenarios only blockers uh you know are are the cases where you'd want to avoid you know controlling for something so so you say that in the case of embas for instance if we have this collider node and we control for it it allows this uh this purious flow of information but if we would be able to control for those two variables that are connect directing to the collider that would be again blocked exactly and so it's it would solve the problem exactly yeah I'm not sure if uh for our listeners if that would be clear uh on YouTube we'll try to draw a little graph for you yeah uh you also had a great post uh on it so uh thank you thank you um I had a very interesting conversation with with Jacob zitler recently in Oxford and so Jacob's perspective is that caal assumptions come with a cost so for instance running a randomized cont control trial allows us to assuming that it's designed properly and and carried out properly it allows us to assume that there is no hidden uh confounding in our in our data um but this comes with a cost another example would be Consulting with with experts subject matter experts uh and building a complete graph of a problem a complete in a sense of how treatment and and and outcome of course uh but this might also be very very uh costly now on the other uh side of the spectrum we might make certain assumptions about the data generating process uh and this on the other hand might be risky so we might be wrong with those assumptions in some cases we might um perform something like partial identification that can bound the causal effect in a useful way for us or we can perform something like sensitivity analysis that can tell us like hey if this is a hidden confounding U the confounder should be I don't know three or 30 times more powerful than another thing in our model in order to make uh the effect to be zero for instance uh how do you think about your work in the context of this Frame of costs and and and risks yeah that's that's a great question so I'd say you mentioned all sorts of costs like you know um making certain assumptions or um you know uh Consulting domain knowledge experts and in my mind it all boils down to the cost of man hours that you need to pay so for example to to make um well judged um assumptions about your data might require a lot of men hours of a highly trained individual for example or conversing with a domain expert knowledge might entail cost again in men hours of a specific expert whose time is very valuable so to me all kind of boils down to time in in in man hours that you need to pay in order to get guarantees on the results that you are seeing and you know cost of an could also be equated with that so you pay that amount of money in order to be able to get those guarantees and in order to not pay it you'd need to pay that amount of men hours which you could through salary for example you can equate those so what I ended up doing in my work is addressing those situations where uh organizations don't have that many resources and trying to come up with methods and ways is to utilize causal inference in those low resource environments and talking back about what he said that means um having lower guarantees about what you can say from the data and the Silver Lining here is that while you have to be satisfied with a lower degree of guarantees and what you should be thinking of is what type of guarantees you'd have without utilizing those methods and so I always think about causality not as a binary state but rather as a spectrum and as long as you are able to make some estimates somewhat more causal then that's always better than using you know play plain correlations for that matter and so what I've been most interested with is finding those exact methods that yield the highest guarantees with the lowest costs you and your team were were able to help some companies uh improve their their operations indeed significant yeah yeah we so you know going back to the example I gave earlier we had a customer who who has a golfing app and he he had one of these um you know the features in the app that wasn't getting a lot of traction and you know even intuitively thinking when you have a feature that doesn't get much traction it leads you to think that well maybe it's just not a good feature but the the situation is that there are many confounding factors here for example it's possible that that feature is just located somewhere in the product where it's inaccessible or it's possible that it's very valuable to a small subset of users and so using the methodology I described earlier we were able to disentangle uh all these biases and uncover the reality that that feature had a great potential to drive that users's uh kpis and so using um the recommendations we gave him you know he started pushing that feature ahead in the product and he made it part of the onboarding process and by doing that he was able to really pump you know is is Topline kpis by numbers that you're not usually accustomed to seeing I mean I personally was a bit surprised by how successful it was you know us being so skeptical about models and hidden confounding Etc so you know seeing it actually work quite well you know was both surprising and pleasant surprise and you know we serve many clients so you know that story kind of repeats itself in many different scenarios part of of the reason why I work where I work is to be able to propagate the type of research I do to many companies so in a way to be able to multiply um the uh effect or the uh the way that these methods affect uh you know decision- making in general what are the main challenges in terms of data and expert knowledge um or anything else that is substantial or relevant for for projects like this that you encountered in in your career so far yeah I love that question and you know the challenges that you face tend to be very different to those that people usually associate with applying causal inference for once I find that the biggest challenge to um applying causal inference methods um in our case but in general as well is being able to convince um that they are valuable because at the end of the day organizations already collect data and they already aggregate it in some ways and they are already using it for decision making and you need to be able to come up and tell them I'm going to aggregate your data differently and that somehow is going to generate a lot of value for you and this is the main pain point where you need to be able to somehow convince customers that what you're doing is real because up until that point it all kind of floats in the air you can talk about confounding and you know tell them stories but at the end of the day when they need to make a decision it it can be very challenging to make them realize why what you saying is true by the way out of the way of dealing with that challenge is setting the expectations right you're not telling them that what you are saying is necessarily true you just telling that there is a higher probability that it's true or that it's more correct than what they are seeing in their regular data so it's a lot about setting the expectations right and kind of um comparing yourself not to running an RCT or knowing the truth but rather comparing yourself to doing regular associations and then in that context it's easier to show why your work matters or why it's valuable many practitioners raise this uh raise this talk about this topic and about communication with stakeholders and ways to convey value regarding causal causal methods What In your experience worked best right so I think that as at the end of the day one of the main drivers of um trust is your ability to tie back the results that you are showing to the data that you are using that is a point where if that connection is done clearly enough where You' might be able to convince stakeholders uh in truths that sometime might be hard to swallow so often times you know if you tell um decision makers uh you know um results or recommendations that they already believe then it's much easier to swallow and and they they'd be much more willing to cooperate but then you wouldn't be delivering much value because in a counterfactual world where you didn't exist they would do it anyhow it's those places where you're telling something that that is counterintuitive or that might even make them look bad that it's in you know it's Paramount that you'll be able to to convince them and no matter well at least in my experience no matter how much um rigorous you me your method is doesn't matter how sophisticated it is or how well it is shown to perform in some simulated scenarios if you're unable to tie it back to the data then you you'd have very small chances of convincing them in those you know difficult positions and so one of the um main uh pillars of every method I develop is that it's completely transparent in in the sense that you can um see how every calculation that gives you the final answer is derived from your data now and then they can repeat the calculation themselves and see that you know it adds up and you kind of empowered them and gave them ownership on the results because they are now able to rec reproduce them recreate them and believe them that's kind of of of the main line of of of thought uh there in addressing you know the need to convince in those tough decision points is the future causal yes definitely but like I said earlier I I think it's very prudent to treat causality as a spectrum so I definitely see the futur as being more causal um I think that the way that causality will be adopted across organization will depend a lot on the resources that they have at hand um the ones with more resources will tend to use state-of-the-art methods and to answer big questions with high guarantees while the smaller players would probably adopt some sort of um say automation or um Services you know like SAS solutions that would um enable them to tap into causal inference you know with with lower guarantees because again you're lacking domain knowledge you don't have experts that are sitting on it but you will be able to to derive more causal conclusions so definitely and the I think that there is a lot of going on now for coal I mean I think it's been when I started out you know in 2017 it kind of started to take off with the AIC conferences where they would do these data hackathons like competitions and awareness starts to build slowly but intriguingly enough I think that the rise of llms really made and you know the way that they behave like people and kind of give it the sense that machines can be ultra intelligent then it kind of um opens the door for people believing in things that they might find how to understand initially uh thanks to to these late latest developments so I do seeing causal INF frence as a field also starting to to pick up and gain steam H you know in the community and not only in the academy where it's been hot for a few years now but also in the industry itself more and more um stakeholders and Business Leaders feel the need um to invest in causal inflence and understand its value so definitely people starting with causality or even starting with machine learning in general uh sometimes feel a little bit overwhelmed with the amount of information uh that they need to learn in order to make those things work uh you in your life you tried many different things um and you are successful in many of them what would be your advice to people who are just stared right that that's a good question I still remember um the sense of uh all when I started reading up those papers about causal inference back in the day and not knowing where even to start so um one important question is one important answer is don't try to take short cuts especially in the age of llms and chat gpts it's very um you know it's very um tempting to just go ahead and ask the machine for the answer but causal inference requires a very deep level of understanding which I think can only come through very meticulous study of that field now as to how to go about studying that field I guess different people have different things working for them but I can say that at least for myself what worked best was um I started my own blog about CLE in France it was uh back in 2019 um and no one around me was talking about C in France I had no one to consult on that topic and no one was even asking me at work to apply it um but I realized back then that I need to to understand that field and uh by just reading those papers I felt like I only gained a very shallow understanding of what's going on in there and if someone were to ask me about them I would probably be able to give an answer that's somewhat equivalent to what Chad GPT does I mean it sounds about right but if you need to apply it then you're are going to be lost so since I I wasn't be I wasn't doing it at work and I guess that for many people that would be the scenario right because it's kind of like the chicken and egg you don't know how to do cause on inference so no one hires you to do it but if you are not hired to do it then you don't know how to do it so one intermediatory step is writing for example a blog post um or giving lectures whenever you do one of these activities you have to gain a much deeper understanding of the subject matter in order to you know successfully deliver those uh that kind of content so you read the paper and then when you try to explain it and you try to be as specific as possible for example do simulation analysis and you know run specific examples on your own only then where it really starts to sync and you start to really understand it and see how it might behave in settings different to those in the paper you just read for example or how it relates to other papers or what are the limitations that you know not uh mention in the paper but just from your experience you come to realize them so uh that that would probably be it find a way whether whether it's writing blog posts doing presentations iting a newsletter whatever um to start and and get yourself acquainted with the field and then the next up is start working in it now that's that ones can be a bit tough and but if you are able to find a job where you are expected to perform such types of analysis and that type of skill is sought after then you should definitely go for it even if it sounds like a demotion at some point it's it's probably worth it because that's by far the best way to get into any kind of domain also caal influence for that matter who would you like to thank oh well um that's an easy one um so I'd like to thank elad Cohen he was my team lead at ViaSat the first company I worked for you know as an adult data scientist and what I'd like to thank him for is uh first off being a great friend but also imbu me or kind of uh educating me from the first day of work about the value of producing value for the business and you know we analytics people at the end of the day we take data we scramble it some and then we give it to someone else and you always need to be super aware to what kind of value you are bringing to the company and always strive to be able to you know show that value and and pursue it directly and always prioritize it over other stuff like for example the shiniest algorithm out there or I don't know job titles or whatever always look for the value and then from there uh you know go back and think what you should be doing right now to achieve that value and so he was always laser focused and that's something you know I carry with me till now I mean always thinking what the things I do how they impact the business and if I can't answer that question clearly then I know I'm doing something wrong and they should probably rethink this as a child you used to play cello and then you went to study political science is there anything in those experiences that you can translate or you actively use today in your work or something that helps you in your work today well yeah definitely I mean each one of them in a different way I think that and you probably know this too learning to play an instrument is is both fun kind of like our job be fun but it's also tedious and requires tons of um concentration and and so what my music studies uh gave me that I keep with me till this day is the ability to focus and concentrate for very long hours uh you know on specific uh tasks that I want to achieve just like back then I would try to nail that you know amazing uh part in the piece and you'd have to do it repetitively for hours today I would sit in front of an article and try to piece it together and break it part by part in order to really make sense of what's going on there um so that that's for the music part and then for the political science um I think that the years spent in University studying political science and later statistics and economics um really uh cultivated my my awareness to what's going on around us and caring or wanting to make a positive change in people's life and it might not seem highly related at first to what we do but when you think about it decision making makes a large part of the reality that surrounds us basically you humans shape nature and every decision that they are making affects our lives and we all know that datadriven decision making is important but I feel that we still have a very long way to go in doing data driven decision making better and so when I uh help companies and decision makers do that process of datadriven decision making better H I feel like I do contribute uh to to the reality around us to to the people and so in my own small way I am also helping Advance um that front you know little by little beautiful what question would you like to ask me tons of questions it's I'll tell you one that's actually been with me for a while I I would really like to ask you how you you were what advice you would have for folks who want to um take their professional occupation and be able to um well share it with large audiences of people so you know in my case I feel like I've been able to collect a lot of knowledge around caal inference product Analytics statistics and I actually already have thoughts about you know putting it down into some form like text or whatever and just like in the days where I was thinking about making my first steps in caal inference uh it seems daunting trying to you know go out there build an audience and and communicate those um realizations with them in a way you know that resonates with them and and activates them so it was a long question but if you have like one or two tips to how get started because how to do it that's probably a long talk but like what's the thing I should do tomorrow to get me kick started that's a great question uh just start tomorrow I I think you already answered your own question before and I think another important thing is to understand why you're doing stuff you're doing so for me um I learned a lot from the internet just thanks to many people spread around the world who are willing to share their experience of their expertise with with others and anonymously all often often for free and it just immensely help me change my life and I have I have a ton of gratitude towards those people towards the community in general so this is very natural for me to just go there and and do stuff and no my my book is not free but this podcast is free uh my blog posts are free my LinkedIn content is free uh some other content that I will create won't be free but it will allow me to be to build more free content for those people who are not in the place today that they can afford something that costs and some of dollars right right cool that that's really helpful I think that just do it it's very to the point the first step is just do it and do it today or do it tomorrow don't wait uh I think that's the most important part thanks appreciate it what resources would you recommend to people just starting with cality well beside the book you just released and I don't mean that you know as a pun uh it really is I mean it's written very well for begin ERS um aside from texts like yours um which are great introduct ories there are a couple of books and resources that I I've written about in the past so I'll be sure to also uh you know write them about them in LinkedIn soon but one of them is um Judah pearls often less mentioned causal uh inference primer for statistics so it's a it's a small booklet very um to the point uh so anyone wanting to get into dags real quick I think that's the one you should be reading um there is a very good paper that's called a A Tale of Two cultures something along those lines um I'm sure you'll upload the link later but it talks about you know the difference between the statistics community and the computer science community and I feel that it teaches you a lot about about different mindsets of analytics and how we should all be a bit of both I think the these are the main ones I mean d dags for pear and to be honest I mean there are tons of resources out there so I think that Beyond these you'll probably go ahead and drill down on the specific materials that relate to your specific use cases so I hope that helps what's your message to the CLE python community so you know I think that many of the members are probably already aware of how uh challenging it could be to justify the usage of causal inference methods and the uh results that they entail over you know simpler methods and my message to them would be that it's very important to do science correctly and for me it's doing science and when we go and we use data to drive recommendations if we are not doing it correctly if we don't handle confounding if we um remove the time Dimension without accounting it for correctly we could end up hurting the business instead of helping it so it's very important that you you guys persist on your journey to become better practitioners um and remember that it matters it's not just a formal thing where you should be applying causal inference methods because that's proper you should do it because it will help you drive better decision making and ultimately help your business or whatever entity you work for uh Thrive where can people find out more about you and learn more about your work uh where where can where they can find your block where they can connect well first off LinkedIn is a place where I roam quite a bit so anyone feel free to to reach out um and I'd love to connect and and and and chat I do uh as you briefly mentioned maybe I also mentioned uh write a blog post um a series of blogs I try to maintain it every few months and but right now you can go check it out erin.

.io U and you'll find there um a couple of dozen of blogs I wrote over the years and that's about it for now you know uh you do inspire me uh to try and and increase my reach um so hopefully you'll see me in more places beautiful yeah thank you it was a pleasure me as well and I'm so happy that we were able to meet in this beautiful place today me too it's uh it means a lot for me congrats on reaching the end of this episode of the caal bandits podcast stay tuned for the next one if you like this episode click the like button to help others find it and maybe subscribe to this channel as well you know stay caal