So really causality has a hope to make these artificial intelligence systems to be more responsible for the decision they are making. Personally I'm also very concerned about two topics. One is education like education of our future generations. The other is how information is spread especially if it's relevant to uh next round of elections or public policy making. I think their language models might have sort of unseen unpredictable large consequences that might affect our society in a really negative way.

Uh we have recently uh finished a work called a causal AI scientist that basically gathers a set of formal causal reasoning techniques from do calculus to uh other statistical methods like difference in differences instrumental variables and so on. There we are taking this more symbolic leaning approach to let language models only handle simple routing things such as check whether there is a valid instrumental variable and then let it generate code or call an existing piece of code on instrumental variables to get the causal effect.

Uh another line of my multi-agent work looks into AI safety problems. So we put multiple agents in a simulation. One of our 2024 NERPS paper govsim governing in the commons simulation basically looks into if we have a digital fishing village and initiate every large language model as a fisher man character. So we simulate a whole calendar year where each month there's an iteration of fishing. And uh most models didn't uh survive until all the 12 iterations. >> Hey Causal Bandits, welcome to the second season of the Causal Bandits podcast, the best podcast on causality [music] and AI on the internet.

>> Math didn't seem practical enough to her, so she decided to pursue a career in computer [music] science. She's interested in language, ethics, and causality and their interplay in multi-agent systems. She grew up in Shanghai and now she prepares to open her new research lab in Toronto. Ladies and gentlemen, please welcome Dr. Xi Jin Jin. Let me pass it to your host, Alex Mo. >> Hi, welcome to the podcast. >> Hi, Alex. Thanks for inviting me. >> I'm really I'm really glad you found time to talk to us.

I think there are so many topics in your research that will be very very interesting to our to our audience. I wanted to start with a topic that is very hot today something related to large language models and agentic systems. We had a few guests before in the show uh who work at this intersection of causality and large language models and agentic systems and sometimes reinforcement learning like we had Andrew Lampin from Google deep mind amit Sharma from Microsoft research and this intersection is also central to your to your research interests.

I was wondering what are your thoughts on the ethical implications of using large language model based systems what we call agentic systems today in real world scenarios and how do you see causality uh playing in in this in this context? >> Yeah, thanks for the great question. I think nowadays there are a lot of discussions on how to develop AI responsibly or the rising AI safety trend as well. uh to me specifically I feel like uh many of us develop these models with the goodwill to make lives easier, more efficient, more productive and then there are side consequences whenever the agents have non-perfect accuracy then for these error cases who bear the responsibility or on the other hand like even if they have perfect accuracy do we want agents to do the decision or not.

Yeah. and then bring this forward. I think the role of causality can be helped to reduce inaccurate decisions. In the case of for example if the HR like a human resource screening is replaced more and more by models then do they use correlational features which can be gender race and so on or causal features that are the skill sets that's relevant to do the job well. So really causality has a hope to make these artificial intelligence systems to be more responsible for the decision they are making.

And then I guess on the other hand of like whether at all humans should use AI to replace their decision and grant them agency. I guess then there's a mixture of ethical discussions, legal discussions and maybe a type of metacausality in terms of if the system is deployed who will be affected and sort of a bit like a uh consequence reasoning over the society. When you think about this this ethical implications from the perspective of a person who researched co language models in terms of causal reasoning and and formal reasoning, what would be two or three things that you think practitioners that are implementing these models in their respective fields might not be thinking about, but these things might have serious consequences for them or important consequences for them.

>> Very good point. So I think potentially um the HR example is a commonly mentioned thing where the models can make a wrong decision to generalize it to large language model use cases. There could also be models can write malicious code and given that more and more programmers are relying on TGBT and other LRM to handle their programs there could be systematic loopholes that's done in important systems and on the other hand uh it could be about when we are trying to because I've been in touch with people working on different things.

Personally, I'm also very concerned about two topics. One is education like education of our future generations. The other is how information is spread especially if it's relevant to uh next round of elections or public policy making. I think their language models might have sort of unseen, unpredictable, large consequences that might affect our society in a really negative way. >> How could we deal with that? >> Um, that's a big question. I think on like for technology, there are always two ways.

one is to regulate first before the technology is introduced and the other is let it make a splash in the society and I have a feedback loop around it. Um from my understanding I think on the education side since many of us like the school side and the education system side are trying their best to put regulations but in the end it might be too hard to ban students from using LMS to assist their homework their uh projects and many others. So on that side, I'm more a believer of the feedback loop of let this happen and then wrap the education best around this new change.

Whereas on the misinformation and uh um social stability side, I'm a bit inclined with for example how Europe enforce regulations on social media a while ago and there should be regulations on how should be used and how it is consistent with universal values that the western at least the local system uh are endorsing. In one of your papers uh in one of the papers you co-authored uh called called C ladder or cladder you focus on benchmarking clutch language models in terms of causal in terms of causal reasoning.

Just this week we had in our newsletter cosal python weekly we had a piece from David Roder from from Citrio criteria research where he talks about emergent intelligence versus direct optimization and he says there are basically two paths of thinking when it comes to quote unquote intelligent systems, right? One is that we accumulate some kind of complexity and then we hope that something useful will will emerge at the end. Maybe we don't understand the mechanics. We rather look at the mechanics from the complexity point of view or chaos theory point of view.

And then the second path is looking at a system, understanding the mechanics and and and trying to build something something useful like a useful estimator of causal effects for instance and so on. And David concludes in his piece that he doesn't in in his current view he doesn't see these things as compatible. He he doesn't know how to combine this emergent and this direct or or systematic ways of thinking together. In the clatter paper, you are in a sense trying to build a common ground for these two perspectives.

That's at least my understanding. I I don't know if you would agree with that. What was your experience in in this regard? And what are your thoughts about these two paths? Is there is there a common ground? Are we are we moving towards building a common ground for these two approaches or rather they will remain in a sense separate in the foreseeable future? >> Yeah, I think this is a very important question to ponder for all the researchers pursuing color reasoning or other reasoning capabilities of language models.

So translating the framework in the context of clad formal causal reasoning I guess what is meant by direct design is more like give language model more narrow prompts uh tell them exactly that we're estimating average treatment effect at look for these confounders and then please consider the following adjustment methods and so on. I think that's definitely one way to make system under control and reliable. Uh it might be a good way for run two causality in that do calculus with three rules is a complete and sound solution which is also one of our following work covering.

uh whereas on the other side side people do have this uh like in the field of machine learning people do hope to update the parameters of the neural network so that the network itself can handle causal reasoning in an end to end manner and with that I guess if we look at two types of stages for large language models the first stage which is pure pre-training and just uh uh sequence completion as a signal. Um then with that we have at least in the clatter paper and maybe also follow-up studies observe a certain cap of accuracy for models on the benchmark.

However, recently uh people have been actively thinking about how to fill up the rest of the gap between the current performance and uh uh perfect performance on color reasoning and other reasoning tasks. So potentially adding reinforcement learning and other techniques would be helpful. then we might see a potential middle point happening because to make reinforcement learning really work, we need dense enough reward signals. However, how do that come from? It could come from that there is an expert system implementing certain steps of causal reasoning really correctly and then we use that both to generate numerous data for the model and to have step-by-step reward signals so the reinforcement learning system can be more directed towards the desired reasoning.

>> That's very interesting. It reminds me of uh of a debate we had in in the machine learning or AI community a few years back maybe more than a few years back maybe a decade or two back. Uh so we had this system I don't know if you heard about it it's called psych ci uh that was a huge uh symbolic reasoning system knowledge based so there were many people encoding their knowledge in a graph-like structure and the system was able to uh reason correctly over this huge body of knowledge and one of the criticisms of the system was that it requires a huge amount of human work in order to solve of problems and it's not it's not self self-growing right so it's not self incremental it cannot it does not learn on its own then a few years back we started having this uh huge public uh revelation right that large language models uh might actually learn things that seem very very impressive on the surface level we have a deep learning system there right uh something that is learning to predict the next word.

Then we have this reinforcement learning system that you mentioned where we are trying to align this model or fine-tune it in order to make it more useful. Um but I think what we don't see in this current generation of models is that there's still a huge amount of human work in the back end, right? There are literally thousands of people as as I heard or read somewhere labeling the data doing the reinforcement learning like giving providing the responses for the reinforcement learning loop and so on and and there are all the guardrails and so on and so on.

So it seems to me sometimes that we made a circle in a sense and trying to make these models that were supposed to learn by themselves trying to make them reason more systematically. we again put a lot of human work in the back end just in a slightly different way right it's maybe a less structured way at least at the face value what are your thoughts on this >> so I do feel like technology I I like the uh statement from one of my previous uh professors they mentioned that technology improve probably like a spiral and so therefore we might revisit a concept that was proposed decades ago but then given the new hardware and the computational structure uh like we're implementing that in a much more powerful way there could be lots of concepts that are revisited if we look at machine learning publications and uh at least for the deep learning jump it feels to me like the engineering and all the compute resources made everything to another step and then after the big step the further improvements can again be applying methods that has been previously shown successful.

Um and uh we still don't know whether there will be a further step forward and then on that maybe even machine learning is a way to bring some incremental improvement again and uh we'll see. >> You currently also work on on multi- aent systems. How do you think about the systems? what what are your hopes when working on these systems and how you see the role of causality playing out in this context of multi- aent systems. >> So there are different uh uh interpretations of multi- aent systems or maybe instantiations of them.

I can give two type of examples. one type is that I think the ground concept is more that we initiate different large language models and then the type of multi- aent system that we're building for improving causal reasoning has both tool calling ability and also uh critique and debate type of skills. So let's say on the tool calling we basically enable large language models to also implement code and therefore ground formal causal reasoning into actual deps that are programmable and executable.

Um we can also analyze user uploaded CSV data. therefore really imitate how uh uh an actual uh statistics department, a causal inference researcher doing. And on the other side, adding the debate ability into large language models has the hope of sort of pointing out each other's errors or diversifying the space of exploration. Let's say that maybe for run three uh counterfactual causality there is still a lot of problems that haven't been explored prove identifiability and so on. It could be possible that a model come up with various solutions and brainstorm it with another model and together collaboratively refine it to see whether we can automate a mathematician's job to improve the realm of all the um deductions in causality.

Where do you think we are currently with with this ability or capability of models uh building systematic reasoning chains so to say? >> Yeah. So I think at least for causality uh we have recently uh finished a work called a causal AI scientist that basically gathers a set of formal causal reasoning techniques from do calculus to uh other statistical methods like difference in differences um and uh instrumental variables and so on. Um and uh there we are taking this more symbolic leaning approach to let language models only handle simple routing things such as check whether there is a valid instrumental variable and then let it generate code or call an existing piece of code on instrumental variables to get the causal effect.

So I think with that we might just need a bit more engineering effort uh to make models understand a various set of scenarios um before reaching a package that can be widely usable uh across academic disciplines. >> That sounds really great. Uh is that something that you you already published or uh something that is still in in review or or the process? >> We recently put it up. Uh so it's available as a paper and it's in the process of being included in the PIY uh library. So we will put it as a part of the opensource efforts.

>> Oh that's that's really great. That's really great. It reminds me of of my conversation of course with with Amit Sharma and uh some of the ideas that you mentioned here were also the ideas we were discussing with uh with with him. >> Amazing. Yeah, we have a lot of research synergies and I think the spiral de uh development is also our ongoing discussion uh because I really like his aimatic training which encourages models end to end reasoning ability even more. So it's really exciting to see the diverse explorations.

>> Yeah, that's that's really really exciting. Um I I also think so. >> What are next steps for you in your research? >> So uh we have a line of work that try to uh make causality more and more reasonable. Um and we also are trying various reinforcement learning lines of work to see whether they can also evolve end to end reasoning abilities and uh for that entire line including causal AI scientists uh our goal is to make a very handy maybe a chat GPT but the causal version causal GPT that's uh really helping both education like a future generations when they uh need to learn about formal causal inference.

No need to dive into a heavy textbook with a lot of prerequisite knowledge. But I hope it can be a very friendly tutor uh to lead the learners through all the steps. And then I always think that when we man manage our knowledge there is education which means teaching um people with less knowledge background to reach for almost standard and then there is also AI for science meaning that we want to lead people who like more or less have a bit of this knowledge and a very deep domain knowledge to be able to utilize this tool and further advance the frontier of science.

So we're looking forward to this whole line of causality um as a tool helping AI for education and AI for science. And then on the other side I guess as you mentioned uh another line of my multi- aent work looks into AI safety problems. So we put multiple agents in uh simulation. One of our 2024 Europe's paper on uh govsim governing the commons simulation basically looks into if we have a digital fishing village and initiate every large language model as a fisher man character then do they know about how over fishing lead to environmental disaster and there it's more of trying to check whether models know the consequence of its own act and as a toy study before releasing these agentic models into real world maybe they will also do something that's harmful for the climate the workspace if they don't know uh what's their actions will lead to >> this reminds me of the uh tragedy of the commons problem >> yeah totally >> yeah what were the results in the study.

>> The point when we were simulating this uh most models actually couldn't so we simulate a whole calendar year where each month there's an iteration of fishing and uh most models didn't uh survive until all the 12 iterations and probably at that time the state-of-the-art models have around 50% survival rate uh making it to the end. Yeah. And a lot of open-source models that we tested at that time still have 0% um survival rate and they usually overfish immediately in the beginning. And we're also working on seeing how much if we map it to real world situations, how much um this behavior will transfer to if we let models control factory resources and so on.

how well they exploit the pool of resources. >> That's very interesting that uh that so many uh instances of these models just didn't survive in the environment, >> right? Yeah, there are a there's a combination of different things. Some of them is reasoning problem. Some of them is shortsightedness like a a big portion of them actually only look at this month's outcome instead of looking into the long-term future. So I think there it's might there might be a chance for causality to come in and a responsible AI should know about the causality of its own action in four different settings.

One is one pair of them is short-term effect and long-term effect because there are various ways that makes this monthly or this year's revenue to be very high but cause problem long term and then the other pair of consideration is um selfish like self-interest versus group interest. Yeah, there are ways maybe make one factory earn a lot but make others suffer and how to correctly understand both pairs of consequence and make decisions. I feel like it is an important uh piece for the future responsible AI.

>> Yeah, I fully agree. uh when I imagine I I work a lot with people in industry and when I have conversations about this agentic system sometimes I feel there is a lot of uh perception that the systems are very mature and they can do a lot of things they can replace humans in in many different areas uh at the same time I feel there is not uh a lot of awareness of the potential consequences especially mid to long-term consequences of using these systems and and leaving them um somehow unsupervised to make decisions in the longer term.

And I think the example that you gave and and and your research points to this uh this risk very very in a very very pronounced way so to say. There was another uh research very interesting about uh diplomacy. Have you heard about this? >> Yeah. the consequences were also uh not that positive. [snorts] >> What what do you think is the missing point that leads to this self-destructive uh self-destructive paths that the models take in in complex scenarios like this? >> So I think there is really a mixture of causality and morality.

uh that's why also I'm steering my lab's research at the intersection of those more and more. So the fundamental question here is does being what's the relation between being smart and being moral? And if we draw a grid uh potentially if we observe results until today a lot of the a lot of the failure is because the the models are still a bit silly like they don't understand that even for their own self-interest they should do certain actions. Yeah. a failure of self maybe multi-turn uh interest and long-term interest.

That is some of the driving reasons and then in the future we might need to handle a grid of like the models are already smart enough. Then it becomes a question of morality or choice like they can both do something that maybe make them the monopoly of the world of an industry or leave space for other free competition and so on. Then with that capability do they choose to be choose action A or choose action B and then that is a bit more on the motivation of the model. It can come from different resources uh different sources such as maybe in the RHF process um certain mentality is encouraged or maybe in the pre-training process humans wrote about what type of people get successful and these traits got learned by the model and imitated these could both be possibilities and I remember in Jeff Hintington's recent interview, he also mentioned that maybe it's important to inject emotions or maternal instinct to models.

So they are more kind and empathetic and so on. That's one more direction. Um if the model have the choice, which will it lean to? >> That's very interesting. uh when we think about our own motivations, human motivations, there's seems to be a complex interplay of evolutionary and evolutionary in a sense of biological evolution and social uh factors. Do you think that this kind of a complex intertwined interaction between different motivations, different motivational factors could be something that we could create and inject into models in a way that would be effective in shaping their behaviors? Whatever effective means.

Yeah, there is hope like at least yeah for humans as you mentioned like there is maybe selfish greedy interest at the moment but then our community also push us to actions that's more beneficial for the group and more sustaining. um and then these pushes can be distilled into law or unspoken rules in the community. Uh similarly in the context of large language models there are this constitution AI line of approach that in enforces certain rules and principles into model decision making. And on the other side, I also give hope on if models really reason and know about the consequence of their act and also even reason about what is a more reasonable goal like maybe destructing uh the earth or being the monopoly of the market is not that much uh fun if we also think about aspects on the other sides.

Yeah. And uh if it's hard to use reasoning to reach that then we will try to enforce like maybe how uh other people's welfare or other players in the field's welfare should matter to model's decision. But this is still really an open field and we probably need the help of uh ethics researchers and uh philosophers to chime in as well. Do you think that that moral reasoning would be possible without causal reasoning? >> Definitely causality is a key part um in different senses. Let's say that within moral reasoning there are uh two very big branches amongst others like consequentialism and deontology.

uh consequentialism in its essence has a notion of causality in it in that for example if it allows uh if it says like telling a lie is bad but telling a white lie might be good then it's basically talking about maybe if there is a very sick person on theos hospital hospital bed and the level of optimism actually changes the medical outcome of this patient then telling a white lie have certain um possibility to make this person recover and therefore this action might be justified because of the consequence then we're looking at the causal effect of it on the other side for a deontology um let's say uh Emmanuel Kant when he tries to uh look into what are some moral principles that all of us should obey he still has certain type of uh universalization type of reasoning.

Let's say we should not rob people uh because if everyone rob each other gets money just because of physical strength then it is an undesired outcome for society. Therefore like on no ground we should allow that. So there are some proof by contradiction but the backbone engine here is causality. So there could be various reasoning process behind that taps into causality. >> When we think about Kantian ethics in particular, you mentioned Emanuel Kant um and and the moral imperative, right? one of the formulations of the moral imperative that you mentioned if everyone uh was behaving this way this way then something that then the thing itself would would become impossible right that's one of the formulations but I think even when we think about in canon terms we can always met uh some um some special circumstances in which just applying the rule might be very difficult right I don't have a good example right now but we can imagine a situation where there are two moral rules that are contradicting each other in in a certain case.

So that would also require us uh to reason in a systematic way in order to get to some outcome to make some decision. What are your thoughts about uh situations like this? And what would that involve in artificial systems in order to deal to to deal with uh situations like this where we get to this possible contradiction? >> Yeah, totally. I think causality is a very fundamental part of it. Uh or maybe there are two types of reasoning or or two type of elements that's needed when we look into a conflicting scenario.

One is sort of keep running the simulation of what will happen if rule one is dominating, what will happen if rule two is dominating and what will other people see maybe not the direct parties involved in the system but if this is established as an example how will the other like let's say that we have seen in a lot of ransom situations internationally like the number of lives on each side is not equal But when all the nationals watch that news, there is some effect to the whole society that a certain action is leading to.

So that's also part of the consequence. On the other side, apart from causal reasoning, understanding what a future will be like given action A and action B. On the other side, I feel like there is also some like intuition, human intuition, human common sense like maybe in certain case it doesn't make sense numerically weighing the two choices but like we feel something and uh yeah that common shared intuition among people that may be another regulating factor and these two together help us treat situations that we have never met with with more principled solution.

>> Mhm. I have two questions in my mind. Uh I don't know which one would be better now, but I will pick one that my intuition tells me to. So some time ago, I I went to uh Franuis's page with ARGI. Um, and I've seen the the benchmark for for life language models on AGI2, the the the benchmark data set, the new benchmark data set. Now, it's not that new, but but it has been a while ago. Um, and I thought, oh, actually, um, I'm thinking so much about this, how these models can work on this and whether this requires them to reason systematically and so on and so on, but I actually haven't done these tasks myself.

So I booked an hour in my calendar and I did I think maybe like 30 tasks or something. And my thoughts were that that I agree with with France Franu that if you understand start understanding like how these tasks are constructed they become essentially very easy for for for you right for a human being. And so there were maybe one or two tasks that I thought that there were two different rules that I could infer from the task. And so one solution would be deemed correct, another one incorrect. But nevertheless, I was able to find both both rules, both possible rules, right? And then say, okay, one of them probably is more likely.

When we look at the even the latest uh language models and their performance on these data sets, they are still not too great. I don't remember exactly what the numbers are, but they are still much worse than than even like a teenager probably would do on these tasks. Why is that the case? I think we might need to dive more detailedly into the questions because a naive solution I imagine would be that we write a very detailed instructions than maybe a step by step how to unfold human reasoning in this case with a couple of illustrations.

In that case there is a chance that the models can pick this up if they following this reasoning template. Yeah. >> Mhm. Um the other things could be that actually more thinking that the answer to this question might be hinged on the exact type of reasoning. Um there is an ongoing line of research at uh certain large language model companies called elicitation. So in certain cases models might already have this type of reasoning. they have seen it in the pre-training or the post-training process and maybe the context of >> how the question was set up in this specific query didn't make the model think about this realm of reasoning and expectations.

So it might be worthy to look into this and if after confirming that this is really a problem um there is ways to do rof and other processes to distill this. Um then the question becomes do we want it to be a principled layer of reasoning before it enters everything else or how do we prioritize yeah different rows of reasoning maybe a pure interest and pure uh problem solving centered reasoning versus like ethical reasoning before the model conduct any action optimization. So if I understand you correctly uh what you're saying is to give a neuroscientific metaphor here is that there is a part of of of there is a neural structure in the model to say metaphorically that would be capable of tackling this kind of a problem.

The problem is that this neural structure is not excited or turned on so to say when the model is approaching this this kind of a problem. So it's not recognizing in a sense that there's already a existing tool inside of it to tackle the the task. >> Yeah, it could be totally like that and we'll we'll need to see the exact problem to double check. >> In our conversations with with Emra Hijiman from from Microsoft research, he's also involved in PY uh the PY project you mentioned. We were talking about about Sora and these early uh video models.

And so we had the conversation about this hypothesis that these models are might have might have local approximations of how physics work but they they are not physics simulators. Right? So we we we were able to see from the early videos that uh there were a lot of examples showing that there is a violation of physics even if the prompt is just following a let's say regular physics right it's not suggesting irregular physics right the models are generating something that would be physically impossible on on on planet earth anodominy 2023 or four or five but some of these things look very plausible so one of the ideas we were discussing was that oh they might look plausible because the model was able to infer from the videos from different videos showing different angles, different scenarios, different situations some approximations of how physics works work locally and so there is a local representation within this model that is a very good approximations of of of physics but it's not a general model that could simulate physics.

Do you think that when it comes to causal reasoning in large language models, something like this would be also possible? That there might be some approximations to causal reasoning on different ranks like rank two, rank three and so on. But the model in its entirety is is not a good simulator for causal thinking. What would be your thoughts? >> Yeah, so I think there are two types of uh causality here. One is knowledge based, one is formal reasoning based. Let's say that if we're talking about smoking causes cancer comes in in the training data so much that models probably already have a shortcut here.

>> Um or when it rains the ground is wet. Um yeah so at least in one of our my papers we call it either knowledge based causality or common sense causality. Uh models do have a large set of them. And on the other side, if we are talking about reasoning tasks, um, such as if we give a hypothesis here, then is the model quick enough to realize that there's a confounder or there is a collider causing the correlation to be different than the the the isolated one. then that could be a case of like the models are still not that internalized with true formal causal reasoning and they could learn from I don't know confusing correlation with cor causation from the training data itself as well.

Yeah. And we could say that like because a lot of humans fail at these tasks and if the training data largely comes from how human compose their understanding of the world and write out things then the intuition becomes the wrong bias coming from humans as well. >> And what what do you think um would be the path forward towards making the these models more accurate when it comes to systematic reasoning? So I think so far um the aspect that we can make sure maybe if there's a spectrum also sort of go back to the neuroscience um uh analogy then uh in the spectrum it's very clear that we're handling a causality problem then maybe both models and humans will be like alerted and think about now let's pay attention to listing all the possible confounders and figuring ing off the color graph before moving on.

That's great. And I think the real problem is maybe when we ask model a very daily situation or we casually put out a prompt then the models might intuitively think about directly look into pre-training data and output whatever that's a smooth completion here but not actively think about causality. So I think the lab setting is more likely to be addressed very soon and then it's more about like how to permeate that into a scenario where we are not that alerted or the model is not that aware that uh it should actually mix causal reasoning into it.

I think these type of things maybe universally like or very easily activated causal reasoning might be a potential direction to aim at. In your career, you started working with causality at at some point, but before that, you were you were interested in natural language processing. That was very similar for myself as as well, by the way. What inspired you to study causality and to make it uh to put it at the at the center of your research? >> Yeah, I think causality and the language are two different things that really excites me.

I think they both have properties of universality in them. Like language wise, we talk every day. We express ourselves through writing, speaking, listening, and complicated ideas get into books to pass on to different generations. That's great. And then orthogonally if we think about the content of these knowledge I would see that causal knowledge is something that really shines out and make us feel that this is the right like grasp of how things work and it can benefit our future actions a lot.

I do personally have lots of causal thinking but in a very informal colloquial manner and encountering my PhD supervisor Benhad Shukov is definitely a great leap. Um I think before I started my PhD, Banad's causality for machine learning a lot of his talks are really mindblowing in terms of like how we can build a model that is mechanistic and also domain variant. So it has both elegance in its own and also uh effect on making models treat. Yeah, I really like the conclusion that if we only live in the ID world, the learning would have a machine learning correlationbased predictor is great.

But if we want to tag into the OD world when situations change, then being with causality is the way for us to handle unseen situation much more reliably. So I think that is an academically elegant branch of research and also nice philosophy in life to have it through tech along career life and so on. >> What's the what's the best advice you ever got from from Bernard Schulov? Ah, I think he has a lot of them. And uh um I'm also personally very inspired that Banhad has expertise in uh math and statistics but also he has solid studies into physics and also ongoing interests and above that I think he one of his past degrees is related to philosophy as well.

So I think always have these lines of thinking into our mind and be an extra actual mixture of expert is great and also although all the domains we mentioned are very classic Benhad is also in general curious about everything when new students come and in interviews I've heard one of the past student interviews they were even talking about poems And uh yeah I think having in his case three solid domains where he can keep drawing inspirations from a lot of causality insights actually comes from classical physics um that has mechanisms that's invariant.

Um in addition sort of combine it with daily new information coming from students collaborators encounters of diverse background. I think that just helps make this whole I personally feel like it's a academic and philosophical framework more and more complete over time. >> For you, it's also a special moment because you are on the verge of of of a professional transition. >> Yeah, I think so. And uh uh I feel really lucky um that uh actually I got on the job market relatively early uh rather than like compared to my peer PhDs or people in other stages of their academic career.

uh it it feels very lucky that it's a combination of the PhD research is smooth and there were a lot of deliverables along the process and also because more and more schools start to pay attention to the rise of large language models. So I had the fortune to be interviewed a lot of places and encountering different opportunities >> and your decision was >> my decision is the University of Toronto. >> Um it was congratulations. It's great. Thank you and um really really looking forward to one of the birthplace of the modern deep learning.

>> How does it feel? >> It feels very exciting. It's definitely a different set of researchers than my normal let's say in Germany my maxplunk institute circle and uh academically like I have this ACL circle and the causality community like I think University of Toronto gives me a feeling of a very large and diverse like whole university structure where there could be I could knock at the door of philosophers or psychologists and so On on the other side, the machine learning line of research feels very historical whenever I talk to colleagues because they can tell the burst stories about how different activation functions work, how Adam optimizer was emerged and how early lectures of Jeff was like within University of Toronto and even before imageet how convincing Jeff's argument ments are and inspired a lot of generations of U of students.

>> When are you when are you starting at the University of Toronto? >> Hopefully soon. Uh I think my students are already starting and uh we are forming the Genesis lab. Uh also in memory of one of my early mentors Patrick Winston when I was doing an exchange at MIT. His lab was called Genesis and he really wants to implement societies of mind and many different AI theories. I was very very inspired by all his passion into AI and his principled way as a scholar. Unfortunately, he passed away 2019.

And so I think making the lab name both a combination of the pronunciation from Patrick's lab and a little bit of my own name is uh sort of integrates my hope to also proceed in the career of AI with more determination and trying to have something unique into the field. That's a really beautiful story and uh it it really feels you have a lot of uh gratitude towards him. >> Yeah. Yeah. Definitely. And I I I think maybe everyone have the shared experience of like when you're just starting the career uh exp you explore a lot of different research projects and uh people have different work styles and there are so many different AI related topics or within CS different professors have their own style and I'm really really grateful to have very kind mentors.

I think Patrick was great showing me how a successful scholar um who is very very senior can be like and I also have direct mentors PhD students also leading me into natural language processing especially teaching and um so some of them I met during undergraduate some of them during an internship at Amazon and they really helped me to embark my NLP academic IC career before I grow strong enough to navigate the tide of large language models. >> That's that's really great. Before we finish, uh I wanted to ask you about your your more per on a more personal note about your experience with with different cultures.

So you you were growing up in Shanghai. >> Yeah. And now you spend a significant part of your life in in in Germany and in particular in Tubingen which is which is not a huge uh it's it's not a huge city. It's it's a smaller place uh with its own culture and its own uh its own vibe so to say. Now you're moving to to to Canada. How did these experiences shape you and what would be something that you think was the most important in them in making you who you are today? >> Oh, I really really appreciate this question giving me a moment of reflection.

I think in Shanghai I am really really grateful to how my parents protected me out of the super stressful education system. And then there I learned to be very focused and very into education books and uh various like pinning down knowledge great determination and in Tubingan and later Toronto. But Tubingan is a place where I spend a lot of time with the peak of my intelligence and the peak of my energy because of the uh the timing of the age reasons in and then as mentioned I think Banhard's expertise area like math, statistics, physics, philosophy although I'm very amateur in all of them but coming as a natural language processing researcher but being aware aware of all these domains and being surrounded by scholars often featuring expert with expertise in one of these domains opened my academic mind and make me appreciate the continental Europe style of uh reasoning and also for me the life in Tibing is very tranquil and I can focus on a problem continuously without any interruptions.

So it can be in a walk 10 minutes 10 minutes away from the office full of greens and then new ideas emerge. I think that is really really a gift in life and uh I think the story of Imanua Kant walking every day and many other European scholars just keep serving as prototypes when I am a scholar in Tingan. While I will still hold the co- affiliation with Maxfunk Institute and then moving to Canada, it sort of gives a very different vibe in that now I see people from humanities disciplines more out of my like scholarly circle in Tubingan and I see a university whose maybe number of students is equivalent to a small town and that gives a very different vibe and people can com for the city.

They can arrange their life in very different ways. Plus all the faculties who whose name is already super famous in deep learning and played both historical and important current roles. That gives me a feeling of what's the next step of AI where do I expect myself to be in the history of AI if I look back 50 years later. Yeah. So it's a really really exciting place and with a flux of talents now that certain political situations happen and then more and more people might look for places welcoming open science and open ideology.

Yeah. So uh it will be a fascinating start that I'm looking forward to. >> I'm wishing you the the greatest possible experience in this journey. It sounds it sounds really incredible. Uh, and I'm so happy to hear that that that you find so much opportunity in this. The last question I wanted to ask you is about reading. You mentioned reading just before and the importance of reading for yourself in your in your own development. What are two books that change your life? >> Great question. It's hard to list the top two.

uh but I think just very intuitively right receiving the question I really like the history of western philosophy uh by betrand Russell in general like Russell's writing and his way of thinking I was especially inspired by like overview of various philosophers and starting from the ancient Greek time where lots of debate And the pure curiosity like before there were even division of disciplines like just pure curiosity leads to explorations and formalization of a lot of topics. Another peak I'm really appreciating is the emergence of political philosophers in the enlightenment movement figuring out what really works for society.

And then the other book that contributes to my PhD time is uh a biography of Leonardo Davinci. And uh I sort of feel that a lot of us researchers we might be able to resonate with how Dainci observes life, take very very nice notes and try to draw connections of things. Um yeah, there are a lot of ways of doing things that I feel like I both personally resonate with it and I look forward to mentoring my students into these ways of thinking and keep a very curious heart for everything. Disregard the fixed disciplines so that we follow our hearts.

If you're interested, certainly interesting something in psychology, go ahead and pursue it and connect it back to natural language processings if we can or we navigate something that's cool and also just yeah be brave on whatever topic once we apply our thinking into it there is some space for innovation. So I really like that type of courage and diversity. >> What's your message to the causal community? to the cosal community. I think maybe I'm way too junior to give a message there, but um if there is a space for my very very personal view, then I would think that I look forward to causality's connection with more and more domains.

I look forward to its actual impact on things in industry or academic wise how it affects different departments even more. There are a lot of valuable ideas or maybe even how to transform future news consumption and so on. There are a lot of great essence in causality that we should spread it to more and more domains. >> Jin, thank you so much. That was a wonderful conversation. I really appreciate your time. >> Thank you, Alex. This is great.