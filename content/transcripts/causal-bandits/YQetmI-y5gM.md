What are some of the specific challenges when it comes to making causal inferences on data describing human behavior? The problem I see with um Jonathan um hate's work I think that he is tackling many things in a very sloppy manner and it's sloppy on so many levels that it's really hard to criticize it because there's so many aspects that are just ignored such as changes in diagnostic criteria, right? Like if you start telling all doctors they should screen teenage girls for symptoms of anxiety you will find more diagnosis among these groups and so on.

The girls report more strain on their relationship with their parents and so on. So this is actually just a part of the population that completely changed in its composition and it does make it look like the the gender gap opens but it could literally just be like a different population that's moving in. But I think we have good reason to believe that this could actually be like a survey mode effect that like answering on the tablet affects how the girls just answer to that question. I don't think it actually affects the underlying life satisfaction but maybe just their expression of their life satisfaction which points to the issue that you raised that like the whole measurement thing is already a causal causal thing and there are causal assumption in there that um people report the same way over time.

What would be two or three points advice from you to someone who is not a psychologist but for whatever reason they need to work with with questioners determining um which outliers to remove or if to remove any is not a matter it's not a statistical question I think from that perspective like oh what do the dags add right you don't need the dags you can do it without the dags you just need to think harder hey causal bandits welcome to the second season of the Causal Bandits podcast. The best podcast on causality and AI on the internet.

She fights the replication crisis in psychology, but thinks that every scientific field has its problems. She loves causal graphs and clear assumptions. Personality psychologist on a mission to promote causal inference, advocate for transparency in science, and senior editor of one of the top journals in her field. Ladies and gentlemen, please welcome Dr. Julia Rora, let me pass it to your host, Alex Mo. Julia, welcome to the podcast and I'm super excited to have you here. Uh, there are some things that our audience might be not aware of.

Uh, and this is about our common background. So, you're a psychologist by training and I'm also partially a psychologist by training and I'm very curious where this discussion today will lead us. Yeah. So, thank you so much for having me on your podcast. Great to have you here. You published um a paper paper you co-authored in nature human behavior called causal inference on human behavior. What are some of the specific challenges when it comes to making causal inferences on data describing human behavior? So, there was one of those many author papers.

So there are a lot of names on there and I only contributed a small um part but thinking about it I think um one major challenge that psychology faces is that we are interested in psychological causes and these are kind of fuzzy like okay how do your emotions affect your behavior and so okay how do we measure emotions but also what's our identification strategy here right so how can we maybe intervene on emotions to um arrive at valid conclusions or what else can we do to um get at it valid conclusions and for example I'm a personality psychologist so I'm interested in causal effects of personality and that quickly gets very complicated because kind of by definition personality is like a stable disposition and I mean people aren't very fond of causal inference for stable dispositions like some people say they can't have effects and so on so we have all these like major causal inference issues and then there are different ways that the field is trying to tackle them the default is kind of doing experiments although that doesn't work quite as well for many of the variables we're interested in.

And this is combined I think with um a research culture that kind of denies the possibility of causal inference with observational data. So we are like in a very peculiar spot where the causal inferences are very very hard but at the same time we don't train people to think about causal inference rigorously. When people think about so-called reproducibility crisis in sciences, psychology is often one of the first disciplines that comes to mind. And that's probably because around 2010s uh there was this huge discovery that many seinal uh studies and and many very popular studies in psychology were not replicable.

So when other teams were trying to replicate the experiment as as closely as possible to the original description, uh it was the results were surprising in a sense that they were different from the original results. You worked on reproducibility and I was very curious about your thoughts. What role can causality play or causality education play in improving the state of affairs when it comes to repro reproducibility? So I'm not sure how much causal inference can um really contribute to reproducibility directly.

However, I think those topics are very closely connected. And so my original like oh let's improve science background is from the open science side. Um and I think if we look at these failed replications they will often be experimental studies. So it will often be although the the causal inference is kind of plausible but then the statistical evidence is so weak and it's likely been slightly peck to get the right results and so what I've been seeing actually what people do now is oh okay so let's switch to something more robust for example let's do correlational studies of questionnaires and it turns out that these tend to be highly replicable and that is because it's cheap to collect so you can get thousands of data points but also it's very likely that you have some bias or confounding in your data And that confounding happens to be super replicable.

So my background just personality psychology and psych personality psychologists like to get smug about how our findings are super replicable and are not that affected by um the replication crisis. So it must be something that we are doing um right but the thing that we are doing right is just taking correlations between self-report measures that are highly confounded and thus super replicable. So I think in a sense the push for better causal inferences and the push for um higher reproducibility like they need to work together otherwise people go for the findings that are easily replicable but most likely confounded.

That's very interesting. I'm recently reading this book uh called apes in lab codes. I don't know if you know this book. It's by Dennis Landre. I bought it because uh Stephen S recommended this on um on on LinkedIn and we had an interview with Stephen in the first season of our podcast and so Landram is is writing about a couple of interesting he's sharing a couple of interesting insights here in this book. So one of the insights is that psychology is one of those sciences uh you know that inf famously were very often mentioned in the context of of reproducibility crisis and so many people think that social sciences social scientists maybe are not very very well equipped uh to run experiments.

But what he says here is that in social sciences often people are more aware about the role of things like randomization variability and impact of those things on on on the results and on experimental design and on interpretation and so on. He says that's not necessarily the problem and the penetration of these ideas related to experimentation causal identification is in fact according to him uh much lower in so in so-called hard sciences. So that is an interesting claim I would say. So psychologists are very aware of the need for randomization but to the point that it's just like a given right you need randomization for causal inferences but then people also kind of forget why.

So it's just like a rule that you learn by heart and that does lead to all of subsequent inferences that are just wrong. So for example, people will assume, oh, I did a randomized experiment. I manipulated that one factor, right? But now I'm conducting a mediation analysis and I have that mediator that I just observed and that I did not manipulate or I'm interested in how the effect of my treatment varies by this third variable which also wasn't actually randomized. And so there is very little awareness that the randomization of one thing does not justify all causal conclusions.

So I think so one can say this is a good thing. psychologists do know oh I should randomize something in practice I think it would be good to teach them a bit more so that they are aware why they are randomizing that thing and which inferences are supported by that and then of course there are huge parts of psychology that are not experimental and that cannot possibly be experimental and um those are just left um ill equipped so I do kind of agree that psychologists do have like the underlying like right idea that experiments are great they don't quite fully understand why and thus certain things go wrong.

But another part of this is I think right like psychology is always invoked for the replication crisis. But it it has been my impression when I talk to other people at conferences and in private it's very much that other fields of research like epidemiology or economists and so on are like oh it's so great that you already had these debates. So there is already awareness that findings can be false positive, right? And if you try to make that point in economics or in epidemiology and so on, you add like the discussion just isn't quite that far yet with respect to just high variability in findings and a lack of replicability of findings and a lack of computational reproducibility.

I think psychology kind of gets to be the scapegoat, but it really is that we have been really just kind of laser focused on highlighting these issues, which is good, I think. And my hope is that this inspires other fields to move along and to also have these discussions. And for people trying to start these discussions in the other fields, it's always good to be able to point oh look in psychology this and that was discussed. What does it look like in our field and so on. So I do hope that the discussion kind of spreads and um I mean it's unfortunate if people get away with the impression that psychology is not to be trusted.

I think to some extent that is justified. It takes a lot of expertise to figure out like what's the noise and what's the signal when it comes to psychological research findings but I think like on a broader like in a broader perspective it's probably good for science there is one field that really went into that and of course in medical research and so on there have been these discussions as well but I think psychology is um special in that it has been super serious about that and the public has been hearing it maybe because the public was suspect of psychology in the first place or maybe also because the hard sciences like to make fun of the social sciences and so on.

But I think it's a good development overall. That's very interesting. There's a recent or relatively recent article in I think new scientist uh that focuses on reproducibility crisis in physics. So this is something we hear about much less often but it's also happening and perhaps this is also related to the ideas that Landram is discussed in his book uh regarding maybe a lower awareness in the community regarding the sources of variability and their potential to impact our results. You have a very interesting paper and a paper with a very interesting title and this time this is your solo paper.

It's called causal inference for psychologists who think that causal inference is not for them. Why do some psychologists might think this uh this way I think and so this is just the baseline. Nobody in psychology thinks they are doing causal inference based on observational data. Everybody is doing something else. So either you're running experiments and you're not aware that a mediation analysis is in many ways an observational analysis or you are looking actually at observational data but then you just change the apparent goal of your research.

So I get to see a lot of manuscript as a senior editor at psych science and there are so many predictive models. A research study will be framed as oh we are trying to predict this relationship outcome or something like that and then it will be like framed as a predictive research question but both the introduction and the discussion make it 100% clear that this is about causal inference like I want to know how your partner's um characteristics or behavior affect your relationship satisfaction and it really only makes sense from that perspective because if you took that literally as oh we are building a predictive model well you're building a predict ictive model that uses as an input like 10 really long psychological questionnaires and then predicts a single variable.

Sometimes it's not even in the future but at the same point in time and that single variable is like a 10point rating of your relationship satisfaction. Like what would you do with that model, right? Like in which situation do you need to predict people's numerical assessment of their relationship satisfaction and have all these variables to feed into your predictive model? That is quite unlikely to happen in practice. It's also not how like um as a relationship therapist you would work right you wouldn't have a a numerical prediction model instead you would want to know oh what affects relationship satisfaction right like how can I tell whether this is going to make you unhappy or happy and so on and that's actually causal inference it's it's not treated as such so there is that thing that people know that there's a lot of uncertainty in causal inference based on observational data and now instead of trying to draw these causal inferences but adding all the care and pointing out the assumptions.

Instead, they pretend they were trying to answer a different question to which they can give a certain answer. Oh, there's an association here. And then um the uncertainty is well, we observe this um association. What could it mean? Well, maybe it is causality. Who knows? So, it's moving the uncertainty into the research goal rather than into the the answer that you provide. And I think this is very unfortunate. Um it is like sort of an adaptive strategy to cope with the knowledge that correlation does not equal causation but then oh you only can only get correlations.

You don't know how else to get it causation but then you need to sell it like tell a causal story. And so it's kind of like a hodgepodge of things that you mash together into a single paper. And this is really like this is such a common phenomenon like when you start pointing it out to people they're like oh but this is how we write papers. like this is all of our research how like how could this all be um confused or conceptually wrong in a manner and so it's really like deep in the field people will highlight these papers as like examples of really good work but it's like okay but the study doesn't even get the research question right and so this is the one thing I'm up against right now and I think it's like the mission I'm on now is like make people realize that they are actually trying to draw causal inferences and it's fine it's okay so it's not meant as oh you're drawing a causal conclusion here.

I'm going to reject your paper because of that. It is you are trying to draw a causal conclusion here. That's the right thing to do. That is what we're interested in. Now, let's make this in a more systematic manner. What do you think is behind this uh this this cultural let's say this cultural coloration, you know, of of the field of psychology that this is the way we write papers. Uh this is the way we think about it and so on. So I think if you go back um a bit more I think part of it that psychologists have that strong self-concept that we are very sciency science we are like a natural science we have an experimental tradition right and so if you ask psychologists where like modern psychology start it will be here latic it will be vilhelm wound in the first like psychopysics lab so this is very close to our physiological research it's all experimental and so on and I think many psychologists like to think of themselves in that tradition and not in a philosophical tradition.

I mean, but that would be just as valid because we do have these rules, right? But we like to think of ourself in that like purely natural science experimental tradition. And then there's just a mismatch with the research question that we actually trying to answer and with the broad domains that are covered by psychology. Right? So we are not all cognitive scientists running like experiments where you have to push buttons. We do lots of different things but somehow we like to think of ourselves as these sciency scientists.

I think that leads to some mismatch and then people just try to cope with it and they come up with strategies that seem like good solutions and so on. And I think there's also like a dynamic in here. And so this is something I got from um Conrad Kuring who talked about um a different corner of psychology which is like um neuro neuroscience where people talk about functional connectivity. And so you put people into an fMRI scanner and then you have all these voxels in their brain light up and then you do fancy correlations between those voxels and that is functional connectivity right and it's it's a correlation.

It's a fancy correlation but it is a correlation. In the best possible case it might be something like granger causality which is just not causality it's something else right and people are some people are aware of that but at the same time the whole field profits from just kind of ignoring it. And so his point was a bit is like you submit a paper that does this functional connectivity analysis. It's clear that it shouldn't be causally interpreted, but then the reviewers are in on the whole thing, right? The reviewers are publishing their own papers that are telling those causes stories based on these correlations and so on.

And so nobody is really incentivized to say, okay, let's stop this, right? And the same, for example, is true, I think, for personality research. So personality research is a lot of like, oh, we are correlating some self-report measure of personality with something important in your life. And it's very easy to find correlations there. things just correlate, right? And then it's like, oh, this is like the unique predictive role of extraversion for whatever outcome it is you're looking at. And then there's lots of ambiguous language to make it sound, oh, this is the importance of extraversion for career success or something like that.

And so you could say, yeah, but what do you mean like that? What's the role of what is the importance? Do you mean to say that this has an effect or do you mean to say something else? But nobody profits from asking these questions because personality research as a field like thrives on the notion that personality is really important. Everybody wants to think that personality is really important and it being really important means it has all these causal effects. So nobody quite profits from pointing out that the emperor has no clothes or maybe that we don't have good reasons to make these causal claims or these implicitly causal claims.

And so I do think it's an incentive problem and that we are not really incentivized to criticize like the premises of our own field. And this is why I'm always super excited when I can get reviewers from outside of psychology who are just not willing to go by this. Oh, we implicitly agreed that it's okay to put it this way and they like wait what do you mean here? Like what's that supposed to mean? Like what are your assumptions here? And so on. And I think we really need that input from other fields to um highlight these issues.

Before closing the topic leaving the land of reproducibility, I wanted to ask you one more question because I remember in in 2010s when the reproducibility crisis started to be too loud to be ignored, so to say. We had people like Brian Nosk for instance and we had this multilab uh replication project if I remember the name correctly. How successful these initiatives has been from the perspective of 2025. So from my perspective and I'm a bit biased because I'm sort of part of that um movement um but I do think it's astonishing how much has happened within only 15 years.

So people will be like what the hell we've been debating about this for only 15 years. But I think we have seen some super important changes. For example, the notion that you should make your data available to others to verify your findings. I think that was perceived as like threatening and maybe even like how dare you like ask me for the data to at the very least people agree that this is desirable and should be the norm. So people might still um try to find excuses and they are still like oh but what in this edge case where I absolutely cannot share the data in which case of course all the open science advocates are like no then you shouldn't share it right it's okay it's like just like an different situation but I think we have really shifted the discourse from you need to justify why you would want to check anybody's findings you need to justify why you would want to look at their code right their code is none of your business to in principle this should be delivered and in principle it should be checked.

Now how often it's actually checked and how often people come up with excuses and so on that is like a different manner but just shifting the default norm from you can't ask for that it's a weird thing to ask for to no obviously if you want access to these things it's your your good right to ask for them and then the authors are in the position that they need to deliver. So I think a lot of happened there and I I do know it's like hard to keep track um of change over time. I mean this is a causal influence problem but it's also like the age period cohort problem right so um when I think back I was also a lot younger I was a lot more passionate about things right and now I might feel just like oh nothing actually changed um it's all still like horrible and so on but I think some of these changes are just like I have a different perspective now I'm less passionate about these things maybe as I've age but I think objectively there are many indicators just that the norms have really shifted there's a lot of good change um so for example I'm at um psychological science and the editor-inchief now Sime Vazier has been super involved in all the replication crisis open science discourse and um we are now like really on a policy level on the level of the journal implementing many new things such as reproducibility checks so it turns out that is a lot of work but in principle if your manuscript is um accepted at Psych Science it's accepted pending a reproducibility check and then we have a team of so-called star editors who try to reproduce the findings And um I know other journals have done that before in political science in particular.

But having this at black flagship journal in psychology is sending a signal to everybody. Okay. So this is where we're heading now. This is how we are doing things here. And it's such a difference if you compare it to 2010 when people barely had data in any sharable form or maybe some Excel spreadsheet with multiple pages and so on and were like oh why should I take care of that to a point where it's like oh okay if I want to submit to this prestigious outlet I better make sure that everything is reproducible and I think these are like huge wins and then there are many things that are still not perfect there are some situations in which um like the interventions have resulted like negative side effects.

I think one of them being researchers thinking that um just correlating stuff is an easy way out and it is you will get replicable findings but they are just not very meaningful either. So there's the these other topics that we have to tackle now. We need to discuss theory, we need to discuss measurement and so on but overall I'm quite positive about the developments. What do you think would be the main lessons for people from other fields from this progression that we observed in psychology in the recent decade or decade and a half? I don't know how much generalizable insight there is that isn't trivial.

Um I think one um one important thing I think that psychology has highlighted is that maybe before venturing out to do like super expensive replication studies maybe first check using the original data whether it actually holds up because in many cases it turns out it doesn't hold up and then you can still ask oh do I still want to replicate this right but you need to update right oh wait the original finding wasn't what it looked like. So maybe now the replication isn't quite as urgent any anymore and so on.

So I think um there have been a lot of developments in psychology towards oh let's not indiscriminately like replicate everything although I think nobody tried to do that but um I would be careful to say oh a replication is only meaningful if this study is super important and if there's genuine uncertainty because at the same time I absolutely see how the large reproducibility project we had in which 100 studies were replicated. it was important not necessarily to find out the precise rate of replicability in psychology but rather just to raise awareness.

So, we just picked some studies from some um high-profile journals and they don't replicate. Now, what and this isn't maybe the most effective way to conduct 100 studies and maybe not the most informative, but I always think about like what role does this now play for the field and it might be necessary to do something that from a scientific perspective, many of these studies might have been preposterous in the first place, but it's still good, right? Like people take the literature seriously, so let's take it seriously and see how replicable it is.

From that perspective, I wouldn't say, oh, you need to handle this differently. I think you do need to like raise awareness. You do need to flag that this is an issue on a like on a broad scale and then you can go into all the details and so on. But you do need to have the willingness to discuss this as a big problem. When we started our conversation today, you said about psychologists having this challenge that we often need to work on using latent variables. So we we say like oh this is I don't know whatever emotional state or emotion or this is a personality trait.

These things are are very often probably more often than not directly observable. They are our inferences about certain observations right and and they are in themselves in a sense causal models of what is happening inside a human right or another animal if you're a comparative psychologist for instance. One of uh popular topics recently has been the topic of anxiety and depression in younger generations and some of big names in psychology and here I mean primarily um Jonathan height proposed that social media is behind this change that we observe.

This hypothesis that he proposes involves many latent variables. It also involves inferences over time. And when we combine all of this and a couple of other challenging topics that I'm I'm sure you will probably discuss in in the answer to my question that is coming. When we combine all of this, it gives us a very challenging cocktail of things. Especially when we look at this cocktail or want to look at this cocktail from from the causal point of view. What are your thoughts about Height's hypothesis and his defense that the claims he's making are actually causal? That is a very good question and so I am kind of following this discourse.

Um I think my co-blogger Ruben Aslan is a lot more involved and actually going in into it quite deeply. But um I'm actually just right now working on a manuscript whether the gender gap in life satisfaction among students in Leipzig has widened which um I think would be aligned with like the popular narrative that um like it's girls who are getting unhappier now because of social media. Now, in principle, I think um the idea that social media impacts people's well-being, like there is it just rings true, right? It has the the right vibes, it has the right wipes for me as well.

And in particular, I think I have like a I found a good way to deal with social media. So, I mostly get positive emotions out of it, right? You know, when I see that, oh, I'm now scrolling Reddit and it's all horrible. actually I just stopped doing that but I don't need to make my myself feel miserable and instead I'm having a good time on blue sky with um other people working on similar topics or I have a slab with flag with close co-workers and I actually think a lot of young people are more moving into closed spaces where you have like your friends and actually are just chatting on WhatsApp or something like that but in principle I could see that if you like like for individuals who haven't figured out a good way to integrate this into their lives and so on it could be an obvious issue and then there is I think always like right there is like edge cases where people find a community that's not healthy for them right that resonates with them but that leads them actually to do to develop mental health issues as they are overly focusing on one aspect of their lives right and so in psychology in principle right the idea that how you think about things affects how you feel and might lead to mental health issues it's it's super valid and it makes a lot of sense that things like that could happen on social media now the problem I see with um Jonathan um hates work.

I think that he is tackling many things in a very sloppy manner and it's sloppy on so many levels that it's really hard to criticize it because there's so many aspects that are just ignored such as changes in diagnostic criteria, right? Like if you start telling all doctors they should screen teenage girls for symptoms of anxiety, you will find more diagnoses among these groups and so on. And so some patterns in the data that he picks up I think are such diagnostic changes and so on. And this is leaving out the whole whole issue right that of course um apart from that diagnostic behavior might change in its own right which we know from for example autism diagnosis right so there are studies that suggest that the symptoms have been fairly constant but then diagnosis have ticked up and I mean this is very much like one of those measurement issues right like what's the causal connection between the thing you're interested in and that which you can observe.

So there are these issues. Um then there is just like you know the thing where you take like a a timeline and then you see something goes up and you're like yes this is where social media started. it it doesn't quite work like that, right? Like um I mean this is more like economist's domain, but being like, "Oh, I have a time series and here something happened, so this is causally attributed to what happened there." Is kind of it doesn't quite work that well, right? And I think there are layers upon layers of issues there.

Um including so um I think um hate says like, "Oh, I'm super open to criticism and so on and so here's like a page of Irata and so on." But then he repeatedly gets called out for known issues in his data and so on and they don't show up on that page. At least they haven't showed up in the last time I checked. So there's like a commitment to let's do this super scientifically and so on. But then it's kind of like playing it very loosely. And I know a lot of psychologists play loosely in that manner, right? You just throw more data at people and then you hope that they agree with you that you can't deny that something is happening there.

And I don't appreciate that. I don't appreciate in particular because I think this is an important research question in which I'm interested myself, right? And I have young kids and it will be relevant for them as well and so on. So I wish there was better research there. Um I'm 100% certain that social media will have negative effects for some people and I'm also quite certain that it will have positive effects for other people, right? Um, so this is kind of like a given because the effects will of course vary depending on what people do online and so on.

There are these like broader like um cultural counterfactual questions like what would the world look like now if we didn't have social media? Would we have Trump or not and so on and but these are like a different level of analysis again right and this is not something at which you can get easily with um randomized experiments where you tell people to um like um stop using their phones and so on. So I think it's just a lot of uncertainty there. And I mean for example um I can tell you a bit of my of my own study that I'm working on if you're interested in that.

The city of Leipzig conducts um surveys of um school children. And so we have data from 2010 2015 and then 2023. You can already tell that the gap there got wider because during co they couldn't go into the schools to collect data. So you already know oh something changed there right? And so in our data there does seem to be a pattern. So first of all, everybody is slightly less satisfied with their lives in 2023. So this would be life satisfaction as one possible measure of subjective well-being.

It's not the same as mental health, but of course it is correlated with that and it's a good indicator of how people feel about their lives. So first of all, we see that everybody is slightly less happy. Um which I think makes a lot of sense because these are students who um uh had the coid9 pandemic during a critical phase of their life. So this was definitely an advantage and it will like um impact their well-being at least for the time being, right? Um but then we also see that the gender gap seems to widen more among girls and that's kind of curious, right? Like what's going on here that is making um girls less happy and so we kind of try to figure it out because it turns out it's not quite that easy.

For example, we do find that the widening of the gender gap is super pronounced among students with a migration background. Then when you look at the um migration backgrounds in litig they changed a lot. So in 2010 um we mostly had students with a Vietnamese migration background. So these were like contract workers who came to East Germany during the socialist days and so on and then they had children and these are like the the students with the migration background in our 2010 data and this is just a very peculiar group.

So for example, they were like super successful academically speaking and so on. And if you now take the 2023 data, the students with the migration background, they are often from Syria, Afghanistan and so on. And it's entirely plausible that there we just have different gender gaps in life satisfaction. And it is quite plausible that they are larger in the um populations from the Muslim countries and it seems to be the case that the students also report like the girls report more strain on their relationship with their parents and so on.

So this is actually just a part of the population that completely changed in its composition and it does make it look like the the gender gap opens but it could literally just be like a different population that's moving in and then we having all sorts of other issues. So for example in 2023 they started collecting paper on tablets rather than on paper. So we find that super peculiar thing in our data. So they couldn't always use tablets. Sometimes they had to switch to paper because some schools opted out which obviously is like indogenous to the school but sometimes it would also like they had not enough tablets available because there were multiple classrooms being served at the same time or they had technical um issues and so on or they had no Wi-Fi and so some classrooms used paper and now we have that very peculiar observation that the gender gap is larger on tablet than on paper.

So the girls who responded on tablet report that they are less satisfied with their lives than the girls who responded on paper. And that's not the case for the boys. For the boys, it's exactly the same average satisfaction. And of course, this could be confounded because we don't quite know how the classrooms ended up um using paper. But I think we have good reason to believe that this could actually be like a survey mode effect that like answering on the tablet affects how the girls just answer to that question.

And I don't think it actually affects the underlying life satisfaction but maybe just the expression of their life satisfaction which points to the issue that you raised that like the whole measurement thing is already a causal causal thing and there are causal assumption in there that um people report the same way over time. So with all of these nuances we still find slight widening of gender gaps but it's in particular for social life and for um for leisure time activities. So there might be something that makes girls unhappier relative to boys, but it's kind of limited to a certain space and there are all these other things happening.

So we do have that change in the underlying population. We do have the change in survey mode and so maybe girls just have negative associations with tablets or they are not confident using them or maybe the boys are just like, "Oh yeah, I love this. This is where I play my games, right?" And so they are not affected by it and so on. and we have all that nuance in there and the conclusion is less catchy when um like smartphones or social media are ruining a population. But I do think it's also more interesting to actually look at it more closely and to try to figure out what's going on.

And so my one one major concern and I think this is not something that we could easily rule out just by collecting more data or running fancy analysis and that is of course that the possibility um that how girls assess their lives might of course have changed and this could actually be due to social media right like social media might for example um give you a sense that oh actually you're entitled to the same as boys right so like certain feminist narratives and so on and then if you check at how's my life going and now you translate that into a number the translation might change because you're like well it's going fairly well right but I see that I'm being held back maybe by society so your rating gets lower right and you just can't distinguish that from substantive changes in the life satisfaction right because we just get the observed data and now people do try to like oh let's use a multiem scale and then let's do an analysis of measurement invariance um however I don't think that there are statistical solutions to the philosophical issue here.

And um I think ultimately for that part the solution is really that you need to talk to people and ask them like what goes into that decision and there um is exciting um forthcoming on like in preparation work on that um by um Mark Fabian and colleagues where they actually ask people how they use these scales these life satisfaction scales and it's already super interesting because for example you give people a scale from 0 to 10 they will tell you I'm a seven I think an Eight is the highest I could be.

If I had a higher value, I would need to be a different person and I don't want to be that. So, and this is like if you think about it that manner, it's like okay, so what variability are we actually capturing with that item? Because people explicitly tell you like in this world there is no version of them in which they were a 10. So does it mean you should rescale the data and the eight becomes the highest option for them or how what does that have as implications if you aggregate across people which you always have to do right and I think there is a lot of stuff going in there and I think one conclusion um from that line of literature for me is that these life satisfaction scales the very top they are kind of like losing their regular properties it's no longer the case that you can just assume that just because somebody gave a higher number they are actually more satisfied.

So maybe if it's from zero to 10 maybe the variability 8 n 10 is not whether people are more satisfied but rather whether they like how they think about their lives and the best possible world and so on and I think that does actually affect um this whole literature because at least if you look at western um countries you often do have a lot of bunching at very high responses so usually like a seven or an eight and that does mean that it's unclear which variability we actually pick up and then if its comes to looking at people maybe in the global south I'm less concerned about that because there there is I think a clearer mapping between like five six and so on it's like okay you know it's going better now so I will give a higher number but um we are kind of reaching a point of saturation here where people can't really imagine what the 10 would be that's very interesting I had a conversation recently in the context of doing causal inference in the tech industry where you also have some kind of questioner some kind of response on on a scale um and and I had a conversation with this person that you know like in psychology people sometimes say well we can treat it as a continuous variable this the response on the questioner right others will argue that well the mapping is actually it's an ordinal mapping we cannot make it continuous because the differences between two and three might be very different than four and five but now what you're saying is that there there might be another challenge that people might actually think that their scale finishes at eight even though the nominal scale finishes at 10.

And I think that's a completely new dimension, a completely new challenge when it when it comes to thinking about those uh questionnaires as as valid variables in any kind of inference scenario. Yes. And so the right people always argue, oh maybe it's just an ordinal measure at best, right? So you need to like fit a cumulative probit model whatever. But that is like the easy part I think. And I mean we do that in our paper just to confirm that the results actually don't change at all. So um that's the part that people like to argue about because it is like oh there could be like an objectively right or wrong answer and we kind of know how to get at it right because we know that the ordinal model is the more general model and so you should actually always just run the model except you don't because it's complicated you don't understand it but that is like the easy part I think the much harder part is okay are people actually using that scale in the same manner and um I I people use scales in different manners like I'm teaching questionnaire design and it's obvious that people interpret these scales in a different manner and one example is when I was studying psychology my then partner now husband would sometimes be like my guinea pig and so he had to fill out some questionnaire and there was like um some item that was like oh I am like fairly happy with this or that and then he would be like oh but wait the response scale is now like agree like don't agree agree a bit very much agree and so on and he like oh but now if I take this literally then maybe my answer should be and it's like no no no it's not supposed to be meant literally right like usually if you ask like dichomous question we still hope that people use the whole scale to just give us their like variation of where I am on this and not take it literally and be like oh this is a dichomous item so there are two possible answers here the one end or the other end but there are these people who take language um quite literally and they might be over represented in computer science I don't know um but it is like it is raising all these um measurement issues and we do know um that it's um a problem for happiness scales also in another context.

So we have that nice scale that goes from 0 to 10 or 1 to 10 and it turns out that some people are just overwhelmed by that and turn it into a threepoint scale. So it's zero or 0 five or 10 or 1 5 or 10 and then you will have bunching of responses there and it does seem to be um people for example with um lower levels of education which might sense because they might be just cognitively overloaded whereas me as a psychology student I I know how to use these scales. have done it a thousand times and for them they might simplify it into something simpler and then it might actively bias our conclusions with respect to for example how um educational attainment affects um happiness because our happiness measure just operates in a completely different manner depending on the level of um of educational attainment and so these are issues that go under the label of measurement invariance I guess and I'm going to submit a paper on that today um but I think treating it just as like oh this is statistical issue and we run these model checks to get it.

It is it's just not cutting it. It is going deeper than that and I would be excited if people took that more seriously and also did more qualitative studies. I would like to see more of that because I think a well done qualitative study is a lot more informative than like a shoddily done um quantitative empirical study that just follows like a standard template and correlates a bunch of variables. Great. Congrats on the paper. What would be two or three points advice from you to someone who is not a psychologist but for whatever reason they need to work with with questionnaires.

So maybe we can imagine someone working I don't know at Booking.com or Google and they want to measure uh the satisfaction of um of of their their users with with an app or or like another or some aspect of the web page for instance. What would you say? So I mean my top advice but this is more top advice I would give to psychologists because they are more likely to forget it and I think actually people in the industry might be more aware of that but like you should absolutely like closely look at the items and fill them out for yourself and just like you know keep an eye on what am I thinking while I'm filling that out and then maybe also hand it to a couple of other people's because you are the one designing the survey so you're kind of biased to say oh this is all fine right so give it to your partner to your roommate to whomever, right? And let them fill it out and let them tell you afterwards or why they are doing it, like why they are giving these answers and what comes to their mind.

And so this is what I teach my students to do. So it's a lot more um actually looking at the content of what you're asking and whether that makes sense. Then there's often like the mistake that people use response scales that don't fit the items. That is very unfortunate now. is just like inelegant and you can avoid that easily by giving it to somebody who has a skeptical eye on that. So this is I think what I would consider um very important and then there is of course all the um standards um like stat right oh is it really a continuous variable should you order analyze it as ordinal well just do it as an ordinal that's fine as long as you can interpret the model um but then I think what's important is to keep in mind when you try to interpret finding you just always need to keep in mind okay your outcome is people's agreement with that statement or people's response on that questionnaire.

It is not necessarily the thing in itself. And I think that awareness is often sufficient for you to be able to generate um alternative explanations here. Right? It's like oh maybe they are just saying that but they are saying that because of this or that or because they want to make me happy or because they feel that they might get um if it's people working for you they might feel like you they are being monitored and so on. So they might get questions and so keeping in mind that distinction between the thing you're interested in and the responses that people give you that I think is a good rule of the thumb just to get in some like critical distance and be aware of potential alternative explanations that um could be just as plausible.

In psychology, we sometimes uh inspired by by psych psychologist, statisticians, right? Those people with these two backgrounds, we sometimes repeat items in questionnaires uh to check validity or we add some items with negative framing, right? So if somebody says like oh I agree with this to 10, then I should disagree to one, right? It's the the response should be reversed. uh in practice we know that these repeated questions and the negative formulations or like different formulations of the same question might be also discouraging for people and and make them turn from the question or drop drop the questionnaire.

What would be your advice for people working on questionnaires when it comes to this aspect uh this this trade-off between validity and and u providing a sensible user experience to the user? First of all, I would advise from presenting the same question over and over again mainly because I think um hopefully the people filling filling out the questionnaire are thinking human beings and it's a social situation. You are asking them question questions, right? And they answering. And so imagine if like you ask me the same question like twice in a row or with some time in between.

I like that's weird. Like why are you asking again? Do you want to hear a different answer this time, right? Like should something have changed in between or do you want to have the same answer and so on. So there are all these like things that happen that you notice if you actually like participate in your own study where you're like, "Oh no, this is really annoying, right?" And so on. So I know that it is a problem in particular if you collect data online that there could be malicious responding and so on.

I I think bots change all of that anyway. So maybe this is going to be a different topic in the future altogether. But I would try always to um do any like screening for attention or seriousness and so on as early on as possible like before the actual part starts. There are other reasons why you would want to do that because if you have the attention checks baked into for example your outcome variable and then you exclude respondents because oh whatever they report um is not plausible or is not consistent then you are actually conditioning on a post treatment variable.

So this might be causally affected by whatever you're interested in and this might actually just induce new bias and um so we actually have a blog post on that a PhD student of ours time ai and he's just um talking about how excluding careless responses requires a causal justification because the careless responses might as well be valid responses and if you discard them you might be inducing bias and inducing new associations in your data. So, um I would always suggest from a causal influence perspective, but also from a like a user perspective to check that as early on as possible and then do the questionnaire with an eye on the user experience, not asking the same thing over and over again.

So, I know people also do that to ramp up Chronox Alpha to get like, oh, this is a highly re reliable questionnaire. I think that's never a good justification to ask questions that are fully redundant. Um, so if you reach the point where you are like, okay, I'm trying to come up with 10 different ways to ask the users whether they like this or not, I think you're just doing something wrong on a substantive level, like you have a narrow construct, right? You can ramp up that statistic by asking it over and over again, but your measurement is not going to get any better.

It's just like the impression of higher reliability. So maybe don't just ask one question, but just keep it usable. And so for example, I think um people working in the industry might have a much better eye on that than psychologists. We will just go all out and do 30 items for people's attitudes towards recycling and think that's reasonable because it gives like good statistical properties and it only works because we can force undergraduate students to fill out the surveys for course credit. So um in many ways I think psychology there has some problems that are less likely to occur in the industry and less likely to occur if you just stop and think about what you're actually asking people.

When we think about the data and removing observations from the data, many people who had some introductory statistics class or data analysis class probably heard this uh word outlier. And people say like, oh, if it's five standard deviations, six standard deviations away from the mean, that might be an outlier, right? Especially when when when it's, I don't know, just one point or two points and so on and so on. But we know that sometimes these outliers well sometimes they might come from the measurement error and that might be and that might be a reasonable assumption but sometimes they just have some causal mechanisms behind them.

What are your thoughts about working with outlying observations and removing observations? So you you mentioned this very interesting idea that if we remove u a response from our questioner data, we might actually introduce a collider bias as I understand right because this might be driven by the outer bias with outliers more generally speaking. What are your thoughts on on on working with these observations and what would be your approach? So um again my take would be that like determining um which outliers to remove or if to remove any is not a matter it's not a statistical question right it's like the substantive question like do you believe that this person has such a high value because actually the value does not reflect what you're interested in right maybe they I don't know slipped why they filled out the survey on their smartphones or something like that or do you are you just blindly applying a statistical criterion Yeah.

And so for example, I mean my own research, I think I've never removed outliers. And to be honest, I'm like in a privileged position here and then I'm working with fairly large data sets. So I think it shouldn't make a difference either way, right? Unless those outliers are systematically biased. But then if they are systematically biased, it's raising the the question, okay, what's going on here? And I think you can't get around that concern like what's going on here? Um so for example if you have um a reaction time experiment right and somebody is just way too slow um maybe one point why the like one idea why the data point might be invalid could be or maybe they were inattentive and they didn't even notice that the trial started and so on.

But if you then exclude these observations, you need to keep in mind, okay, I'm now conditioning on attentiveness. And attentiveness might be causally downstream of your um experimental intervention. For example, if the conditions vary in how much attention they demand and so on and so on. So my personal intuition, but I'm not working with these types of data. So I'm not the substance expert, but I would always always do the full analysis as well. And if there are discrepancies, you need to be very certain that the people you excluded because their data are invalid and not they are excluded because they are in a part of the distribution that's far out there, which might as well be the case.

So some people just are far out there, right? Like there's a lot of um inter- individual variability. And maybe I'm not removing outliers because I'm a personality psychologist and I'm like, oh, people are so wonderfully different in so many ways that we can't anticipate. So if people just tell me they are way out there, I'm just going to um believe that. But of course it does raise concerns. Um now in combination with you don't have full control over the data collection and so on. You don't exactly know what's happening there and so on.

And so there are trade-offs here. I would personally I'm not a big fan of outlier removal but I'm also working in a field and with types of data where it's less relevant. I just think in people in in general people like to treat this as one of these statistical things right you learn that oh it's this and that many SDS and so on but um it doesn't absolve you of um the task of thinking about whether it's plausible or not and sometimes it's implausible right like nobody is like three m meters tall so you can discard that person because they are likely like a malicious respondent right but if somebody is maybe 2 m tall that might as well be the case right so you need to actually keep the variable in mind and I think you should actually also look at the absolute values.

I know that people in psychology like to standardize everything and then they're like oh this is 2 SD above the average and so on but if you look at the um original unit that might also be just very helpful to see whether it's a plausible value or not. One of the ideas that I think are really underused in both in science and industry um ideas in in cosal inference I think are sensitivity analysis and partial identification. I think these are very very powerful tools that for maybe educational reasons primarily we don't use that that often we don't discuss them that often we don't teach them that often.

When we met before the recording today, you told me about this new trend in in psychology called multiverse analysis, which in my understanding is a type of sensitivity analysis, which is maybe even additionally powerful in the sense that it might take into account the study setting and so on and so on. Can you tell us a little bit more about this multi multiverse analysis? what is it and do you think it's it's a good idea and how could it be useful for anyone interested in in causal inference? So this multiverse idea um has been I think developed multiple times independently in psychology in sociology and so on and it's essentially the idea that you do have um all these decisions like outlier exclusion data prep-processing but also then how you set up your model maybe which co-variants to include in an experiment and so on.

These are all researcher degrees of freedom, right? And you might be able to justify either way actually. And now the idea of the multiverse is that you just do all of them. So you take all the combinations of all the decisions have in front of you and you just conduct all the analyses. And as you might imagine now, you get into like the territory of factorial explosion, right? Because you combine every possibility with every possibility. And so you do run a large number of analysis. Ideally, there is like one quantity of interest.

So, you extract that quantity of interest from all of the thousand to million models that you're conducting and then you look at the distribution of these effects and then you might combine that with looking at how different factors affect the results. And that's a general idea. So um multiverse analysis is like um I think the paper that branded it best which is why I think this is now the way people call this style of analysis. I think um Eonas called it vibration of effects and then there is specification curve analysis um from um psychologist that actually comes with like a p value for the whole curve and so on where it's a bit unclear what the p value stands for but it does come with a nice illustration.

So people will conduct a specification curve analysis but then combine it with the label multiverse to get the just like the nicest combination of things. So I have conducted such an analysis myself and it was specifically to make um a point about P hacking. So um substantively I've done a lot of work on birth order position on personality. So that's the question or are firstborns systematically different from later borns on various personality traits and so on. And if you look at the literature, and maybe that's to be expected, but it's an absolute mess.

And I think part of that is because any finding could be explained after the fact. For example, you could find that firstborns are less social, and it kind of makes sense. But then you could also find that they are more social. Maybe that also makes kind of sense because they're more likely to listen to their parents and so on. And so you have that situation where any pattern um could be could be explained. And one example for this is so one pattern that we do find in western countries is that firstborns are slightly slightly smarter on average but it's only like I mean maybe a tenth of a standard deviation or a bit more and it's of course not deterministic.

I always have to add that because I have a younger sister, right? In any case, I presented these findings and then one student was like, "Oh, but they have seen all these Tik Toks and they thought it was agreed upon that the laterborns are the smartest." And I was like, "Oh, that's kind of intriguing that you can take it either way and spin it into a story." So anyway, the literature is a huge mess. There are a lot of underpowered studies and it's very clear that, you know, people just hand out like a large battery of questionnaires and then somebody is like, "Oh, wait.

We can like reconstruct people's birth order." Oh, so let's compare the groups on all these like 50 variables and if we find one significant effect, we publish it. And um we try to like emulate that process. So we took a data set, we got all the personality variables, we had all these data analytic decisions that could be justified. So for example, which siblings do you count, right? Is it just biological full siblings that you grew up with or maybe do um half siblings also count if you grew up with them and so on? So you can do that in different ways.

And we just did all of that. And um as one might have expected, we mostly found nothing because we tend to find no effects of birth or opposition on personality outside of the intellectual domain. And if we look at adults, so there doesn't seem to be much going on. But we just wanted to make the point that individual specifications light up like they give you a significant effect. And then it's like, oh, but in larger families, the later born will be more um will be score higher on trust. If you adjust for gender and if you count all siblings and so on.

And we just wanted to make that point and show the whole curve clearly shows you there's nothing going on, but you can pick a point right over there or right over there and tell like opposite stories. And so this is how we used it in that spirit. And I hope it worked well. So I was on the the advisory committee of a PhD student who did skeptical work on multiverse analysis and he was like no no I invited you because your paper shows like how to use the method well um I think what you can't do with that method is just look at the curve and think that this is like essentially the the distribution of the effects and the uncertainty in it right so you can't just like say okay now I take the average and that is the most reasonable estimate because the problem is of course that not all specific ification are equally justified.

Or maybe you think they are all equally justified, but somebody with a bit more expertise in causal inference might tell you, "No, no, no. It's always better here to control for gender, so you should always pick those specifications." So, you actually can just ignore the others. And so, um, I wrote a bit about multiverse analysis and what purpose it serves. So I think if there's a genuine situation where either way is equally defensible and people agree with that in general then by all means conduct all the analyses right and give me all the results and then hopefully the results agree because otherwise it will be very hard to um find out which defensible analysis should be preferred.

But what's more likely to happen I think is that there are decisions in there that are arbitrary decisions that are not arbitrary at all such as outlier exclusion. Right? this can be more or less correct depending on whether you exclude only the invalid data points or also valid ones. And then there might be decisions in there that are clearly wrong, but the authors just don't know because people don't know how to set up the best possible um model of course in practice. And so you end up with a bit of like a mosh of various things and it's just hard what to make of the total specification curve of all the results unless they all agree.

If they all agree, you're always fine. You can always say it doesn't matter either way, right? right? You arrive at the same conclusion, but it often does make a difference and then you get into the spot where you actually need to think a lot about why this model would be more justified than the other one. Always keeping in mind that of course there will be sampling variability unless you have infinite data. My take on this now is so I think it is rarely um the right tool to figure out what's precisely the right answer to a research question.

I think again it has an educational value in that these analyses just highlight also how little we know about how to set up our analysis. So um if all these things are all equally defensible, maybe we just haven't thought about how to actually set up the model, how to actually specify an estimate like which assumptions go into the model and so on. And so this is a bit like the replication project where you just pick 100 studies no matter whether they actually need to be replicated and replicate them to highlight something.

And I think for the multiverse analysis it is just like highlighting this one issue. And um I think one of the original proponents also has um a manuscript that is like okay so here's how to use this in a classroom setting. And I think this is very valuable and I think large parts of the psychological literature are essentially classroom setting right we're trying to figure out something together. we are trying to figure out how to improve our practices. So it's less about the specific findings.

It's more about highlighting how uncertain we are in these decisions. Um and I think for that purpose it's a good thing mostly overall of course it has been abused like everything has been abused but I do think it highlights important issues. I do think it's really like just educationally like working through what could be defensible and what isn't and so on is valuable I think. So I'm not opposed to it even though I have written a blog post that is quite critical of it. But I do conclude with oh maybe it's about the insights we gain along the way and not about these specific analysis results.

We will link to the blog post in the in the episode description so everybody can learn your critique. You you mentioned uh this this educational aspect where we go and we look for different possibilities and we hope to learn something on the way. One of the things that I noticed in your in your publications is is that you also often talk about graphs like graphical models, DAGs and so on. What are your thoughts about the usefulness of graphs in your work and maybe in more general um in more general sense when you think about teaching causality to others? So I love cos graphs.

I'm a bit biased here and but I actually do love them for a reason and that is because I think they are super helpful for teaching and for explaining stuff and I think a lot of science is just about explaining stuff to others right so you have that idea in your head that this is how the data need to be analyzed but now you need to communicate that and I think graphs are just a super good way to communicate that my opinion here might be colored by the fact that I'm a psychologist and so we do have a strong tradition of structural equation modeling, path modeling or just people drawing these little process graphs with with arrows all over them.

So it's a very intuitive language um at least for me as a psychologist and then I do find that it's super intuitive for my students as well. So I have taught this to um like first year undergraduate students in psychology who hadn't heard anything about statistics but actually don't need much statistics to understand um graphs. The one thing is somehow you need to introduce the notion of conditioning on a variable but you can come up with like you know like easy examples where it just means stratifying and then it's fairly easy and then you just say oh and you will also learn about regression analysis which is another way to do that and then you can um let them reason through all of these like complex causal situations and um I think that worked really well.

So I also use the graphs to explain different types of samples. So in psychology you usually learn about oh there's a probability sample and then there are convenience samples and then there are like samples could be um selective and so on and you can all solve that with graphs and then the trick is just you have like a node that the person gets into your data set and then in your data you always have conditioned on that node and so it's very easy again to see oh okay so there's all these like collider biases introduced between all variables that affect whether a person um gets into the sample or not.

So it it is not trivial but if you learn the cause of graphs from the beginning and use them for everything it is quite natural for students to understand these situations and then the rules are super simple right it's like algorithmic rules like you don't need to think about what the variables mean you just need to think oh two arrows point in here condition on that then suddenly non-causal association here so I really like um using them in my own teaching um I even force um students in my personality seminar to read like a a pretty to DAX and um I think it works quite well because in the seminar you then critically discuss um findings and the dags give you a nice language to also like articulate your criticism.

So the authors authors do this but actually I think here's this confounder that affects this and this and thus this could explain that way their findings and so on. Um now when it comes to my um writing which maybe is less directly educational however I do actually think of all of my papers including my substantive papers as having like some educational purpose um because I think if you read like a paper in psychology it's always possible that the findings don't hold up and so on but I want people to get away something that lasts.

I think what you can get away what lasts are like more general insights about how causal inference works or maybe how statistics connect to causal inference and so on. And so my ideal is that people might read this without actually being interested in personality much but then be like oh yeah oh no of course this is how it works right and just take that insight maybe forget about where they got it from and then just move on and maybe later apply it um somewhere in their own work for a completely different topic just being aware of like the the individual structures that could induce bias and so on.

And so this is why I use them in my um own writing as well. So I know that people also like to um hate on decks because they cannot represent everything and because um it's not helpful for everything and I fully agree with that. I I don't think they need to be helpful for everything to be helpful for some things and I think a difference here might also be so um I think psychologists are very focused on um like communication and making things accessible and then you have more technical literatures where it might even be a point of pride that it's hard to follow along and I'm not sure maybe I'm being uncharitable here maybe it's just those people don't know better how to make it accessible because it's also hard but at least for example among econ economists I do have the feeling that for them it's like oh no it's only like you need to understand methods the hard way so that it's actually like a costly signal that you got the difference in difference right and so on so it's just like um making it harder so you can distinguish between levels of skills and so on and I think from that perspective like oh what do the dags add right you don't need the dags you can do it without the dags you just need to think harder right but for me it's like I don't I don't operate on the assumption that people enjoy enjoy thinking hard.

I know there are people who do that and um it's good for them and there are whole fields of these people but in psychology um people don't necessarily enjoy working through proofs and so on right so I really prefer to have something that is accessible and then it's not automatically accessible right so you need to work through okay so reader who doesn't know about these things what will they think about this how will they be able to follow along and so I'm thinking about dags as tools for that so there isn't also like the one true dag that you draw or there's like a deck that you draw to illustrate a point, a specific point.

Um, and I'm much more likely to use it them in that spirit than in the spirit of here's the deck that justifies my analysis according to deity. Although that's also a good use case. I think I would love it if people did that. But for me, it's more like highlighting specific complex causes structures that are not trivial unless you already understood them. And if they are not trivial to you, the graph might be an easier way for you to grasp it. and then if the graph doesn't work. So I have situations where I feel like this substantive example actually works better than the graph.

But you can always just present both. So I do think like people like to be like oh I need to write this like short and there should be no redundant information. I think most papers are published online these days anyway. So space isn't actually the big concern and I would much rather people have longer write longer papers that are more accessible because if you have the paper shorter well it's faster to read it but it's not necessarily faster to actually understand it. So I prefer to just offer redundant information and multiple angles and so on and the decks are one very very valuable part of that.

I really u think you are making a valuable point when it comes to the criticism of the idea that well DAXs are not useful for everything. Of course they are not. And I met these arguments online as well sometimes very surprised by them because it seems like um some of the people at least who who are using uh this kind of criticism or presenting this kind of criticism assume implicitly that DAG uh should be a full causal model, a fully specified causal model. While we know exactly if we look at the history how these graphical models how how Pearl uh proposed them why he proposed them for what reason and what's and what stage of of his discovery of of of the framework or construction of the causal framework that we currently know as the structural equation model framework.

We know that DAGs are just a tool just a structural information. It's just structural information induced by the structural causal model. By definition, they are not complete. They cannot describe everything in a causal model and and they are not designed to do so. So, I think that's uh that's a that's a very valuable thought for anyone who's who's maybe thinking about the role of DAGs they could be useful for in their specific use cases um and setting expectations. Before we finish, I wanted to ask you a question from from another dimension.

What? Yeah. What uh what were two books that changed your life? Oh, that is a really tough one. This is really hard because this is a causal question, right? Whether it changed my life. So, there would need to be a counterfactual world in which I did not read the book and things turned out differently. So, I'm going to do the psychologist thing here and like just pretend to answer a different question. So um I think as far as social sciences are concerned, there is actually one like social science pop science book that I really enjoyed and that I think did affect my finding and my my thinking and that is everything is obvious once you know the answer by uh Duncan Watts.

And I I read that a couple of years ago, but I had the impression it aged well despite the replication crisis and so on and had some valuable like pointers towards traps, how you might like fool yourself into thinking that you know something when you don't and so on. So um I really enjoyed that one. And then another one that I actually really enjoying right now and um that I do think is um shaping my thinking is by um Adam Khaski or I don't know how it's pronounced in English Kaskki or something.

an epidemiologist who has written a book called um proof uncertain science of certainty. And so this does cover a lot of causal inference but across like very different contexts including the history of various statistical approaches and so on in a manner. It's like a very very general book like how can we be certain about anything right and then it does talk about for example mathematical proofs about um proofs in a like in a legal context about law but then also like how do we know about causal effects like what's the proof that an association is actually like there and not just um variability and so on and I'm really enjoying this so um this is I think the first like I mean supposedly pop science book that I really feel has been written for somebody like me I'm also cited in like somewhere in there which is why I started reading it in the first place but I really enjoy that and I think I do want to kind of move into that direction with my teaching that it is a lot more general science so it's not just oh how do we run studies in psychology and what does that tell us about the world but it's more about okay what are in general the principles how can we generate cause and knowledge and then the examples can be from psychology I mean some will have to be from psychology so that students are not like what the hell is going on here but I do actually think there are a lot of other contexts that are of interest also to students.

So for example, um health issues I think are always of interest. Then um I just really got into um lab muffin beauty like Michelle Wong who is a beauty or skincare YouTuber but is going really deep into okay how can we know which products work and which don't what are the procedures in the lab and so on. And so she has a very very nice book where there's a very fun section on like you know you try out different skinincare products but how do you know whether they actually work right and so she suggests that you can like um do a trial with on yourself and you use half of your face as control.

So you have a new product you apply to one half of your face right and then you don't um apply to the other half and so on. And I was like, "Yeah, no, that makes perfect sense." And actually, I think that would have been useful information also to have like much earlier on, right? Like as a teenager, then it would have been, "Oh, that makes perfect sense and this is how it should be done, right?" And it highlights like, "Okay, you need a control group, right?" And then it's and immediately like like potential confounders come to mind.

So for example, you might be sleeping on one side of your face, right, which changes things and then yeah, okay, that's an issue here. So maybe it's not perfect, but maybe it's as good as it can get for your individual face and so on. And so I do really like um just that angle to bring in like more fields of um research and so on. And I actually think it's even if you have students who want to study psychology, I think um I mean some of them discover that psychology is not for them. Some of them move on to study medicine or to go into a different field.

and I would like to give them something and also if they move into the industry like to give them something that just applies like generally. So here's how you find out whether something works in your own life um if you're later working for a company and so on. And here's how to use these things and how to design these studies so that you can actually learn something maybe just about yourself. Um maybe about the wider world and I think that's a really cool way to put it and Adam's book fits right in there but I think there's a lot of material actually that you can use from different fields of research to make these points.

What would be your advice for someone who is starting or entering a complex field like u like science in general, machine learning, causal inference? Oh, general advice is always hard because I think it always depends on the um specific of what you're doing like if this is about like you are starting maybe as a PhD student I and this is the psychologist speaking but I would take great care to find a good advisor and you do find that out by secretly talking to former grad students or other grad students and um finding out um what the advisor is like because in my experience that is that makes such a big difference.

So for well-being on the one hand and I do think that's important. I did my PhD on well-being so I do think that matters but also I think just for your like intellectual development. So there are these labs where students are just taught you should do what everybody else is doing to crank out those papers and I think that is not a great way to learn things and so if you can identify up front that this advisor would make you do that thing you can just say no it's probably not worth it and so um my anger there is always like um like if you decide to go into a field it is a commitment right like it will have all sorts of downstream consequences and so I would make sure in the beginning that this is a place where you feel good where you feel comfortable and where you feel like you can actually learn things and then the rest like follows later.

But I would just like ensure that you don't um get yourself into an abusive environment where you are then stuck for some time and at some point you feel like you can't leave the field anymore. So I've I've had multiple points in my career where I considered leaving psychology like after my bachelor's degree, after my master's degree and maybe also sometimes now. Um, right. And I do think it's healthy just to consider like is what I'm doing is that it like a useful um way to spend my life because we spend a lot of time at work.

Do I really want to do that? And I mean you might actually be able to say more about that, right? Because you've switched a lot more. Um, but I do think it's healthy to think about that like whether this is what you want to do. I think it's good to take into account the quality of research of the field. So I have some students um who are like oh wow you're telling us all these stories about psychology and that's quite awful. And so what I usually answer to them is like yes actually I think if I could start out again maybe I would study something else.

On the other hand there are worse places than psychology. psychology is having good discussions and if you stay you can make a difference right and then you need to take these things into account and figure out what you want to do because I also don't want um want all students who care about science to drop out of psych out of psychology just because of reproducibility issues right that would be like a very unfortunate selection effect um leading to like an even worse field so I think you should keep in mind all of these aspects and then there will be so much uncertainty right so it will be very hard to tell up front so I would um I prioritize like immediate well-being and being in a good environment.

And I think if you're in a good environment, it's also always easy to like, you know, change tracks and so on. So if you have like a good advisor, if the field doesn't work out for you, they might tell you or they might say, "Oh, we need to refocus on something else to make this work for you and so on." Have a good advisor so that you have the behavioral flexibility to adjust to new um new information coming in. What's your message to the causal community? So I I just attended um Eurosim where I got invited.

I wouldn't have attended it otherwise and um as expected it was very hard for me to follow along any of the talks actually. Oh that's kind of concerning but then I I met other people I knew and they were like no I also don't understand anything and so on. I think for the more technical causal inference community um I think my message would be is that it is important to try to like um build bridges and reach out to more applied researchers in like in a good manner. So this might involve like maybe simplifying things and so on and providing less technical um explanations.

I don't think so. The the image I used in my talk at Eurosim was like a letter and at the very it's like a different cause letter. At the very top you have the people developing new causal inference methods, developing new algorithms and so on generalized solutions. Um I was informed about the existence of targeted learning in great detail there and so on. It's all happening up there. And then at the very bottom you have like the applied researcher who's trying to like scramble to write a paper, right? having barely understood that correlation does not equal causation and so at the very top there's good stuff happening that could um help improve the research happening down there but the knowledge needs to get passed down right and so I think there are multiple steps usually involved so the people who are developing new methods don't need to write tutorials for applied researchers there would not be efficient usage of anybody's time but I think it's like breaking it down for slightly le less technical people and then at some point maybe I'm on that letter right so I'm very close to the applied researchers.

So I really try to get people who are doing their own substantive research with little um technical guidance and so on and so um I would just like to see more people from the top reaching out down. I already see a lot of that happening. So we do have great like um causality communicators and so on. I also acknowledge that this is hard like this is a theory of mind task, right? Like dumping things down for people who are slightly less advanced in their technical understanding than you. But I would like to see more of that.

I think um causal inference researchers should do that more for the very selfish reason that it is just nice to have an impact on actual practices. So um in psychology we have a similar situation. we have like our own psychological methods field which is really a distinct field with its own journals and not every field of research has that but you can already tell that um there is not that much actually flowing into the applied research community and so there are stories about the German methods conference and somebody will be like I developed that model so does anybody know anybody who who could have data on which we could actually fit it right so it's developing a model for its own sake without any actual use case available um anytime soon and so on.

And I think um everybody could profit from a bit more um exchange here. And of course, this is also like a two-way thing, right? It's not just like the technical people provide resources and reach out, but also of course the applied researchers need to step up their game. So I think for that also we need to kind of like make sure during peer review and so on that the right issues are flagged. So the applied researchers are pushed to actually look at what's going on there on the more technical side.

But yeah, that's also something I'm working on. So um I think we can make great pro progress there and I actually think there are a lot of like really lowhanging fruits like just confusions in the applied literature that could be easily um resolved by somebody with a bit more expertise and causal inference. So I think there are low hanging fruits to be picked there by venturing out into the more like gritty applied world. And I know it can be depressing because then you actually see how bad the studies are on average and so on.

But I think it's a worthwhile endeavor because I think ultimately everybody in science I think does care about like the collective output of science right like is the field of psychology working well like is the research and the findings we are turnurning out is it actually reliable and so on. And I think everybody has an interest in that even the people who only only develop the models and so on. Great. Thank you so much Julia. I really appreciate your time. It was a great conversation and I would love to ask you I have at least five more questions here but I think we need to take it maybe for part two of our conversation.

I really appreciate I really appreciate it. It was a great conversation. Any closing remarks from you? Fun talking to you. So, ah, sorry. Any clos I I asked if you have any closing remarks before we before we say goodbye. No, I don't really have any closing remarks. Thank you so much. Thank you. Thank you for finding time uh to speak to us and I'm confident that the community will really love this conversation. Yeah. Thank you for having me. Thank you.