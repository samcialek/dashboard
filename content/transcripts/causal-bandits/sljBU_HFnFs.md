hi Cole Bandits it's Alex welcome to Cole Bandits extra in this special episode we will hear from researchers presenting their work at the triple AI conference ellot this year in Vancouver Canada and participants of our workshop on cality and large language models enjoy hi my name is Usman and I am a PhD student at sispa Health Center for information security in Germany can you tell us about the motivation for your work that you presenting here at the confence my this work is uh among uh some lines of going work that I did in my PhD the main overarching goal of most of them was to have practical causal Discovery algorithms that we can actually apply in real world scenarios and this one is about the case where you can adapt your causal Discovery algorithms to data as it comes along so instead of having your data set fully specified from the beginning you could make use of data that arrives over time and your learned causal Discovery algorithms to to learn as the data comes in could that also be applied in a scenario where the structure behind the data generating process is dynamical it could be so I mean if you look at the underlying concept it comes down to learning by compression you can show that you can use this particular concept not just if you assume that the incoming data set has fixed causal relationships but if those causal relationships over time the compression based strategy that at least I work on can be used to basically separate out those two different types of structures from each other very powerful what are the main insights or main learnings that you learn during this work because the problem setting is slightly more General it feels a lot harder there are a number of challenges for example it's not always easy to Ved out the differences especially if you have really you know you're just starting out data is starting to arrive and if like in the worst case at each point you get some data that is different that is from a different dynamical network then at least practically it gets really really hard to distinguish them and if you can't do this well earlier on your later results could be not as good as you like them so that's one practical consideration we don't have a solution for that yet but we are let's say trying to figure out and work out how we can overcome that what impact your work you see could have in the real world and what impact would you like to see what impact of your work would you like to see in the real world well the obvious one is if this could be applied to some practical problem so let's say for example in healthcare domain where you might not have for diagnosis of different diseases right you might not have enough data to begin with but at the same time as you get to know more and more and more you'd like your algorithms to not always just start from scratch but basically use what you already have and then adapt your knowledge and that I think would be a powerful tool especially if applied into domains like diagnosis and I hope that it is one day mhm before we conclude one I would like to ask you one technical question we know that some causal Discovery algorithms like PC are not very well at scaling so the the number of computations scal Super exponentially with the number of noes what are the computational requirements for your algorithm that the one that you presented in your work well I work with a score based quad Discovery so in the worst case there is no escaping the exponential you might still have to pay that that much of a cost in terms of scaling in practice however what kind of saves some of the score based algorithm is the greedy nature of search so in this particular case you could basically instead of exhaustively searching over everything you could try building networks or try discovering networks in a greedy fashion and assuming that your the score that you use to evaluate your networks fits the assumptions that you have then even the greedy approach Works quite well in practice it can even scale to let's say in my case for one of my earlier algorithms this greedy approach was able to scale to let's say 500 variables but that was let's say but there are certain assumptions aside but 500 variables over a sparse Network it's still able to work but that only comes down to how well your score kind of captures the assumptions that you have about your data mhm and what assumptions uh do you rely on in your work as far as the causal relationship functional assumptions are concerned the main one that we have is basically nonlinear functions with additive gion noise it's still an assumption nonetheless but at least in my subjective opinion this is not as restrictive as assuming let's say linearity hi my name is Emily mcmilan I am an independent researcher I'm here presenting research at the intersection of llms and causality I also am a research scientist at a major company what's the motivation behind your work yes so starting with hypothesis that um even though llms are trained on increasingly ever more data it seems like the whole world's data in fact they are not the whole world it is a data set in the end that they're trained on so it's some subsampled representation of the real world Target domain in which we use them and so if if there's sampling then there is potential for sample selection bias and so I explore different areas where there may be selection pressures of Interest what are the main insights from your work I mean definitely to consider the data generating process definitely don't assume that your data set is IID no matter how large it is also you know there's a lot of parallels really between large language models and like a lot of the conditional probabilities that you can hypothesize with a a causal dag so there isn't a lot of nice parallels you know you can propose a dag and uh see if uh you can measure the conditional profitability that it entails in an llm and there also you know I just encourage like looking actually at the log props at the probabilities that are coming from the llm you know they don't necessarily give you a strong um it's hard to assign meaning to them but you do know nonetheless that you're not looking just at the token you can introspect one level deeper at the at the probabilities mhm what have you personally learned during this project I've learned a lot so I'm I'm an independent researcher my PhD was in a very different field so coming into AI definitely a different world I learned a lot about peer review you know even just the scales there's many scores that you get you get the confidence you get the impact you get the actual score of your work so there's just a lot to navigate a lot to learn definitely learn there a lot of great fantastic Community out there machine learning collectives and and different organizations that are interested independent research and can support you along the way what impact can your work have on the real world so I would hope that you know particularly this this missing data problem the problem of sample selection bias I do think that there are definitely maybe very important things that you can hypothesize are being left out of the data sets and it might be the most mundane the most boring the least interesting but maybe that's actually just the common sense reasoning that we have that language models are lacking because no one's taking the time to write down what is obvious everyone since you know they're a month old what is the most interesting coal paper that you read the last month okay so last month so definitely interested in the causal parrots work I definitely find it very like um satisfying and this is now maybe even a couple years old but it might be interesting to people who are interested in llms and causal causality and it was a 2020 paper in nups and they looked at mediation analysis and they actually like introspected into the llm and they froze some weights and they had a nice parallel between like a causal dag and the Transformer model and they validated some interesting hypothesis and I think in general just an interesting method that people could use uh my name is Scott Muer I'm affiliated with UCLA and uh and Judea Pearl he's my PhD adviser what was the motivation behind the work you presented here at the conference I presented a paper along with uh three other co-authors uh I was not the the first author she had Visa issues so I I filled in here along with uh TR Toyota Research Institute and the idea there of the paper was about proc procurement of electric vehicles how to incentivize that effectively so the the problem was or is that uh governments and policy makers try to give incentives for people to purchase electric vehicles for greenhouse gas emissions and environmental sustainability and that's that's great because people then buy electric vehicles and electric vehicles in terms of environmental sustainability are better than ice Vehicles internal combustion engine vehic the problem though is they're not always better because the manufacturing greenhouse gas emission costs are far higher for electric vehicles if the elect I feel like I'm giving the whole the whole presentation here but just to frame the problem and and what we're trying to do the problem is is if you don't drive the electric vehicle that the government incentivized you to purchase or you don't drive them much which many people buy them as complimentary Vehicles so they're not driving them that much most of their miles are on their ice Vehicles then they can actually have a negative impact on the environment as opposed to if they had not ever bought an electric vehicle to begin with it's important that governments and policy makers incentivize the right people and so this is a problem of counterfactual nature with unit selection and with probabilities of necessity and sufficiency so for example it may be very important that that governments incentivize group of people that have the characteristic such as those households respond a certain way to these incentives some households might respond negatively to incentives maybe the incentive is coming from a government that's the opposite political party that the household is a devote of and so then they may never buy an electric vehicle because they were incent strongly incentivized to in a way with a message or from from a person or a group they don't like and then we've cut them off from electric vehicle and lower G greenhouse gas emissions for a very long time and so that's a really really a bad thing that policy makers want to avoid as opposed to you know the opposite is that somebody upon receiving this incentive they purchase the vehicle and drive it a lot and if they didn't receive the incentive they would not purchase the vehicle or they would purchase it and not drive it much and so that's the notion of a probability of benefit or probability of necessity and sufficiency and we want to wait those response types accordingly so you might want to wait the the harmful Consequence the harmful response type where upon receiving the incentive now they do not buy the electric vehicle not just now but for the foreseeable future as well and if you do not give them an incentive they would have actually bought it on their own and maybe that has a weight of -10 you we really want to avoid that whereas the the opposite where the incentive really works and really benefits the individual in the in the environment that has a weight of two still positive and still we really want that to happen but we really don't want the opposite to happen and so we incorporate all that into our formula that's what our presentation was about what are the main insights from this work you can get depending on your preferences for those weights you can come up with really different conclusions of who what groups to incentivize as opposed to if you don't take that into account at all and what kinds of incentives you want to give what have you personally learned during working on this project during your work on I knew the causality involved I don't think I learned any extra there it was more an application of the knowledge that I and others had what we learned was the electric vehicle and the ice vehicle sort of industry and how that works and uh to some degree how governments and policy makers make their decisions what impact on the real world this work may have well the hope is that governments and policy makers make decisions incentivize in the right way and for the right people in a way that benefits the environment what was the most interesting czzle paper you read last month there's a gray one by a colleague of mine at UCLA Yu chin along with uh with his PhD adviser Adon darwich on uh causal basian networks along with structural causal models where the variables if they're deterministic meaning like in a caal basing Network you know you have ones and zeros if certain variables are deterministic you can get Point estimates you can identify probabilities of causation or factual probabilities even if you don't know what those deterministic variables what formulas or the conditional probability tables are I'm Andrew linin I'm a researcher at Google deepmind and today I was talking about some work we've done trying to get a scientific understanding of what language models could potentially learn about causality from the passive training that they get what was the motivation behind this work well it was sort of to make sense of a puzzle in the literature which is that on the one hand we that systems can't learn from observational data about causal structures on the other hand people were showing all these things where language models were able to do some sort of interesting causal seeming tasks and the sort of resolution that we come to in this paper is that there's a difference between observational data which you really can't learn about causal from and passive data that might contain interventions and we argue that actually internet data is of the latter form and we show that systems that are trained on this kind of data can learn about generalizable causal strategies and other kinds of causal reasoning what is the main Insight from this work the key Insight is well I'd say there's two insights one is just this distinction between the observational Interventional Dimension and the passive active Dimension and understanding how language models fit into this space because of the nature of the internet text that they're trained on but the other Insight is actually to make sense of what systems can learn from this kind of data and it turns out that it's possible to learn strategies that will allow a system to experiment and generalize in new situations just from passive training which has interesting implications not just for causality and language models Al for things like philosophy of agency what do you think uh could be the impact of this work on on the real world probably very little under the other than understanding what language models can and can't do but I think that more generally it's hopefully going to be useful as a fundamental science contribution to understand the nature of learning causality what can be learned from certain kinds of data and I hope that that will help us to make better more robust Learning Systems in the future and to understand how we can shape systems to generalize better which is another aspect of what we studied in the work what's the most interesting Cal paper you read last month I think that probably some of the most interesting caal papers I've read recently have actually been studying humans World models and the extent to which we do or don't have them and there's some particularly interesting work looking at the kinds of situations that people's World models fail to capture fully faithfully and seeing how humans when they try to mentally simulate something will sometimes be unable to discriminate between physically possible and physically impossible situations in certain kind of ways suggesting that our world model is making some imperfect abstractions about the world any particular title or authors you would like to mention the paper that I was mentioning there is uh one of the main authors is Todd GIS who's a cognitive scientist at NYU I think that would be the place to go to look for it I forget what the title is hello my name is Ali I'm a researcher at Huawei and I attended this Workshop today and it was one of the most amazing and interesting workshops and talks that I attended in this conference so far my name is ABD Rahman Zed or just I'm a PhD candidate at Mila which is the Institute of artificial intelligence in Montreal I'm here in Vancouver to attend triple Ai and I really love the workshop on causal influence it was one of the best workshops I've ever attended in my life honestly I love discussion on whether or not large language models can learn causal relations and I like both points of view when they were just like each group was trying to put claims as to why they are not going to or are going to learn CA of relations so I see both points of view and I'm actually that's the best part of the workshop and when you have both points of view in your mind and you keep thinking about which one is the correct one but I also really like the the person who at the end who said maybe it's too early to decide now maybe it's just we need to wait a bit that was one of the things that were said at the last 5 minutes in the discussion which I loved really because the person was saying okay large language models have been around only for the last like let's say 5 years that like this like really huge size of language models so maybe it's too early to say do they have CLE can they learn caal relationships or not that's something I like but apart from this person everyone else was saying whether they think they can learn causal relationships or not and providing claims and I really love this kind of discussion I also of course love the uh talk by The Godfather of causal inference juder it was really a pleasure I mean just listen to him talking was really a pleasure talk about everything I mean I read his book of course the book of why he went a bit over it but he also added more things because there is a theme for this Workshop so the theme is whether or not large language models can have cause of relationship so he provided his own insights and thoughts about this question and I really appreciated the all of the questions that were asked to him and all of the speakers I love the workshop really what would be the main Insight from the workshop for you the main insight is that the part when when judia per said so Jud said something I really loved he said if the only thing you have access to is just observations then you would not be able to come up with causal relationship because you only see observations but this theorem or this statement could be proven wrong in the case of llms because llms have access to a huge amount of data and this data already has come interventions so this kind of theorem or this rule doesn't really apply to them because they have access to more data than like human beings could actually access when they read a book or two books so this statement might be actually proven wrong if that could could actually happen and they might actually have some causal like understanding so I like the fact that ethereum could also work in maybe all cases except maybe this specific so I like the fact that we are now looking about looking at llms as they actually are because they are not us they are not human beings we don't I don't train myself on like billions of I don't train myself on the whole internet so I shouldn't compare myself to a large language model so I like how he tailored the theorem so that he sees whether or not it will apply to a large language model and this is basically how he started talking about his answer to this question about whether or not large language models can have causal understanding great what are you currently working on I work on fairness in language models as well so anything related to bias so why language models can learn biases why language models can even make the bies that we have in our society even worse how to mitigate these bies how to measure these bies anything related to bues be it gender bu race bu religion bu any bias yeah this is my PhD mhm if you could recommend one paper that you offered or Co offered uh to people what what paper would that be so we have a recent paper on pruning like a large language models so we basically say that there exist some parts in every language model that are responsible for bi if you find them and you remove them the model will be less bied and we prove that this is the case