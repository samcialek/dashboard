if I recommend you to stay in Paris for three nights you'll get the price of three nights which is quite expensive if I recommend you to stay for one night you'll see a price that much cheaper and maybe will encourage you to continue looking for properties it's quite costly to test the models online you said that your team experiments with everything runs ABB test even for back fixes how was your experience when you joined this team so I must say it was quite shocking for me what was the most surprising finding about human Psych ology that you encountered in your work hey caal Bandits welcome to the causal Bandits podcast the best podcast on causality and machine learning on the internet today we're going back to Tel Aviv to meet our guest as a child he teamed up with his grandfather to design his own version of Monopoly he always loved to build but didn't want to become a purely technical person that's why he enjoys working on personalization and recommendations for real humans senior machine learning manager at booking.com ladies and Gentlemen please welcome Mr DEA Goldenberg gold let me pass it to your host Alex mola welcome to the podcast Dima thanks Alex hi how are you today good I think it was a very interesting day today like had a a lot of meetings as usual also some conversation with you and uh we're right before the holidays here so try to get a lot of content at the same day and meet as many people as possible mhm when we traditionally learn about recommended systems in machine learning courses those recommended systems are presented as predictive devices or associative devices in your work your and your team's work you merch the idea of a recommended system with causal inference what can causal inference bring to recommendations what benefits I think first of all if you take the story in the right uh sequence um the fact of trying to do personalization and introduce personalized recommendation to the customers and to the users is a very key Concept in any Ecommerce platform and uh you can think of this of like hey I can build a system that can predict what's the most suitable item what this most suitable uh recommendation for you and um the idea behind it is that if I'm using classical correlation based methodologies I can come up with a better recommender get a better accuracy get a better recommended systems metrics like accuracy top k or something like this but then you face the reality you need to find out the application itself so uh many times what we do is test all of this application basically any change on the website but in particular any machine learning solution and the recommenders in an AB test and try to see how it changes key metrics that we care about so we're trying this on high scale and uh many times you learn that although you built a really good recommender that can improve the accuracy of the previous Benchmark and uh have find the most suitable item for each of the customers and the users it might not move the needle in a CLE way because what you could do is just move traffic from one part of the website to another change the behavior of the customers or maybe just make something that is evident for them so technically I just recommended you the same destination that you plan to go in any case so why would it have any change and incremental impact on the go so that make us think about how we shift the behavior in an incremental way how do we introduce a change treatment to the website or to the experience of the customer in such a way that they would do something different from what they would do if we didn't do that and uh to be honest that's not something that's super intuitive to think what would happen if I didn't do that and especially if you're working on recommenders you have so many alternative options of what would I do differently what you need to think about is how do I change the behavior what's going to be incremental here and uh what going to make customers change the behavior and maybe react differently and in this case maybe what I want to do is not necessarily to find the most popular the most uh frequent solution in the database that's going to fit the customer needs but actually what going to be Innovative and change so it starts with the fact that uh maybe I want to set up the recommendation problem in a different way I want to find maybe instead of popular destination something that's trending something that's going to incentivize the customer to change their behavior or if I even want to frame it in a different way so I would say the role of the recommender while you even doing this uh presentation on the website plays a lot of role how do you frame it in terms of user experience the copyrighting in terms of timing and how do you place it and many other things that actually you think about them as something that's going to be incremental change the behavior of the customers and not just predict the things that you already know and um many times you do see this causal effect of this expected correlation that by the fact that I'm surfacing up the most relevant items if it's a destinations if it's a hotels or anything else if I'm finding something that's more relevant to the users it's going to be easier for them to find what they were looking for and make this incremental uh reservations on our platform but at the same time many times just by the fact that you're relevant might be not enough to change something in the customer Behavior so you're talking here about the thinking in terms of counterfactual outcomes rather than just predicting something that is likely for a given user to yeah their their behavior on the destination that that they would like exactly you need to focus on something that actually going to change the outcome between A and B because I think like we're doing a lot of ab tests we try run hundreds or even thousand of ab tests in parallel sometimes and trying any change that we do on the website starting from copyrighting and ux to back fixes and obviously any machine learning models or something like that so you need to build this uh change in a way that you're expecting to change to see significant difference in the metrics and it's also quite harder when you keep improving and have this uh quite optimized so I would say you need to find the things that's actually moving the needle a lot I would say that um as we expanding to New Markets new types of products Etc we also found out that in travel people like uh a lot driven by Price differences so anything related to discounting Promotions uh coupons and things like that that's actually something that change a lot the behavior of the customer and it also became one of the biggest strategies that we have in terms of let's try to offer more value to the customers by maybe even funding some of the travel options and giving discount which going to change the behavior and we've seen that in many cases changing the uh affecting the price given discount could be much more beneficial much better to to incentivize customer to book rather than being just relevant so this is a really good opportunity to extend your uh customer base and increase the volumes um but obviously it comes with a price right you can just give discounts to everyone you're going to lose money so you need to play this optimization game and to understand when it's benefici offal and you need to give the discount or maybe what discount to give and uh when not and uh here comes the the contrafactual problem exactly like the point that you raised of um many times you might say okay let's just give discounts to the customers that are less likely to book right we want to convince people so let's focus on the weakest uh segment that we have that's the main gap of understanding that people don't uh see that instead of just finding what is the segment that you need to give because they're less likely to book or something like this you need to understand what would happen with and without the treatment and obviously with the discounts as well and I think it's uh it's actually a very common problem in causal inference or causal learning to understand what would happen if I give a discount what would happen if I didn't give the discount and to figure out if I'm changing the behavior of the customer or not because optimization Wise from the perspective of the marketer if the customer would book anyway you probably don't need to give discount you want to make the the original outcome of the problem and you want to give the discounts only in a case where you're changing the behavior of the customer in a positive way obviously it's also not uh such a de diatomic problem many times it's just probabilistic and changing probabilities but you still want to measure this effect mhm um I think maybe what's different in the kind of discounts we're talking about compared to other promotion that you might have is also the structure of the cost cuz in many cases when you go for a marketing campaign or something like this the fact that you're targeting a specific population or the fact that you're merchandising um a discount you might have a cost Associated just with the fact that you're doing the treatment for example sending a physical letter to your customer or paying per click in some campaigns in this case we have more like a setup of a trigger cost so if uh the customer actually likes your offer and books then the cost is triggered and then you have to pay the cost but you also have the the booking and if the customer w't book whether it's because of the fact that you showed the incentive which is quite weird because but it sometimes happen that uh promotions and discount can confuse customers and basically scare them away from the platform or whether it's because they never had an intention to book in the first place which actually quite common in in Commerce the converion rate is quite low I'm not even talking about you know bot traffic and uh some automatic uh sessions that not expected to generate any bookings at all in these cases you don't have the cost so you're mainly focusing on the cost factor of the customers that booked and then you need to give the discount and you associate it together and making sure that was beneficial to offer that MH that's also related to another topic that is present in your in your work your in your team's work that you are operating under some fixed budget or some budget constraints maybe say talking more General yes so in a sense uh because the promotions are associated with cost we can't just give the discounts to everyone we need to control it in some way to make sure that we're not just constantly losing money but uh as a growth strategy of the company we still want to utilize as much budget as possible so the budget that was uh either given from the beginning or maybe even something that's self-funding campaign something that can be incremental to give the discounts to as much customers as possible in a sense it's a an interesting win-win game where you can have a self-funded campaign and by this give more value to the customers and actually get more discounts from them but because you have this constraint you have a quite interesting problem that may be not very common in the industry where most of the industry would just try to optimize the profit of the the incremental profit of your uh of your campaign in our case we're many times looking about incremental volumes how much new customers you bring how much new reservations you make while at the same time you control for the incremental cost so I don't want my budget to exceed the predefined budget or I don't want the return investment to be below a specific threshold and in this case if you have an spare budget extra budget you actually want to reinvest it and to get much more incrementality so it creates this kind of like two-dimensional incremental game where in one side it's very clear right you have the binary outcome of um let's give a discount and see if customer books or not and uh on the other hand you have the outcome of the promotion in terms of cost it could be it could lose you money because you could just give a discount that if you wouldn't get it you would make more money it could also bring you more money and be incremental in Revenue because the fact that you give a discount made people spend much more on your platform and it's incremental so by selecting and understanding how to allocate this uh in a clever way you have this optimization problem where each set each interaction you have incremental value and incremental cost and uh it actually brings you back to like very fundamental optimization problem of napsac problem which items should I pick which items I can't pick and how can I fit within the capacity budget constraint so it's also actually very nice mixture and combination between causal inference which is one field to an optimization which is completely different field and they come together in a very nice way that's great and I think I think that's very interesting what are the main challenges in your journey to combine this op this optimization perspective with the causal inference perspective and the recommendation perspective and so actually quite funny because like I used to work in each of these fields separately I learned a lot of causal inference when I first joined the company and did a lot of ab test and worked a lot of on incremental measurements Etc I used to do a lot of optimization work well even in my studies I had a lot of like combinator iCal uh optimization background and uh obviously machine learning and recommendations but looking at the fact that I need to solve some problems and start to use each of these components separately or suddenly Al together that what was uh bringing it in a really nice symbios and where you can interact and use all of them together and the apply part is quite different from the theoretical optimal solution because if you know the napse problem you pretty much know that hey it's NP hard it's needs a solver to solve it and we don't have a good uh solution for that at the same time you have approximations so you're just using this fractional approximation of like how much value divided by how much weight and you can sort uh your outcomes then on the causal inference you Lear learn a lot of U papers and research of how to do uplift modeling how to basically find the right segments to Target Etc in reality it's harder because the is super noisy you have a lot of overfitting you have seasonality and you're trying uh to bring it all together to make it work for the business outcomes and to make impact on where you actually needed and to fit the business needs and not just solve the hypothetical theoretical problem I think in this point it's also very interesting that from one point you're working in a dynamic environment where basically you can deploy the changes fast and see the reaction of of the customers how them and the business itself and at the same time you learn that the problem evolves because usually when you kick off with like hey I want to sell as much as possible within this budget as a starting point you figure out that once you uh deploy this solution to production even if you get the exactly right answer of the exact right problem you got from the product people you suddenly see new constraints of like hey I want this segment to be more dominant in this results we have some fluctuations in the um seasonality or something like that maybe we want to deploy the solution on more platform or more products so the problem is evolving all the time and you not necessarily solve it by introducing new variables to the mathematical formulations sometimes you just solve it again and again and again in a different setup sometimes you're not looking for the optimal solution but maybe for the most robust solution that will uh be helpful and reusable across different scenarios and uh many times you just need to monitor the outcomes and to make sure that you're ready and prepared for these changes because uh they will happen you don't know how and when but you need to make sure that you uh ready for that I would say maybe one key difference in experimentation about discounts promotions and anything related to pricing compared to let's say your ex changes or even deployments of other systems is the fact that the environment is very Dynamic so making a change and then just like ship and forget is not really an option many times you need to monitor it you need to compare it to an holdout so that's quite a common practice to see over time what would happen if I didn't give this discount compared to the fact what would happen if I give it in certain way one way or another and to see how this evolves over time how it change because the business evolves lots of moving parts around I just mentioned that we have tons of experiments happening in parallel even the D of the promotion itself might change and I think this is something that's very important in any applied machine learning but I think in this case specifically to connect to the understanding the customer because first of all we're doing with independent assumptions that we have from the original data are not really the case when we're dealing with real users real problems and uh we need to control for that I would say that uh we not always know how to control for that MH so uh I think the best the first thing that you need to do is to acknowledge them to understand that you have these gaps many times we have really nice models in offline evaluation we build the cool uplift models have really nice skinny cares measuring the best uh performing models and then when we deploy them in production we see some problem and um the more understanding you have what makes the the difference and misp parity or parity between online and offline the better you can improve your models offline cu it's quite costly to test the models online eventually we giv discounts so if you're doing it in the wrong way you might get uh Mis optimal results so it's better to test and train these things as much as possible offline what are what are some of the most useful strategies or the strategies that you find the most useful when it comes to evaluating models online and learning from this online evaluation in order to translate it to something new in the offline evaluation whoa uh so that's a lot I think uh the first step and that's actually quite a boring type of work but very important and very necessary is to align two fundamental things one of them is to make sure you having the same metric online and offline and while it sounds to well because it's coming from different sources and timing Etc it could be have quite different definitions that's one and the second one is the same with the data make sure that you're data collection process for the online and offline processes is the same and you don't have some fundamental biases or gaps between the two and just by that you're making sure that the there's alignment between online and offline and I would say the best way to test it is kind of like make sure that you have a very simple Baseline offline running offline and have this number and make sure that you're able to get exactly the same measurement when you're running it offline and if you're not that's also fine but you need to understand how much is this Gap and the that you can predict and expect it in the future iterations now besides that what we try to do is kind of like to do continuous experimentation so it's not enough to test some specific treatment in a specific period of time but actually we're trying to run it continuously to see that the gap between the no treatment and treatment or one treatment to another uh stays the same and if not we try to react to that so this is another solution and many times what we're also trying to do is to build the bit more robust solution in the sense of portfolio approach so we not necessarily have one treatment that is the optimal one and meets the exact threshold of what we set with the business but actually several different strategies sometimes very divers sometimes coming from the same uh from the same methodology and by that you kind of like ensuring that you always have at least one two working methods so if something breaks you can still rely on one or other I will say that eventually also something that we've seen working a lot is uh the fact that you need to stick to simple models it's really nice to go like to tons of features very complex models and um have really nice benchmarking online uh offline but then when you deploy it first of all you have a lot of problems with the deployment then the more features the most Parts you have moving Parts you have the more the harder it's to maintain it and also it makes this like a problem of over fitting much more complex when you're moving on because uh basically any new variable that you add might have drift might have seasonality might affect differently on the outcome eventually MH there's a lot of there there are many moving Parts here and the more variables you add the more moving part adding more moving Parts you risk you said that your team experiments with everything right ABB test even for back fixes how was your experience when you joined this team and you met this culture for the first time so I must say it was quite shocking for me and I joined this like experimentation culture where it's h everything is data driven and uh just show me the data and you'll get the chance to deploy Your solution and uh everybody's right if you have the right data to confirm it but it also creates a nice evolutional game I would say where the best solution the one that pulls to be better over time is the one that survives and evolves it's also quite intimidating because you have lot of as I mentioned before lots of moving Parts lots of teams that's working on similar or different directions and you need them to converge together sometimes it even creates uh some parts that could look the same and could even compete which is other on the platform and then again you can many times see with the data uh which one is better and I think um over the time also with the evolvement of my team that is part of a central Department that tries to build this solutions for uh optimization across the business so for different uh use cases for different problems is to see which parts are useful and uh repetitive between different solutions and to deploy them either as common methodology or maybe as a common platform and give them as a service to the rest of the teams and eventually it's also in a culture of collaboration because you see somebody solved the problem better than you do you learn why how which features both how do you do the experimentation and um you evolve with this over time I think in general what we have at booking is a lot of causal inference causal learning problems where we trying to understand what changes what uh strategies would work better than the others and uh we have this uh opportunity to share the knowledge with the rest of the community we have quite big uh group of scientists data scientists machine learning scientists working on this problem so coming down with the things that's work together for for other teams different teams and collaborate on these Solutions helps eventually to improve all the different uh products across the business MH you mentioned a lot talking to to business what are the main lessons or main insights that you could share with the community regarding in communication with business stakeholders as as as a person who is representing a technical team so I think that we started that from the beginning from pretty much the first question of what's uh what is recommender system and I think you need to understand what is the expected impact of your work I think many times when you're working on machine learning especially by the way not in the ca of work but let's say in many other domains like even content NLP Vision maybe even in recommenders many times as machine learning scientists what you're trying to do is to optimize the accuracy of your algorithm right if you're doing for example image recognition and you're trying to recognize object in the picture you're trying to improve the accuracy of your uh of your classification model but what's really important how it's going to be used where it's going to be deployed how what is the effect so for example if you have a really good model that can recognize toilets in the picture that's nice you can get to 99% accuracy but if you don't know what is the use case and how it's going to be used it's going to be really hard for you to move the needle and I think uh this is a key in everything we do in terms of um deploying machine learning connect it with the actual product need that also means that we have quite a mix and interactive teams so you always have a product manager assigned to your team trying to basically not just take the models and deploy them to be the most accurate but actually change and move the needle in the the important business needs and connect this model so if the toilet model recognizing toilets could be useful for you to maybe on board new small properties on the platform that didn't tag all the amenities they have in a good way then it's a good win for the business and you also need to understand this in a scale of like how important this change going to be many times you might have a good progress in machine learning which has a little to do with the incremental value to the business to the company to the customer I think that's also a very important part because you're talking about the business but eventually uh as a Marketplace who the people who benefit from uh from the outcomes are either people who come to book uh on our website and the customers that they enjoy from this experience or people who put services on our website like hotels or other stuff so you need to think about it from the perspective of like what they benefit from this uh which is nice because it correlates with the business needs it correlates a lot with the fact that we just want more deals to happen and uh we assume that everybody is happy once uh uh somebody finds the right place to stay what would be your advice uh to people who would like to improve technical people who would like to improve their communication with stakeholders specifically people um working in causal inference so so people trying to understand the mechanism from the technical point of view I think not to fall in love love with the with the machine learning work with the with the cool technical stuff cuz uh I would say I've seen a lot of cases where actually the simplest solution the simplest Baseline solution the you know deploying some average result deploying some one number magic number something like that could have a huge impact on the on the results on the customer behavior of what should happen Etc and many times when we trying to uh train models get into fancy stuff get into deep learning Etc we maybe we should do this but not at the first step at the first step we really need to understand uh and even get into this like position of like what would I do if I didn't know any machine learning at all and um I actually can give you a cool example that's related to causal inference but uh take it from a completely different perspective so I mentioned that we did a lot of recommendations of destinations of where you should go and uh one of the sub problems of this problem that we try to solve is let's say we recommended you to go to Paris for how many nights we should recommend you because eventually we need to generate a link that when you click it it's going to get you to to the place hey let's book a property in Paris and we had a really good model that sees the distribution of how many nights people book at paries by the way it's at least two three like people don't stay usually one night there and um we've seen there's like popular time of uh that people spend on the in Paris and we try to deploy the solution for each City maybe even contextual we can say for how many nights we recommend to stay there and then we've seen that it didn't help the customers it didn't make people book more it didn't have people to navigate to the page and basically it's not the right answer although we see that in in our use case and in what we've seen the data that's the right answer we need to recommend many nights and uh one of the things we realized is that it has a lot of side effects a lot of other potential thing that could affect uh people decision I'll give you an example if I recommend you to stay in Paris for three nights you'll get the price of three nights which is quite expensive if I recommend you to stay for one night you'll see a price that much cheaper and maybe will encourage you to continue looking for properties the Same by the way goes with availability there's much more uh properties available for one night than for three nights just because of availability concerns so in reality what we see many times is just by showing one night might be much more beneficial because it keeps people to stay to keep to dig more into into the experience and not necessarily the most accurate answer is the right one and uh that was quite funny realization because we work on this project I think for quite a long and we figure out that there's like the simple answer was always there under uh under our nose that's in a sense beauty of causality right we can come with a hypothesis and then we try to see if our hypothesis works well with the real world and we got an answer from The Real World and in this case the answer was no yes and I think in this case causality is also not only about you know discovering dynamics of like a complex world of like action and reaction it also has a lot to do with human psychology not necessarily rational right because probably the the right amount of knights to stay in Paris is two or three but actually what makes people change their mind what makes people to get to the decide action and by the way it's not that easy to Define what is the desired action obviously when you're trying to sell hotels or something like this probably you want people to book more but in some cases it's not that trivial especially if you're working on other areas of the platform like customer service or anything else so to understand what drives people to move toward the desired action uh many times you need to involve first of all interviews with the customers uix user research around that and but sometimes they don't even know that you themselves so they make this decisions irrational wouldn't confess that that's the reason why they making this choice and you just see it in the data so it's also the beauty of running experiments and just uh uncovering what happens there sometimes you feel like you hack the mind of the people and you're like whoa so that's what happens that's actually the the patterns the behavior that happen and many times it's counterintuitive and the fact that you can connect between people Behavior Uh from one side and the data in the results from Another Side I think it's also one of the most exciting parts of uh my work what was the most surprising finding about human psychology that you encountered in your work it's going to be hard to point at one particular thing think uh I tried many times to test different you know cognitive uh I would say urban legends of like of what happens like with I don't know Choice overload or uh price comparisons and other stuff with the users I would say like without even pointing a specific example I would say that it's inconsistent and uh you see it a lot of like even running a very successful and significant experiment on one setup one platform kind of like you know you have this like excitement of like hey I finally proved it that maybe I should show it in this way present the user experience in like in red color in blue color whatever and then you're trying to repeat exactly the same experiment on a different platform let's say it worked for me really well on like on laptop now I want to make sure that it works the same way on the apps so you super confidence you deploy exactly the same experiment changing let's say the color of the button or like I don't know adding a popup and you just see completely inverse results and you figure out that hey wait a second so it's not that consistent or maybe you see that people from different countries react differently or maybe you sometimes see that just the same person half a year later could have a different effect so I think actually I would say the most surprising fact about human psychology that it's not super consistent it's really hard to predict and while you do learn patterns you still need to make sure that you validate it a lot and I think um at the beginning I was really confident with lots of claims that like a causes B and like you know uh trying also to sell these ideas to other product teams of like yeah you should this thing I think the more I work on this like problems the more I kind of like become humble and understanding of like wait a second the fact that it worked for us not necessarily going to mean that it's going to work once again if you repeat this it could be about using a specific machine learning methodology could be about measurement it could be about changing uh ux uh differences or anything like and but that's the beauty of like that we have the opportunity to measure the results so you could always say maybe it's going to behave like this let's test I think this is kind of like my goto approach like we have some hypothesis we want to try it we have some hinge of like we want to have this thing working this way but the final solution the final result that you're going to measure it's going to be on the actual causal AB test that going to show you that yeah you were right that works this one didn't work it also shows us the limitations of ab tests right that that the external validity is not something that we can take for granted in terms that if in one environment we have shown the effect it doesn't automatically translate to another environment or another population yes I think when you read papers especially like again let's go to psychology papers that come to conclusions that I don't know that people from a specific race would prefer a specific color or specific gender would do that and not that you always like you know open up like hey to who participate in this particular experiment when it was done by whom what is the background of the people because yes maybe this the result that you've seen is significant maybe for the specific people that you ran this experiment it was actually significant and different but if you want to replicate this results many times you will face different problems whether if I even if I'm trying to test exactly the same thing but I'm doing a small tweak and uh not in the way that the recipe said and uh you need to validate it again in again U which again I would say that in in business environment when you're trying to improve uh your platform you need to keep evolving and you need to keep monitoring the changes because even if it worked really well for you couple of years ago it doesn't say anything about how it's going to work now for people working with causal inference whether it be experiments or observational data or mixed observational and and experimental data it can be sometimes challenging especially if they're new to the field to get used to this idea of thinking in counterfactuals comparing those counterfactual outcomes it seems that it might violate some of our natural inclinations or natural intuitions about how to think about causality what's your experience with this so first I would say it's it's also hard for people who experience in the field like uh me and my team working on uh this like uplift modeling causal inference so for the last three four five years and still many times we face this dilemma of like wait we never thought about what would happen if we didn't do that and uh I think this kind of thinking is requires you an extra level of like of cognitive load of understanding that he there's a completely different scenario that I could do completely different actions and the outcomes could be different and uh you can fall into this when you trying to build a a treatment and to understand that like hey I'm not looking at the causal effects I'm looking at just like specific thing but from Another Side you also never have these two scenarios at the same time right you never know what would happen if I do a and if I do B at the same time you don't know what would happen if I give the coupon and don't give the coupon in the same time so you never know what would be the actual outcome so kind of like randomized AB test helps you to do that but still you don't have this particular specific person specific uh change happen with the same uh people twice and it's also hard you know when you're modeling it so even if you get into the side of like let's evaluate the models let's understand is the model is good or bad you don't have tool labels it makes the explainability of the model harder it makes the monitoring of the model harder it makes the benchmarking of the model harder because you don't really know what would be the optimal action you do get it sometimes when you get into simulations this is a game we like to do of hey let's just simulate a world when you have the outcomes of both scenarios and to understand which one would be better but this is a very much like you know extensive thinking and kind of like our way to come to a solution that it's hard for us to grasp it now if you take it to you know to normal people then don't get to use with causal influence all day uh contrafactual are even harder like if I'm looking at different decision that I had to do back in my life like some I don't know some dilemmas that I faced like whether I should study that or that you don't really build this scenario of like hey like there's like this turning uh doors that I'm like I'm going to go to scenario a and end up like that and go to scenario b and end up like that so it's really hard to understand what would happen if you didn't do this and you face it like but once you grasp it you can see it everywhere right you can understand it in U when you're getting into supermarkets and you see like some deals and you understanding wait so they give this discount and now they can sell much more they actually make me uh buy some I don't know some cheese that I never plann to buy if they didn't have this discount or you can see it in public policies of uh basically some incentives some changes in the policies that make people change the behavior and I don't know pay more taxes even in some cases or buy Apartments earlier because they know that tax is coming so you see a lot of things of what would happen if if you do something differently and you see these policies uh but it's very easy to be like you know Smart in retrospective to see people doing uh changing the behavior and moving into a different direction you not necessarily predict this behavior in U in advance especially if you're the one that makes the policies yeah I I think that there there's so um there's so much to talk about when it comes to human psychology and and contactual thinking and causal thinking um and there's so much interesting research on this I would like to take a step back now and get back to the point where we started our conversation so go back to this idea of causal recommended systems if you could give a short introduction to those people in our audience who are not familiar with your work what kind of ideas or what kind of technological building blocks have you used in your work on those on those recommended systems so I would split it because like there's the department of caal recommender systems which I would say could be another podcast and we can talk about it a lot I think that what we working on is a bit more simplified version of it of at least how we call it uplift modeling it also sometimes refer as heterogenous treatment effect of basically trying to find the conditional average treatment effect of some treatment so you have a set of treatments it might be just two treatments like discount give or not give it could be multiple treatments like what should I give and then you can get closer to recommended systems and basically when you're trying to solve this problem you need to understand which outcome is better and better is also depends on which um which metrix do you define sometimes if you have just one metric it's quite easy you just measure which one is better if you have more multiple metrics then maybe you need to do a multiobjective optimization maybe you need to do some kind of like parto tradeoffs between the different tools so we start and the nice part that we almost always start with randomized data technically you could do also a lot of causal inference and causal recommendations for observational data and it happens a lot especially when you're doing like things like ranking recommendation you always have bias data unless you're doing full randomization but even then you might have some bias so if you have bias data you need to debias it in some techniques either in this propensity way think or some other techniques and approaches but you need to come into the understanding of how do i deas h different problems that I have in my data uh if you're starting from scratch and you have let's say just two treatments uh it might be easier because you're starting with a randomized data set so you have uh treatment a and outcome of treatment a treatment B and outcome of treatment B and then you can measure first of all the average treatment effect so was treatment a better than b or the opposite let's say we're offering discounts many times it happens that people prefer discounts and like them so they book more they have a better outcome with the discount and then we get into uh the problem that we might have too many discounts that we gave away we need to balance it and control it some way to fit into the budget so by the way without this constraint it would be an easy trival solution which some uh people don't think about like sometimes it might be okay that just to give the treatment to everyone and everybody's going to be happy with that we not necessarily need to start again with the modeling maybe the trivial solution could be good enough so then you get into another problem that you can't or shouldn't give the treatment to everyone you need to balance it so first of all it could happen that not everybody has a positive outcome of the treatment right with discount it's quite uh straightforward that people prefer a better price they prefer a discount so many times the discount will have positive effect sometime it doesn't especially if the discounts offers you a product that you never planned and then it just takes you completely off the plans that you trade to try to do but in most cases it is and then if you could give the discount to everyone that's good but if you can't then you need to understand who is the most suitable segment for this how you can uh optimize it and by the way the segment doesn't necessarily have to be usel segment it could be anything else it could be based on other properties and um then you need to find a way that says what would happen if I give the discount and what would happen if I didn't give the discount which is the causal modeling eventually the uplift modeling one of the ways we're doing that is by using a technique that uh uh was developed at booking uh retrospective estimation basically training an uplift model just from the converted data and that we could understand how we could uh who should get the discount who's not and we find that it eventually gives us some kind of Soo okay so basically what is the uh potential impact potential outcome of each individual and by that again if I can give as many as possible I would so I need to find what is the right threshold when to stop and what is the segments that are going to be affected and what of the segments are not that's done usually with the causal measurements like K curves when I'm trying to understand what is the optimal teral Point uh and with that we usually go to production and see if our policy is right and I think the fact of tuning the offline outcome and comparing it to what's happening online that's a let's say a separate uh type of uh expertise I would say that you need to like learn and iterate and many times learn from mistakes and from application that could be different from case to case from company to company but I would say that the key for that is just to see what happens and uh react to the data as a kid you like to build stuff how does early experiences in building stuff designing a game with your grandfather how does this impact the way you see your work today so that's interesting I think that uh you know we take this anecdote of one thing that I did is like uh I've seen other kids have the Monopoly game uh and uh didn't find where to buy it even it wasn't even a problem of like can't can't afford it I just didn't find so like we decided with my grandpa to to build this game ourselves and uh we had the first version of this I went to the beal to play with kids where uh I don't know why we didn't play with the ball we just sat and play with that but okay and uh I found it too easy like I played with the original Monopoly I remember like how fast fasinating it was and this one was too easy so I guess I came back I think it was like it I was also going to my grandpa so it was like a month after month after month and we decided that we want to make it a bit more complex a bit more rules and bit more constraints and uh the game became more interesting we also seen that some mechanism doesn't work some the of the design could uh could work differently it wasn't necessar about economy or like you know of other stuff of Monopoly just how fun it is and um I think if you're trying to find the analogy of the work we're doing today I would say again you try to solve a very complex problem okay with lots of moving Parts uh sometimes big budgets lots of customers you should start with a simple solution and it's definitely not going to be the optimal solution I think that's the biggest gap between like you know Academia and Industry you need something that is better than the current one you try it you learn from this and you start to introduce more and more complexities for example when you're doing promotions allocations or uh discounts you can start with like you know the simple understanding of like can I even give this promotion so like just run a test and see what happens see if it's beneficial if it's losing money if customersa well for this again many times the problem or the key is even in the ux and how you communicate it and let's assume it works and you want to allocate it optimize it you would start with like simple lever like give or not give so when I should give it when I shouldn't give it then you can make it a bit more complex and get into the point where you can have another lever maybe which promotion to give maybe I can give different uh discount levels maybe I can offer promotions on different products you can make it even more complex of when to give it or in which part of the user experience uh maybe I should give it at the beginning of the journey maybe I should give it at the end of The Journey maybe we should do this as a incentive that comes later so the timing could play a role and you can make it more and more complex by introducing more and more levers like even on which items maybe I shouldn't give the discount to all of the items maybe I should give it only to some and the optimization problem becomes very complex sometimes you solve them independently and hope that they just going to work sometimes you're trying to build a complex system that's going to take into account some of these moving PS together uh most of the time it doesn't work but you still have some approximate solution of how it works so I would say that seeing how this mechanism evolves and also seeing how the reaction in the feedback is something very interesting in like in designing such systems it sounds like you were engaged with this game with your grandfather in a really long-term project it was not something that happened over a day or two yeah that's because like you know the friends that I had over there it's like it was in a different city so I was like coming over for weekend or for like for some vacations and like playing with like uh other set of friends and like you know and then I had like a break of like of a week a month or two that like there were in between so every time I came back it was like you know completely new game yeah I wouldn't say that this like you know this part was a very much you know life shaping Etc but if I try to remember like this games of monopolis I remember that we had several iterations of that what keeps you going today oh that's interesting uh first of all I think it's uh it's just enjoying your like your environment life work uh whatever like here at booking for example I'm almost seven years seven years here and I really enjoy the people the environment the individuals that I interact with the team that I have like people in my team people in teams that I interact with and uh you know just beside the fact that they're super smart uh I also just enjoy like you know spending time with them so I think that's really that's something that drives me a lot and uh I would say that uh on the professional level is that uh you have a lot of opportunities to try stuff to keep learning to see how it works to see how things react and kind of like keep exploring keep uh learning new stuff and um I think particular I'm quite lucky that I have the opportunity to test different experience on very big scale and see the world of data how it reacts and really see big volumes of things that uh moving and changing and in particular like making this impact on uh lots of Travelers seeing it's like changing lot of stuff and even like you know we're talking about uh something that I'm using on my day today as well like you know my parents planning a a trip they're also using the same tool they see my work so I think it's very cool that you see this like almost feedback loop of like of the work that you're doing and the and the feedback that you get from either the immediate you know metrics in the work or even from your uh friends what would be your advice to people who are just starting with causality causal inference or machine learning in general I think I repeated it a couple of times but uh first of all keep it simple try to understand what are the fundamental things that are shaping the problem and uh first of all understand the problem that you're trying to solve and only then pick up the tools that you need to solve because uh I mean I'm using I'm working on this quite a lot and I'm still making this mistake again and again and again luckily I have enough good people smart people around me that tell me DI my over complicated but really try to keep it simple and uh yeah and try to to come up with like Simple Solutions don't be ashamed of this I think this is also something that's like very typical to uh people who starting in machine learning that they almost embarrass from Simple Solutions because they feel like they didn't really accomplish what they were hired for and uh I would say that it also depends I think from company to company from different environment to another environment maybe different things are appreciated but in this case try it like especially if you're working on a business environment that's looking for like uh impact try to understand what actually moves the needle what actually Chang an import and uh then navigate your effort towards that and not necessarily into fancy cool stuff but actually to the things that work yeah like today the Deep learning culture promotes jumping into architectures without understanding the problem often I think that even then you know like deep learning is not a bad thing it's like it's so useful and it has a lot of cool solution even like you know I would say the recent advancement of generative Etc but then again you need to understanding how it's useful for you for example how do you connect causal inference and generative ey it's super untrivial you could get into the part where like hey I want Chad GPT to just tell me should I give a discount or not but obviously it can just trainer my data and understand how it works at least for now so maybe what you can do is get some context features maybe get get something that you couldn't get from like you know unstructured data introduces to your model so you can find some interaction with that but you also need to be super smart I would say that when you're saying architecture I would say that most of the complex problem that you need to solve is not actually about how to design your deep Learning Network or neural network many times you just have pre-trained networks that doing really good job what you actually need to do is to understand how do you plug a lot of different parts that responsible for different tasks together to solve the ultimate task that you have in your uh problem and uh maybe even connecting back to causal inference I think that one of the most fundamental things in machine learning that you need to understand before doing any modeling Etc is the evaluation you need to make sure that you evaluate the right metric that it's consistent and correlated what you're trying to solve that it's useful across the different applications that it's easy to understand for all your stakeholders and when you're cracking the evaluation in the right way it's quite straightforward to understand what you need to improve and then you only have the problem of how what are two books that change your life so it's a tricky question uh I'm not too much into books to be honest I like a lot Popular Science I think like I told you that uh back when I was in the university in studies I organized a lot of uh Popular Science talks and actually was exposed to a lot of super smart people that were talking about their books but I barely read the those books I think one maybe two books that actually helps to shape a lot of like of the things that interest me is related to let's say uh cognitive science and rationality so one of them is a predictably IR rational I think by denelli and then after reading that I went to the you know to the hardcore Foundation of that to Thinking Fast thinking slow by uh canman and that just passed away like a month ago so yeah that was quite a big change to you know to uh science in general and maybe to also my view at uh again connection like of people behavior and then what you can learn maybe from data I took couple courses to connect again between like my engineering data science background to a bit more how people making decisions How uh Behavioral Sciences and even my research was almost always around these topics of like marketing and uh like seeing what makes human behavior change and again understanding this pattern just understanding the fact that you can really find repetitive patterns in human mind whether they're rational or irrational because I think also rational or irrational or concept that coming much more from economy and optimization rather than psychology and uh figure out that that's how it works and it's repeating from people from person to person between people that's quite cool and I think you know when you working in the field of machine learning AI you see this coming a lot when you're training models right you like just models are eventually outcome of what they're exposed to so what they see in the data that's what's going to happen I'm a fresh that I have um like six weeks old uh child and uh uh and an older one so I also have some experience with that and you actually also I don't know maybe that's a professional disformation but you also see this in the in kids right that they just expose to examples to data and they react and they doing mistakes and they learn from this and you see how they shape their understanding of the world right they see objects moving and first not understanding what that and then they understand that that's their own hand and can respond to that and now they understand they can control it and then they understand that maybe they can't move themselves but can point in some objects and tell you as a parent what to do and U the fact that they crying is actually the optimal treatment to get your attention so that's how they they basically optimize their goal right of getting your attention of getting some love from you by crying I think that do you actually see this that like they're trying different stuff and this is was the one that convers the most so it's quite fascinating to see that like that's how human mind work and this is quite the thing that we're trying to teach machines to do who would you like to thank to thank there's a long list I don't know yeah um look I think uh I'll start with my family and uh you know I like I learn a lot about this and understand this a lot on the go but uh like I moved here to Israel when I was 10 and my parents kind of like left behind everything they had quite nice careers and uh lots of opportunities and immigrated move to a different country with like kind of like new start and I think I never felt that like I felt like they invested a lot into me and uh this is something that I understand in retrospective not necessarily something that I felt back then lots of my education lots of many different opportunities so lot of thank to them probably grandparents as well I just told you this story about building a game with Grandpa I never thought that it would be such a life shaping experience so I would say that uh you know the older you get the more thankful you are for your family yeah I think uh and professionally I would say that I'm really inspired by my colleagues like whether it's my teammates people that report to me and like give me new challenges new opportunities new learnings every day people that I have fun with and just you know enjoy to spend time and uh also other people that there's lots of like learning that you can make from that so I would say that I would splitted into many different people that shape uh my experience obviously my new family the kids that helped me understand the better the world obviously my wife that supports me and all of that so lots of people from also lots of different perspective I think I really like the fact that many different experience uh from the past have this like connecting the dots sometimes in retrospective so like you just expose to some ideas by people and you never understand why it's useful for you and then you know like few years later you tend get to use this particular thing that you learn from that and H from this person you might not understand that like a particular thing that you learn from one person at certain point of time could be really useful and converge with many others learning and knowledge that you see from others people and other experiences and it's it's shaping you a lot I I like to look at everything that I learned every uh learning opportunity that I had every course that I take during my degree is something that I found useful later on on professional career I know that many students don't see it maybe don't believe that hey like why am I studying this like I don't know operational research course it's never useful for me in my like data science work and then like hope like seven years later you're doing napsack problem solution and like uh and things like that or you know any other experiences that could be sound very stupid very simple but then like it might be helpful even if it not directly just indirectly knowing this idea seeing it somewhere connecting the dots and makes you better what question would you like to ask me I noticed that like you know you're investing a lot into like connecting with people and uh uh kind of like you know doing all this podcast and all this work uh with the kind of like I would say a mission behind it of again connecting a lot of uh ideas and opportunities together and I would say that what would be the nicer the greatest outcome that you would expect from just like you know meeting all these people oh that's a beautiful question I think I already have outcomes that are beyond what I imagined when I was starting with all of this with the book and and the podcast um I had wonderful opportunities and I learned so much recording this podcast that I never thought it would be possible but if you ask me about something that the big biggest goal that would be creating a platform that leads to a resource maybe a book maybe a series of books that bring together different perspectives regarding causality so perspectives from continuous optimization reinforcement learning operational research experimentation uh formal causal formalisms like pearls formalism and so on and so on because I must tell you that I see when I talk to people in different sub areas of causality that sometimes those people are trying to solve problems that are already solved in another Sub nich in another little ghetto and now they don't even know that other people were working on this uh this a similar problem because there is a citation Gap in the literature and nobody never cited a person from this other stream I noticed this a lot especially like uh you know even if you're jumping from like one uh discipline from another so like let's say you have economics you have and you have Healthcare they might have even like different uh terminology for the same things I think uh at some point we had one employee that like had a lot of experience with like heterogeneous treatment effect work but never heard about uplift modeling and it I think it took us a week to understand we're talking about the same things with just like we're just relying on completely different literature cuz they're just like citing to different sources and basically working on the same things but with different formulation with different terminology but trying to solve pretty much the same problems and actually I think like he even wrote this like paper Bridging the Gap between heterogeneous treatment effect and uplift modeling to kind of like show how similar they are personally I had this experience of like talking about what I do with people from Healthcare and um while many of the things that we do sound very similar you still see that the problems we're trying to solve are very different so if I'm trying to find the optimal policy they sometimes trying to look for the most robust policies and we're using exactly the same tools we're using exactly the same approach to the problem but we call it in different names we evaluate in different names and I see a lot of opportunities of that converging and I also connect to the point that you mentioned that like people don't know that the problem is solved somewhere else sometimes we don't even notice this like you know within the company we solve a problem we're super proud of our solution we kind of like showcase the solution and then we hear that other teams just tried the same thing like two years ago and it already worked and you have this like kind of like ahuh so maybe we should communicate more and I think it's it's a lot about like yeah building Community talking about this and even aligning the terminology yeah yeah and I I I'm I'm a firm believer you know in in you know building more bridges between those ghettos I think we have wonderful opportunities awaiting us there um regarding many many problems that we are facing today Dima where can people learn more about you your team and your work so luckily we had quite a lot of publication in this field specifically in like uplift modeling causal inference uh as booking.com and causal inference AB testing is quite a very strong tool that we also talk about and uplift modeling specifically so we had couple of recent tutorials in cikm and in www uh we have couple of papers uh published in recent years multiple talks presenting this problem and in general we really like to talk to others about how we solve these problems because we learn a lot also from the industry and how it works I've seen that the causal machine Learning Community is also evolving a lot there's lots of workshop and different confer there are lots of python packages of how to do that sometimes they're doing exactly the same and it's not really clear what's the difference between them but you have a huge group of people to collaborate on this so I think if you just look up for some content that we release and booking at causal inference uplift modeling you'll see a lot and also personally I got so excited about this that I actually after working on all of this started the PHD and started to research that so I keep working on uh creating new knowledge on this topic that's really amazing what's your message to the Cal python Community shell earnings your results your approach and uh don't just think that you're stuck somewhere like talk to others and see if they have a better solution or some solution to your problems Dima that was a wonderful conversation thank you thank you